def _entrata_post_succeeded(resp_text: str) -> tuple[bool, str]:
    """Best-effort parse of Entrata response body to determine true success.
    Returns (success, reason). Treats HTTP 200 with embedded errors (e.g., duplicates) as failure.
    """
    try:
        t = (resp_text or "").strip()
        if not t:
            return False, "empty_response"
        # Try JSON first
        try:
            j = json.loads(t)
        except Exception:
            j = None
        if isinstance(j, dict):
            # Common patterns: { response: { result: { status: 'ok'|'error', message: '...' } } }
            resp = j.get("response") if isinstance(j.get("response"), dict) else j
            res = resp.get("result") if isinstance(resp.get("result"), dict) else resp
            status = str(res.get("status") or resp.get("status") or "").lower()
            msg = str(res.get("message") or resp.get("message") or "").lower()
            # Check for duplicate indicators FIRST (before generic error status),
            # because some APIs return status="error" with message="duplicate invoice".
            # We need to classify these as "duplicate" so the frontend can offer repost.
            status_msg = f"{status} {msg}"
            if any(k in status_msg for k in ["duplicate", "already exists", "already posted", "invoice exists"]):
                return False, "duplicate"
            # Treat any explicit error status as failure
            if status in ("error", "fail", "failed"):
                return False, status
            # Heuristic success
            if status in ("ok", "success"):
                return True, status
            # If no obvious signals, fall back to keyword scan
        # Keyword scan on text
        low = t.lower()
        if any(k in low for k in ["duplicate", "already exists", "already posted", "error", "failed", "failure"]):
            # Prefer specific duplicate reason if present
            if "duplicate" in low or "already" in low:
                return False, "duplicate"
            return False, "error"
        # Default to success if none of the error markers found
        return True, "ok"
    except Exception:
        return False, "parse_error"

import os
import datetime as dt
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any, Tuple
import json

import boto3
import snowflake.connector
from fastapi import FastAPI, Request, Form, Body, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse, RedirectResponse, StreamingResponse
from fastapi.middleware.gzip import GZipMiddleware
from urllib.parse import urlparse, unquote, parse_qs
import requests
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from fastapi import Response, Depends, BackgroundTasks
from itsdangerous import TimestampSigner, BadSignature
from passlib.hash import bcrypt as passlib_bcrypt  # legacy; avoid calling due to backend issues
import bcrypt as pybcrypt
import hashlib
import auth  # New role-based authentication module
from itertools import islice
import re
import gzip
from io import BytesIO
from zoneinfo import ZoneInfo
import time
import calendar
from concurrent.futures import ThreadPoolExecutor, as_completed
import google.generativeai as genai
import base64

# -------- Config --------
BUCKET = os.getenv("BUCKET", "jrk-analytics-billing")
ENRICH_PREFIX = os.getenv("ENRICH_PREFIX", "Bill_Parser_4_Enriched_Outputs/")
OVERRIDE_PREFIX = os.getenv("OVERRIDE_PREFIX", "Bill_Parser_5_Overrides/")
AWS_REGION = os.getenv("AWS_REGION", "us-east-1")
REVIEW_TABLE = os.getenv("REVIEW_TABLE", "jrk-bill-review")
REVIEW_QUEUE_URL = os.getenv("REVIEW_QUEUE_URL", "")
APP_SECRET = os.getenv("APP_SECRET", "dev-secret-change-me")
SESSION_COOKIE = "br_sess"
SESSION_MAX_AGE_SECONDS = 7 * 24 * 3600
SECURE_COOKIES = os.getenv("SECURE_COOKIES", "1") == "1"
DRAFTS_TABLE = os.getenv("DRAFTS_TABLE", "jrk-bill-drafts")
PRE_ENTRATA_PREFIX = os.getenv("PRE_ENTRATA_PREFIX", "Bill_Parser_6_PreEntrata_Submission/")
EXPORTS_ROOT = os.getenv("EXPORTS_ROOT", "Bill_Parser_Enrichment/exports/")
# Catalog sources from exports
DIM_VENDOR_PREFIX = os.getenv("DIM_VENDOR_PREFIX", EXPORTS_ROOT + "dim_vendor/")
DIM_PROPERTY_PREFIX = os.getenv("DIM_PROPERTY_PREFIX", EXPORTS_ROOT + "dim_property/")
DIM_GL_PREFIX = os.getenv("DIM_GL_PREFIX", EXPORTS_ROOT + "dim_gl_account/")
DIM_UOM_PREFIX = os.getenv("DIM_UOM_PREFIX", EXPORTS_ROOT + "dim_uom_mapping/")
# Config storage (accounts to track)
CONFIG_BUCKET = os.getenv("CONFIG_BUCKET", BUCKET)
CONFIG_PREFIX = os.getenv("CONFIG_PREFIX", "Bill_Parser_Config/")
ACCOUNTS_TRACK_KEY = os.getenv("ACCOUNTS_TRACK_KEY", CONFIG_PREFIX + "accounts_to_track.json")
UBI_MAPPING_KEY = os.getenv("UBI_MAPPING_KEY", CONFIG_PREFIX + "ubi_mapping.json")
WORKFLOW_REASONS_KEY = os.getenv("WORKFLOW_REASONS_KEY", CONFIG_PREFIX + "workflow_reason_codes.json")
WORKFLOW_NOTES_KEY = os.getenv("WORKFLOW_NOTES_KEY", CONFIG_PREFIX + "workflow_notes.json")
WORKFLOW_CACHE_KEY = os.getenv("WORKFLOW_CACHE_KEY", CONFIG_PREFIX + "workflow_cache.json")
ACCOUNT_STATISTICS_KEY = os.getenv("ACCOUNT_STATISTICS_KEY", CONFIG_PREFIX + "account_statistics.json")
OUTLIER_RECORDS_KEY = os.getenv("OUTLIER_RECORDS_KEY", CONFIG_PREFIX + "outlier_records.json")
UBI_ACCOUNT_HISTORY_KEY = os.getenv("UBI_ACCOUNT_HISTORY_KEY", CONFIG_PREFIX + "ubi_account_history.json")
PORTFOLIO_MASTER_KEY = os.getenv("PORTFOLIO_MASTER_KEY", CONFIG_PREFIX + "portfolio_master.json")
DIRECTED_PLAN_PREFIX = os.getenv("DIRECTED_PLAN_PREFIX", CONFIG_PREFIX + "directed_plan_")
SCRAPER_LINK_KEY = os.getenv("SCRAPER_LINK_KEY", CONFIG_PREFIX + "scraper_account_link.json")
BW_LAMBDA_NAME = os.getenv("BW_LAMBDA_NAME", "jrk-bw-lookup")
REWORK_PREFIX = os.getenv("REWORK_PREFIX", "Bill_Parser_Rework_Input/")
SHORT_TABLE = os.getenv("SHORT_TABLE", "jrk-url-short")
CONFIG_TABLE = os.getenv("CONFIG_TABLE", "jrk-bill-config")
KNOWLEDGE_TABLE = os.getenv("KNOWLEDGE_TABLE", "jrk-bill-knowledge-base")
AI_SUGGESTIONS_TABLE = os.getenv("AI_SUGGESTIONS_TABLE", "jrk-bill-ai-suggestions")
DEBUG_TABLE = os.getenv("DEBUG_TABLE", "jrk-bill-review-debug")
STAGE4_PREFIX = os.getenv("STAGE4_PREFIX", "Bill_Parser_4_Enriched_Outputs/")
STAGE6_PREFIX = os.getenv("STAGE6_PREFIX", "Bill_Parser_6_PreEntrata_Submission/")
POST_ENTRATA_PREFIX = os.getenv("POST_ENTRATA_PREFIX", "Bill_Parser_7_PostEntrata_Submission/")
UBI_ASSIGNED_PREFIX = os.getenv("UBI_ASSIGNED_PREFIX", "Bill_Parser_8_UBI_Assigned/")
# Scraper bucket for utility PDFs pulled by external scrapers
SCRAPER_BUCKET = os.getenv("SCRAPER_BUCKET", "jrk-utility-pdfs")
# Gemini API for PDF date extraction
GEMINI_SECRET_NAME = os.getenv("GEMINI_SECRET_NAME", "gemini/parser-keys")
FLAGGED_REVIEW_PREFIX = os.getenv("FLAGGED_REVIEW_PREFIX", "Bill_Parser_9_Flagged_Review/")
HIST_ARCHIVE_PREFIX = os.getenv("HIST_ARCHIVE_PREFIX", "Bill_Parser_99_Historical Archive/")
FAILED_JOBS_PREFIX = os.getenv("FAILED_JOBS_PREFIX", "Bill_Parser_Failed_Jobs/")
METER_DATA_PREFIX = os.getenv("METER_DATA_PREFIX", "Bill_Parser_Meter_Data/")
ERRORS_TABLE = os.getenv("ERRORS_TABLE", "jrk-bill-parser-errors")
MANUAL_ENTRIES_TABLE = os.getenv("MANUAL_ENTRIES_TABLE", "jrk-bill-manual-entries")
PARSED_INPUTS_PREFIX = os.getenv("PARSED_INPUTS_PREFIX", "Bill_Parser_2_Parsed_Inputs/")
REWORK_PREFIX = os.getenv("REWORK_PREFIX", "Bill_Parser_Rework_Input/")

# Check Slips DynamoDB tables
CHECK_SLIPS_TABLE = os.getenv("CHECK_SLIPS_TABLE", "jrk-check-slips")
CHECK_SLIP_INVOICES_TABLE = os.getenv("CHECK_SLIP_INVOICES_TABLE", "jrk-check-slip-invoices")

# Admin users who can access CONFIG, Vendor Corrections, Gap Analysis, etc.
ADMIN_USERS = {"tma@jrk.com", "cbeach@jrk.com"}

# -------- S3 Key Security --------
# Valid S3 key prefixes for delete/copy operations
VALID_S3_PREFIXES = (
    "Bill_Parser_2_Parsed_Inputs/",
    "Bill_Parser_4_Enriched_Outputs/",
    "Bill_Parser_5_Overrides/",
    "Bill_Parser_6_PreEntrata_Submission/",
    "Bill_Parser_7_PostEntrata_Submission/",
    "Bill_Parser_8_UBI_Assigned/",
    "Bill_Parser_9_Flagged_Review/",
    "Bill_Parser_99_Historical Archive/",
    "Bill_Parser_Failed_Jobs/",
    "Bill_Parser_Meter_Data/",
    "Bill_Parser_Rework_Input/",
    "Bill_Parser_Config/",
    "Bill_Parser_Enrichment/",
    "Posted_Invoices/",
)

# -------- AI Review: Garbage Line Detection Patterns --------
# Patterns for line items that are likely NOT actual charges (balance forward, payments, etc.)
# Each tuple is (regex_pattern, reason_code)
GARBAGE_LINE_PATTERNS = [
    # Balance/Payment items (should NEVER be line items)
    (r"balance\s*forward", "balance_forward"),
    (r"previous\s*balance", "previous_balance"),
    (r"payment\s*(received|thank)", "payment_received"),
    (r"amount\s*paid", "payment_received"),
    (r"(credit|debit)\s*adjustment", "adjustment"),
    (r"late\s*(fee|charge|payment)", "late_fee"),
    (r"returned\s*(check|payment)", "returned_payment"),
    (r"deposit", "deposit"),
    (r"refund", "refund"),
    (r"credit\s*balance", "credit_balance"),
    (r"balance\s*transfer", "balance_transfer"),
    # Climate credits (often need removal)
    (r"ca\s*climate\s*credit", "climate_credit"),
    (r"california\s*climate", "climate_credit"),
    # Misc non-charges
    (r"total\s*(due|amount|charges)", "total_line"),
    (r"amount\s*due", "total_line"),
    (r"please\s*pay", "total_line"),
]


def _detect_garbage_lines(lines: list[dict], vendor_id: str = "", property_id: str = "") -> list[dict]:
    """Detect line items that are likely garbage (balance forward, payments, etc.).

    Uses both hardcoded patterns AND learned patterns from human corrections.

    Args:
        lines: List of line item dicts from Stage 4 JSONL
        vendor_id: Optional vendor ID for learned pattern matching
        property_id: Optional property ID for learned pattern matching

    Returns:
        List of detected garbage lines with metadata:
        [{
            "line_index": 0,
            "line_id": "__id__ value",
            "description": "Balance Forward",
            "charge": 123.45,
            "reason": "balance_forward",
            "confidence": 0.9,
            "source": "hardcoded" | "learned"
        }, ...]
    """
    import re
    results = []
    detected_ids = set()  # Track which line IDs we've already flagged

    # High-confidence reasons (almost always garbage)
    HIGH_CONFIDENCE_REASONS = {"balance_forward", "payment_received", "previous_balance", "total_line"}

    # Load learned patterns for this vendor
    learned_delete_patterns = []
    learned_keep_patterns = []
    if vendor_id:
        try:
            learned_delete_patterns = _get_learned_patterns("DELETE", vendor_id, property_id)
            learned_keep_patterns = _get_learned_patterns("KEEP", vendor_id, property_id)
        except Exception as e:
            print(f"[AI Learning] Warning: Failed to load learned patterns: {e}")

    for idx, line in enumerate(lines):
        desc = (line.get("Line Item Description") or "").lower()
        desc_original = line.get("Line Item Description") or ""
        line_id = line.get("__id__")
        charge_raw = line.get("Line Item Charge", 0)

        # Parse charge amount (handle string format)
        try:
            if isinstance(charge_raw, str):
                charge = float(charge_raw.replace("$", "").replace(",", "").strip() or 0)
            else:
                charge = float(charge_raw or 0)
        except (ValueError, TypeError):
            charge = 0.0

        # Check hardcoded patterns first
        matched_hardcoded = False
        for pattern, reason in GARBAGE_LINE_PATTERNS:
            if re.search(pattern, desc, re.IGNORECASE):
                confidence = 0.9 if reason in HIGH_CONFIDENCE_REASONS else 0.7
                results.append({
                    "line_index": idx,
                    "line_id": line_id,
                    "description": desc_original,
                    "charge": charge,
                    "reason": reason,
                    "confidence": confidence,
                    "source": "hardcoded"
                })
                detected_ids.add(line_id)
                matched_hardcoded = True
                break  # Only match first pattern per line

        # Check learned DELETE patterns (if not already matched by hardcoded)
        if not matched_hardcoded and learned_delete_patterns:
            matched_pattern = _matches_learned_pattern(desc_original, learned_delete_patterns)
            if matched_pattern:
                results.append({
                    "line_index": idx,
                    "line_id": line_id,
                    "description": desc_original,
                    "charge": charge,
                    "reason": "learned_delete",
                    "confidence": 0.8,  # Learned patterns get slightly lower confidence
                    "source": "learned",
                    "learned_from": matched_pattern.get("source_pdf_id", ""),
                })
                detected_ids.add(line_id)

    # Apply learned KEEP patterns - remove from results if we learned to keep
    if learned_keep_patterns:
        filtered_results = []
        for result in results:
            desc_original = result.get("description", "")
            matched_keep = _matches_learned_pattern(desc_original, learned_keep_patterns)
            if matched_keep:
                # We learned to keep this type of line - remove from garbage
                print(f"[AI Learning] Keeping line '{desc_original[:30]}...' due to learned KEEP pattern")
                continue
            filtered_results.append(result)
        results = filtered_results

    return results


def _write_account_history_record(pdf_id: str, vendor_id: str, property_id: str, account_number: str,
                                   bill_date: str, total_amount: float, line_count: int, utility_type: str):
    """Write a history record to DynamoDB for fast account history lookups.

    Called when a bill is submitted/posted.
    """
    try:
        if not vendor_id or not property_id:
            return False

        # Normalize the bill_date to YYYY-MM-DD for proper sorting
        sort_date = bill_date
        if bill_date and "/" in bill_date:
            parts = bill_date.split("/")
            if len(parts) == 3:
                try:
                    m, d, y = int(parts[0]), int(parts[1]), int(parts[2])
                    if y < 100:
                        y += 2000
                    sort_date = f"{y:04d}-{m:02d}-{d:02d}"
                except Exception:
                    pass

        # Build composite key: VENDOR#PROPERTY#ACCOUNT (lowercased, no spaces)
        account_key = f"{vendor_id}#{property_id}#{account_number}".lower().replace(" ", "")
        item = {
            "pk": {"S": f"HISTORY#{account_key}"},
            "sk": {"S": f"BILL#{sort_date}#{pdf_id[:12]}"},
            "pdf_id": {"S": pdf_id},
            "vendor_id": {"S": vendor_id},
            "property_id": {"S": property_id},
            "account_number": {"S": account_number},
            "bill_date": {"S": bill_date},
            "total_amount": {"N": str(round(total_amount, 2))},
            "line_count": {"N": str(line_count)},
            "utility_type": {"S": utility_type or ""},
            "created_at": {"S": datetime.now(timezone.utc).isoformat()},
        }
        ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
        return True
    except Exception as e:
        print(f"[AI Review] Failed to write history record: {e}")
        return False


def _get_account_history(vendor_id: str, property_id: str, account_number: str, limit: int = 10) -> list[dict]:
    """Get recent bills for this account from DynamoDB (fast ~50ms).

    Uses pre-aggregated history records written when bills are submitted.
    History builds up over time as bills are processed.

    Args:
        vendor_id: Entrata vendor ID
        property_id: Entrata property ID
        account_number: Account number from the bill
        limit: Max number of historical bills to return

    Returns:
        List of dicts sorted by bill_date desc
    """
    results = []

    if not vendor_id or not property_id:
        return results

    try:
        # Build composite key (same format as write)
        account_key = f"{vendor_id}#{property_id}#{account_number}".lower().replace(" ", "")

        # Query DynamoDB - sorted by sk (bill date) descending
        resp = ddb.query(
            TableName=AI_SUGGESTIONS_TABLE,
            KeyConditionExpression="pk = :pk AND begins_with(sk, :prefix)",
            ExpressionAttributeValues={
                ":pk": {"S": f"HISTORY#{account_key}"},
                ":prefix": {"S": "BILL#"}
            },
            ScanIndexForward=False,  # Descending order (most recent first)
            Limit=limit
        )

        for item in resp.get("Items", []):
            results.append({
                "bill_date": item.get("bill_date", {}).get("S", ""),
                "total_amount": float(item.get("total_amount", {}).get("N", "0")),
                "line_count": int(item.get("line_count", {}).get("N", "0")),
                "utility_type": item.get("utility_type", {}).get("S", ""),
                "pdf_id": item.get("pdf_id", {}).get("S", ""),
            })

    except Exception as e:
        print(f"[AI Review] History lookup failed: {e}")

    return results


def _validate_s3_key(key: str, allowed_prefixes: tuple = None) -> bool:
    """Validate that an S3 key is safe for delete/copy operations.

    Returns True if key is valid, False otherwise.
    Checks:
    - Key is not empty
    - Key doesn't contain path traversal patterns
    - Key starts with an allowed prefix
    """
    if not key or not isinstance(key, str):
        return False
    key = key.strip()
    if not key:
        return False
    # Block path traversal
    if ".." in key or key.startswith("/") or "\\" in key:
        return False
    # Check against allowed prefixes
    prefixes = allowed_prefixes or VALID_S3_PREFIXES
    if not any(key.startswith(p) for p in prefixes):
        return False
    return True

def _require_valid_s3_key(key: str, allowed_prefixes: tuple = None, operation: str = "operation") -> str:
    """Validate S3 key and raise HTTPException if invalid."""
    from fastapi import HTTPException
    if not _validate_s3_key(key, allowed_prefixes):
        print(f"[SECURITY] Blocked invalid S3 key for {operation}: {key[:100]}")
        raise HTTPException(status_code=400, detail=f"Invalid key for {operation}")
    return key.strip()

def _sanitize_error(e: Exception, context: str = "operation") -> str:
    """Return a safe error message for API responses without exposing internals.

    Logs the full error internally but returns a generic message to the client.
    """
    error_str = str(e)
    # Log full error for debugging
    print(f"[ERROR] {context}: {error_str}")

    # Check for known error patterns and return friendly messages
    error_lower = error_str.lower()
    if "access denied" in error_lower or "forbidden" in error_lower:
        return "Access denied"
    if "not found" in error_lower or "nosuchkey" in error_lower or "does not exist" in error_lower:
        return "Resource not found"
    if "timeout" in error_lower or "timed out" in error_lower:
        return "Request timed out"
    if "connection" in error_lower:
        return "Connection error"
    if "validation" in error_lower:
        return "Validation error"
    # Generic fallback - don't expose stack traces or internal paths
    return f"Error during {context}"

# Use default credentials chain on AWS (App Runner)
# Configure boto3 with high connection pool for parallel operations
from botocore.config import Config
_boto_config = Config(
    max_pool_connections=130,  # 20 general + 100 check-review + 10 headroom for main threads
    retries={'max_attempts': 3, 'mode': 'adaptive'},
    connect_timeout=5,
    read_timeout=30
)
s3 = boto3.client("s3", region_name=AWS_REGION, config=_boto_config)
ddb = boto3.client("dynamodb", region_name=AWS_REGION, config=_boto_config)
sqs = boto3.client("sqs", region_name=AWS_REGION, config=_boto_config)
_lambda_client = boto3.client("lambda", region_name=AWS_REGION, config=_boto_config)

# Global thread pool for general parallel S3/DDB operations
_GLOBAL_EXECUTOR = ThreadPoolExecutor(max_workers=20)
# Dedicated high-concurrency pool for CHECK REVIEW bulk S3 reads (I/O-bound)
_CHECK_REVIEW_EXECUTOR = ThreadPoolExecutor(max_workers=100)

# -------- App --------
app = FastAPI(title="Bill Review", version="1.0")

# Add GZip compression middleware to reduce response sizes (especially for JSON APIs)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# -------- Performance Monitoring --------
import threading
from collections import deque

_PERF_LOG: deque = deque(maxlen=50_000)  # Ring buffer (auto-evicts oldest)
_PERF_LOG_LOCK = threading.Lock()
_PERF_ROLLUPS: dict[str, dict] = {}      # hour_key -> {endpoint: stats}
_PERF_ROLLUPS_LOCK = threading.Lock()
_PERF_LAST_HOUR: str | None = None       # Track hour transitions for auto-persist

# Per-user locks for directed plan read-modify-write operations
_DIRECTED_PLAN_LOCKS: dict[str, threading.Lock] = {}
_DIRECTED_PLAN_LOCKS_GUARD = threading.Lock()

# Path normalization patterns (collapse dynamic segments for aggregation)
_PERF_PATH_PATTERNS = [
    (re.compile(r'/api/timing/[^/]+/'), '/api/timing/{id}/'),
    (re.compile(r'/api/timing/[^/]+$'), '/api/timing/{id}'),
    (re.compile(r'/api/invoices/[^/]+'), '/api/invoices/{id}'),
    (re.compile(r'/api/flagged/[^/]+'), '/api/flagged/{id}'),
    (re.compile(r'/api/master-bills/detail/[^/]+'), '/api/master-bills/detail/{id}'),
    (re.compile(r'/yyyy=\d{4}/mm=\d{2}/dd=\d{2}'), '/{date}'),
    (re.compile(r'/\d{4}/\d{2}/\d{2}'), '/{date}'),
]
_PERF_SKIP_PREFIXES = ("/static/", "/favicon", "/login", "/logout")

def _perf_normalize_path(path: str) -> str:
    """Collapse dynamic path segments for aggregation."""
    for pattern, replacement in _PERF_PATH_PATTERNS:
        path = pattern.sub(replacement, path)
    return path

def _perf_percentile(sorted_times: list[float], p: float) -> float:
    """Compute percentile from sorted list. Returns 0 if empty."""
    n = len(sorted_times)
    if n == 0:
        return 0.0
    idx = min(int(n * p), n - 1)  # Clamp to valid index
    return round(sorted_times[idx], 1)

def _perf_compute_rollup(records: list[dict]) -> dict:
    """Compute per-endpoint rollup stats from raw records."""
    by_endpoint: dict = {}
    for rec in records:
        ep = rec["path"]
        if ep not in by_endpoint:
            by_endpoint[ep] = {"count": 0, "sum_ms": 0, "min_ms": float("inf"),
                               "max_ms": 0, "times": [], "errors": 0}
        b = by_endpoint[ep]
        ms = rec["ms"]
        b["count"] += 1
        b["sum_ms"] += ms
        b["min_ms"] = min(b["min_ms"], ms)
        b["max_ms"] = max(b["max_ms"], ms)
        b["times"].append(ms)
        if rec.get("status", 200) >= 500:
            b["errors"] += 1
    # Compute percentiles
    result = {}
    for ep, b in by_endpoint.items():
        times = sorted(b["times"])
        n = len(times)
        result[ep] = {
            "count": b["count"],
            "sum_ms": round(b["sum_ms"], 1),
            "min_ms": round(b["min_ms"], 1) if b["min_ms"] != float("inf") else 0,
            "max_ms": round(b["max_ms"], 1),
            "avg_ms": round(b["sum_ms"] / n, 1) if n else 0,
            "p50_ms": _perf_percentile(times, 0.50),
            "p95_ms": _perf_percentile(times, 0.95),
            "p99_ms": _perf_percentile(times, 0.99),
            "errors": b["errors"],
        }
    return result

def _perf_record(path: str, method: str, status: int, ms: float, user: str):
    """Record a request to the perf ring buffer."""
    if any(path.startswith(p) for p in _PERF_SKIP_PREFIXES):
        return
    normalized = _perf_normalize_path(path)
    rec = {"path": normalized, "method": method, "status": status,
           "ms": round(ms, 2), "ts": time.time(), "user": user or ""}
    with _PERF_LOG_LOCK:
        _PERF_LOG.append(rec)  # deque auto-evicts oldest when maxlen reached

def _perf_maybe_persist_hour():
    """If the hour changed, finalize the previous hour's rollup to DynamoDB."""
    global _PERF_LAST_HOUR
    current_hour = dt.datetime.utcnow().strftime("%Y-%m-%dT%H")
    with _PERF_ROLLUPS_LOCK:
        if _PERF_LAST_HOUR is not None and _PERF_LAST_HOUR != current_hour:
            # Finalize previous hour
            prev = _PERF_LAST_HOUR
            rollup = _PERF_ROLLUPS.get(prev)
            if rollup:
                try:
                    ddb.put_item(
                        TableName=CONFIG_TABLE,
                        Item={
                            "PK": {"S": "CONFIG#perf-rollup"},
                            "SK": {"S": prev},
                            "UpdatedAt": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                            "Data": {"S": json.dumps(rollup)},
                        }
                    )
                except Exception as e:
                    print(f"[PERF] Error persisting rollup for {prev}: {e}")
        _PERF_LAST_HOUR = current_hour

def _perf_update_current_hour():
    """Compute rollup for the current hour from raw records."""
    now = dt.datetime.utcnow()
    current_hour = now.strftime("%Y-%m-%dT%H")
    # Compute epoch timestamp for start of current hour
    hour_start_dt = now.replace(minute=0, second=0, microsecond=0)
    hour_start = calendar.timegm(hour_start_dt.timetuple())
    with _PERF_LOG_LOCK:
        hour_records = [r for r in _PERF_LOG if r["ts"] >= hour_start]
    if hour_records:
        with _PERF_ROLLUPS_LOCK:
            _PERF_ROLLUPS[current_hour] = _perf_compute_rollup(hour_records)

def _perf_load_historical_rollups():
    """Load last 7 days of rollups from DynamoDB on startup."""
    try:
        start_key = (dt.datetime.utcnow() - dt.timedelta(days=7)).strftime("%Y-%m-%dT%H")
        items_loaded = 0
        kwargs = {
            "TableName": CONFIG_TABLE,
            "KeyConditionExpression": "PK = :pk AND SK >= :start",
            "ExpressionAttributeValues": {
                ":pk": {"S": "CONFIG#perf-rollup"},
                ":start": {"S": start_key},
            },
        }
        while True:
            resp = ddb.query(**kwargs)
            for item in resp.get("Items", []):
                sk = item.get("SK", {}).get("S", "")
                data_str = item.get("Data", {}).get("S")
                if sk and data_str:
                    try:
                        _PERF_ROLLUPS[sk] = json.loads(data_str)
                        items_loaded += 1
                    except Exception:
                        pass
            if "LastEvaluatedKey" not in resp:
                break
            kwargs["ExclusiveStartKey"] = resp["LastEvaluatedKey"]
        print(f"[PERF] Loaded {items_loaded} historical rollup hours")
    except Exception as e:
        print(f"[PERF] Error loading historical rollups: {e}")

# Load historical rollups on startup
try:
    _perf_load_historical_rollups()
except Exception:
    pass

@app.middleware("http")
async def perf_timing_middleware(request: Request, call_next):
    """Record request timing for performance monitoring."""
    path = request.url.path
    if any(path.startswith(p) for p in _PERF_SKIP_PREFIXES):
        return await call_next(request)
    start = time.time()
    response = await call_next(request)
    elapsed_ms = (time.time() - start) * 1000
    # Extract user from cookie (lightweight, no DB call)
    user = ""
    try:
        user = get_current_user(request) or ""
    except Exception:
        pass
    _perf_record(path, request.method, response.status_code, elapsed_ms, user)
    # Check for hour transition
    _perf_maybe_persist_hour()
    # Add Server-Timing header for browser DevTools
    response.headers["Server-Timing"] = f"total;dur={elapsed_ms:.1f}"
    return response

base_dir = os.path.dirname(__file__)
templates = Jinja2Templates(directory=os.path.join(base_dir, "templates"))

# simple in-memory cache
_CACHE: dict = {}
CACHE_TTL_SECONDS = 300  # 5 minutes for today's data
CACHE_TTL_PAST_DAYS = 3600  # 1 hour for past days (they change less frequently)

def _get_cache_ttl(y: str, m: str, d: str) -> int:
    """Return appropriate cache TTL - shorter for today, longer for past days."""
    try:
        today = dt.date.today()
        check_date = dt.date(int(y), int(m), int(d))
        if check_date >= today:
            return CACHE_TTL_SECONDS  # Today or future: short TTL
        return CACHE_TTL_PAST_DAYS  # Past days: longer TTL
    except Exception:
        return CACHE_TTL_SECONDS

def invalidate_day_cache(y: str, m: str, d: str):
    try:
        _CACHE.pop(("load_day", y, m, d), None)
    except Exception:
        pass

# -------- Submitter Stats Cache --------
# Cache submitter stats to avoid timeout on expensive DynamoDB scans
_SUBMITTER_STATS_CACHE: Dict[str, Any] = {}  # key -> {"data": {...}, "ts": datetime}
_SUBMITTER_STATS_TTL = 300  # 5 minutes

# -------- Activity Detail Cache --------
# Cache activity detail to avoid timeout on expensive scans
_ACTIVITY_DETAIL_CACHE: Dict[str, Any] = {}  # key -> {"data": {...}, "ts": datetime}
_ACTIVITY_DETAIL_TTL = 300  # 5 minutes

# -------- Week Over Week Stats Cache --------
# Cache week-over-week team stats (persisted to DynamoDB for historical data)
_WEEK_OVER_WEEK_CACHE: Dict[str, Any] = {}  # key -> {"data": {...}, "ts": datetime}
_WEEK_OVER_WEEK_TTL = 600  # 10 minutes (weekly data doesn't change often)
WEEKLY_ROLLUP_PK = "WEEKLY_ROLLUP"  # DynamoDB PK for weekly stats

# Late fee detection patterns (for tracking, not deletion)
LATE_FEE_PATTERNS = [
    r"late\s*(fee|charge|penalty)",
    r"penalty\s*(fee|charge)",
    r"past\s*due\s*(fee|charge)",
    r"delinquent\s*(fee|charge)",
    r"collection\s*(fee|charge)",
]


def _get_cached_week_rollup(week_start: str) -> dict | None:
    """Get cached weekly rollup from DynamoDB.

    Args:
        week_start: Week start date in YYYY-MM-DD format

    Returns:
        Cached stats dict or None if not found
    """
    try:
        sk = f"WEEK#{week_start}"
        resp = ddb.get_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": WEEKLY_ROLLUP_PK}, "SK": {"S": sk}}
        )
        if "Item" not in resp:
            return None

        item = resp["Item"]
        return {
            "week_start": item.get("week_start", {}).get("S", ""),
            "week_end": item.get("week_end", {}).get("S", ""),
            "label": item.get("label", {}).get("S", ""),
            "invoices": int(item.get("invoices", {}).get("N", 0)),
            "lines": int(item.get("lines", {}).get("N", 0)),
            "dollars": float(item.get("dollars", {}).get("N", 0)),
            "late_fees": float(item.get("late_fees", {}).get("N", 0)),
            "by_submitter": json.loads(item.get("by_submitter", {}).get("S", "{}")),
            "cached_at": item.get("cached_at", {}).get("S", ""),
        }
    except Exception as e:
        print(f"[WEEK_ROLLUP] Error reading cache for {week_start}: {e}")
        return None


def _save_week_rollup(stats: dict) -> bool:
    """Save weekly rollup to DynamoDB for permanent caching.

    Args:
        stats: Week stats dict with week_start, invoices, lines, dollars, late_fees, by_submitter

    Returns:
        True if saved successfully
    """
    try:
        week_start = stats.get("week_start", "")
        if not week_start:
            return False

        sk = f"WEEK#{week_start}"
        item = {
            "PK": {"S": WEEKLY_ROLLUP_PK},
            "SK": {"S": sk},
            "week_start": {"S": stats.get("week_start", "")},
            "week_end": {"S": stats.get("week_end", "")},
            "label": {"S": stats.get("label", "")},
            "invoices": {"N": str(stats.get("invoices", 0))},
            "lines": {"N": str(stats.get("lines", 0))},
            "dollars": {"N": str(stats.get("dollars", 0))},
            "late_fees": {"N": str(stats.get("late_fees", 0))},
            "by_submitter": {"S": json.dumps(stats.get("by_submitter", {}))},
            "cached_at": {"S": datetime.now(timezone.utc).isoformat()},
        }
        ddb.put_item(TableName=CONFIG_TABLE, Item=item)
        print(f"[WEEK_ROLLUP] Saved rollup for week {week_start}")
        return True
    except Exception as e:
        print(f"[WEEK_ROLLUP] Error saving cache for {stats.get('week_start', '?')}: {e}")
        return False

# -------- UBI Exclusion Hash Cache --------
# Cache exclusion hashes (Stage 8 + Stage 99) to avoid re-scanning on every request
_EXCLUSION_HASH_CACHE = {
    "hashes": set(),
    "last_refresh": None,
    "ttl_seconds": 300  # 5 minutes
}

# -------- PRINT CHECKS Posted Invoices Cache --------
# Cache posted invoices to avoid scanning S3 on every request
_PRINT_CHECKS_CACHE = {
    "invoices": [],  # List of invoice dicts
    "last_refresh": None,
    "ttl_seconds": 600  # 10 MINUTES - use Refresh button to force update
}

# Cache for invoices already in check slips (DynamoDB scan is slow)
_CHECK_SLIP_INVOICES_CACHE = {
    "pdf_ids": set(),
    "last_refresh": None,
    "ttl_seconds": 300  # 5 minutes
}

# Cache for vendor codes (rarely changes)
_VENDOR_CODE_CACHE = {
    "map": {},
    "last_refresh": None,
    "ttl_seconds": 3600  # 1 hour
}

def _get_cached_vendor_codes():
    """Get vendor code map from cache or S3."""
    cache = _VENDOR_CODE_CACHE
    now = dt.datetime.now()
    if cache["last_refresh"] and (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"] and cache["map"]:
        return cache["map"]
    vendor_code_map = {}
    try:
        vend_cache_obj = s3.get_object(Bucket="api-vendor", Key="vendors/latest.json")
        vend_cache_data = json.loads(vend_cache_obj["Body"].read().decode("utf-8"))
        for v in vend_cache_data.get("vendors", []):
            vid = str(v.get("vendorId", "")).strip()
            vcode = str(v.get("vendorCode", "")).strip()
            if vid and vcode:
                vendor_code_map[vid] = vcode
        cache["map"] = vendor_code_map
        cache["last_refresh"] = now
    except Exception as e:
        print(f"[PRINT CHECKS] Vendor cache load error: {e}")
    return vendor_code_map

def _get_cached_invoices_in_slips(force_refresh=False):
    """Get pdf_ids already in check slips from cache or DynamoDB."""
    cache = _CHECK_SLIP_INVOICES_CACHE
    now = dt.datetime.now()
    if not force_refresh and cache["last_refresh"] and (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"]:
        return cache["pdf_ids"]
    pdf_ids = _ddb_get_invoices_in_check_slips()
    cache["pdf_ids"] = pdf_ids
    cache["last_refresh"] = now
    return pdf_ids

def _invalidate_print_checks_cache():
    """Invalidate the print checks cache (call after creating check slips)."""
    _PRINT_CHECKS_CACHE["last_refresh"] = None
    _PRINT_CHECKS_CACHE["invoices"] = []
    _CHECK_SLIP_INVOICES_CACHE["last_refresh"] = None
    _CHECK_SLIP_INVOICES_CACHE["pdf_ids"] = set()
    print("[PRINT CHECKS] All caches invalidated")

# -------- Vendor-Property / Vendor-GL Historical Pair Cache --------
# Scans Stage 7 + Historical Archive for the past year to build sets of
# (vendor_id, property_id) and (vendor_id, gl_code) pairs that have been posted.
_VENDOR_PAIR_CACHE = {
    "vendor_property": set(),   # set of (vendor_id, property_id) tuples
    "vendor_gl": set(),          # set of (vendor_id, gl_code) tuples
    "last_refresh": None,
    "ttl_seconds": 3600,         # 1 hour
    "scan_succeeded": False,     # tracks if last scan got real data
}
import threading
_VENDOR_PAIR_LOCK = threading.Lock()

# Precomputed INVOICES_MAT cache for instant accrual modal loads
_INVOICE_HISTORY_CACHE = {
    "data": {},           # {property_code: {vendor_name_lower: [{"month": "2025-01", "amount": float, "gl_account": str, "gl_name": str, "vendor_raw": str, "line_count": int}]}}
    "account_data": {},   # {property_code: {account_number: [{"month": "2025-01", "amount": float, "gl_account": str, "gl_name": str, "vendor_raw": str, "line_count": int}]}}
    "prop_code_map": {},  # {property_id: property_code}  from dim_property
    "vendor_index": {},   # {property_code: [vendor_name_lower, ...]}  for regex scanning
    "last_refresh": None,
    "ttl_seconds": 7200,  # 2 hours (mat view updates daily)
    "loading": False,
}
_INVOICE_HISTORY_LOCK = threading.Lock()

# -------- Precomputed Search Index --------
# In-memory index of all Stage 4 invoice metadata for instant advanced search
_SEARCH_INDEX = {
    "entries": [],           # [{pdf_id, date, account, vendor, property, amount, s3_key}, ...]
    "by_date": {},           # {date_str: [entry_indices...]} for incremental refresh
    "dates_indexed": set(),  # which "y-m-d" dates have been indexed
    "ready": False,          # True once initial backfill is done
    "loading": False,
    "last_refresh": None,
    "entry_count": 0,
}
_SEARCH_INDEX_LOCK = threading.Lock()

def _scan_historical_pairs_for_prefix(prefix: str, start_date, end_date):
    """Scan a single S3 prefix (Stage 7 or Archive) for vendor-property and vendor-GL pairs."""
    vp_pairs = set()
    vg_pairs = set()

    # Build month-level prefixes for efficiency (same pattern as report endpoint)
    month_prefixes = []
    current = start_date.replace(day=1)
    while current <= end_date:
        month_prefixes.append(f"{prefix}yyyy={current.year}/mm={current.month:02d}/")
        if current.month == 12:
            current = current.replace(year=current.year + 1, month=1)
        else:
            current = current.replace(month=current.month + 1)

    def _list_and_parse(month_prefix):
        """List all JSONL files under a month prefix and extract pairs."""
        local_vp = set()
        local_vg = set()
        try:
            paginator = s3.get_paginator('list_objects_v2')
            keys = []
            for page in paginator.paginate(Bucket=BUCKET, Prefix=month_prefix):
                for obj in page.get('Contents', []):
                    k = obj['Key']
                    if k.endswith('.jsonl'):
                        keys.append(k)

            for key in keys:
                try:
                    resp = s3.get_object(Bucket=BUCKET, Key=key)
                    content = resp['Body'].read().decode('utf-8')
                    for line in content.strip().split('\n'):
                        if not line.strip():
                            continue
                        try:
                            rec = json.loads(line)
                            vendor_id = str(rec.get("EnrichedVendorID") or "").strip()
                            prop_id = str(rec.get("EnrichedPropertyID") or "").strip()
                            gl_code = str(rec.get("EnrichedGLAccountNumber") or "").strip()
                            if vendor_id and prop_id:
                                local_vp.add((vendor_id, prop_id))
                            if vendor_id and gl_code:
                                local_vg.add((vendor_id, gl_code))
                        except Exception:
                            pass
                except Exception:
                    pass
        except Exception:
            pass
        return local_vp, local_vg

    # Parallel scan across months
    futures = [_GLOBAL_EXECUTOR.submit(_list_and_parse, mp) for mp in month_prefixes]
    for f in futures:
        try:
            local_vp, local_vg = f.result(timeout=120)
            vp_pairs.update(local_vp)
            vg_pairs.update(local_vg)
        except Exception:
            pass

    return vp_pairs, vg_pairs


def _get_cached_vendor_pairs(force_refresh=False):
    """Get vendor-property and vendor-GL pair sets from cache or scan S3.
    Thread-safe with stampede protection. Returns stale cache on scan failure."""
    cache = _VENDOR_PAIR_CACHE
    now = dt.datetime.now()
    if not force_refresh and cache["last_refresh"] and (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"]:
        return cache["vendor_property"], cache["vendor_gl"]

    # Use lock to prevent concurrent scans (stampede protection)
    acquired = _VENDOR_PAIR_LOCK.acquire(blocking=False)
    if not acquired:
        # Another thread is already refreshing; return current cache (may be stale)
        print("[VENDOR PAIRS] Scan already in progress, returning current cache")
        return cache["vendor_property"], cache["vendor_gl"]

    try:
        # Double-check after acquiring lock (another thread may have just finished)
        if not force_refresh and cache["last_refresh"] and (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"]:
            return cache["vendor_property"], cache["vendor_gl"]

        print("[VENDOR PAIRS] Scanning Stage 7 + Archive for historical pairs...")
        end_date = dt.date.today()
        start_date = end_date - dt.timedelta(days=365)

        vp_all = set()
        vg_all = set()
        prefixes_succeeded = 0
        prefixes_attempted = 2

        # Scan both Stage 7 and Historical Archive in parallel
        stage7_future = _GLOBAL_EXECUTOR.submit(_scan_historical_pairs_for_prefix, POST_ENTRATA_PREFIX, start_date, end_date)
        archive_future = _GLOBAL_EXECUTOR.submit(_scan_historical_pairs_for_prefix, HIST_ARCHIVE_PREFIX, start_date, end_date)

        for future in [stage7_future, archive_future]:
            try:
                vp, vg = future.result(timeout=300)
                vp_all.update(vp)
                vg_all.update(vg)
                prefixes_succeeded += 1
            except Exception as e:
                print(f"[VENDOR PAIRS] Scan error: {e}")

        # If no prefixes succeeded at all, keep stale cache
        if prefixes_succeeded == 0:
            print("[VENDOR PAIRS] All scans failed; keeping stale cache to avoid false positives")
            cache["last_refresh"] = now
            return cache["vendor_property"], cache["vendor_gl"]

        # If partial scan (some prefixes failed) and we got fewer pairs than before,
        # keep stale cache to avoid false positives from missing data
        if prefixes_succeeded < prefixes_attempted and cache["scan_succeeded"]:
            old_vp_count = len(cache["vendor_property"])
            old_vg_count = len(cache["vendor_gl"])
            if len(vp_all) < old_vp_count or len(vg_all) < old_vg_count:
                print(f"[VENDOR PAIRS] Partial scan returned fewer pairs (VP: {len(vp_all)} vs {old_vp_count}, "
                      f"VG: {len(vg_all)} vs {old_vg_count}); keeping stale cache")
                cache["last_refresh"] = now
                return cache["vendor_property"], cache["vendor_gl"]

        # Apply overrides from DynamoDB config
        vp_overrides = _ddb_get_config("vendor-property-overrides") or []
        for ov in vp_overrides:
            vid = str(ov.get("vendor_id") or "").strip()
            pid = str(ov.get("property_id") or "").strip()
            if vid and pid:
                if ov.get("action") == "allow":
                    vp_all.add((vid, pid))
                elif ov.get("action") == "block":
                    vp_all.discard((vid, pid))

        vg_overrides = _ddb_get_config("vendor-gl-overrides") or []
        for ov in vg_overrides:
            vid = str(ov.get("vendor_id") or "").strip()
            gl = str(ov.get("gl_code") or "").strip()
            if vid and gl:
                if ov.get("action") == "allow":
                    vg_all.add((vid, gl))
                elif ov.get("action") == "block":
                    vg_all.discard((vid, gl))

        cache["vendor_property"] = vp_all
        cache["vendor_gl"] = vg_all
        cache["last_refresh"] = now
        cache["scan_succeeded"] = True
        print(f"[VENDOR PAIRS] Loaded {len(vp_all)} vendor-property pairs, {len(vg_all)} vendor-GL pairs")
        return vp_all, vg_all
    finally:
        _VENDOR_PAIR_LOCK.release()


def _get_cached_exclusion_hashes(days_back: int = 90) -> set:
    """Get cached exclusion hashes from DynamoDB.

    Source: DynamoDB table jrk-bill-ubi-assignments only.
    The archived table is for historical record only, not exclusion.
    This allows unassigned items to properly reappear in the queue.
    """
    from datetime import datetime

    cache = _EXCLUSION_HASH_CACHE
    now = datetime.now()

    # Check if cache is still valid
    if cache["last_refresh"] and (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"]:
        return cache["hashes"]

    print(f"[UBI EXCLUSION CACHE] Loading exclusion hashes...")
    import time as _time
    t0 = _time.time()

    hashes = set()

    # Load line_hash values from assignments table only (not archived - that's for history)
    tables_to_scan = ['jrk-bill-ubi-assignments']
    for table_name in tables_to_scan:
        try:
            ddb_paginator = ddb.get_paginator('scan')
            for page in ddb_paginator.paginate(
                TableName=table_name,
                ProjectionExpression='line_hash'
            ):
                for item in page.get('Items', []):
                    if 'line_hash' in item and 'S' in item['line_hash']:
                        hashes.add(item['line_hash']['S'])
            print(f"[UBI EXCLUSION CACHE] After {table_name}: {len(hashes)} unique hashes")
        except Exception as e:
            print(f"[UBI EXCLUSION CACHE] Error loading from {table_name}: {e}")

    elapsed = _time.time() - t0
    print(f"[UBI EXCLUSION CACHE] Total: {len(hashes)} exclusion hashes in {elapsed:.1f}s")

    # Update cache
    cache["hashes"] = hashes
    cache["last_refresh"] = now

    return hashes

def invalidate_exclusion_cache():
    """Clear the exclusion hash cache (call after assignments)."""
    _EXCLUSION_HASH_CACHE["last_refresh"] = None
    _EXCLUSION_HASH_CACHE["hashes"] = set()
    print("[UBI EXCLUSION CACHE] Cache invalidated")

def _vendor_pair_refresh_loop():
    """Background loop that keeps the vendor pair cache permanently warm.
    Refreshes every 50 minutes so the 60-minute TTL never expires mid-request."""
    _REFRESH_INTERVAL = 50 * 60  # 50 minutes (well before 60-min TTL)
    # Initial load
    try:
        _get_cached_vendor_pairs()
        print("[VENDOR PAIRS BG] Initial cache load complete")
    except Exception as e:
        print(f"[VENDOR PAIRS BG] Initial load error: {e}")
    while True:
        time.sleep(_REFRESH_INTERVAL)
        try:
            _get_cached_vendor_pairs(force_refresh=True)
            print("[VENDOR PAIRS BG] Proactive refresh complete")
        except Exception as e:
            print(f"[VENDOR PAIRS BG] Refresh error (will retry next cycle): {e}")


@app.on_event("startup")
async def startup_prewarm_caches():
    """Pre-warm caches on startup so first request is fast."""
    import threading

    def prewarm():
        print("[STARTUP] Pre-warming exclusion hash cache in background...")
        try:
            _get_cached_exclusion_hashes(90)
            print("[STARTUP] Exclusion hash cache pre-warmed successfully")
        except Exception as e:
            print(f"[STARTUP] Error pre-warming exclusion cache: {e}")

    def prewarm_invoice_cache():
        print("[STARTUP] Pre-warming invoice history cache...")
        try:
            _load_invoice_history_cache()
            print("[STARTUP] Invoice history cache loaded")
        except Exception as e:
            print(f"[STARTUP] Error loading invoice history: {e}")

    # Vendor pair cache runs in a permanent background thread that refreshes proactively
    threading.Thread(target=_vendor_pair_refresh_loop, daemon=True, name="vendor-pair-refresh").start()
    # Other caches pre-warm once on startup
    threading.Thread(target=prewarm, daemon=True).start()
    threading.Thread(target=prewarm_invoice_cache, daemon=True).start()

    # Pre-warm POST helper caches (GL maps, vendor cache, accounts-to-track)
    def prewarm_post_helpers():
        print("[STARTUP] Pre-warming POST helper caches...")
        try:
            _POST_HELPER_CACHE["vendor_cache"] = {"ts": time.time(), "data": load_vendor_cache()}
            print("[STARTUP] Vendor cache warmed")
        except Exception as e:
            print(f"[STARTUP] Vendor cache warm failed: {e}")
        try:
            _load_gl_number_to_id_map()
            print("[STARTUP] GL number-to-id map warmed")
        except Exception as e:
            print(f"[STARTUP] GL number map warm failed: {e}")
        try:
            _load_gl_name_to_id_map()
            print("[STARTUP] GL name-to-id map warmed")
        except Exception as e:
            print(f"[STARTUP] GL name map warm failed: {e}")
        try:
            _index_accounts_to_track_by_key()
            print("[STARTUP] Accounts-to-track index warmed")
        except Exception as e:
            print(f"[STARTUP] Accounts-to-track warm failed: {e}")
        print("[STARTUP] POST helper caches ready")
    threading.Thread(target=prewarm_post_helpers, daemon=True, name="post-helper-prewarm").start()

    # Background refresh thread for invoice history cache (every 2 hours)
    def _invoice_history_refresh_loop():
        while True:
            time.sleep(7200)
            try:
                _load_invoice_history_cache()
            except Exception:
                pass
    threading.Thread(target=_invoice_history_refresh_loop, daemon=True).start()

    # Search index: load from S3 (instant), then index only new dates
    def _search_index_backfill():
        print("[STARTUP] Loading search index from S3...")
        loaded = False
        try:
            loaded = _load_search_index_from_s3()
        except Exception as e:
            print(f"[STARTUP] Search index S3 load error: {e}")
        if loaded:
            # Index is ready from S3, just pick up any new dates
            print("[STARTUP] Catching up new dates...")
            try:
                _build_search_index(force_full=False)
            except Exception as e:
                print(f"[STARTUP] Search index incremental error: {e}")
        else:
            # No persisted index, do full backfill
            print("[STARTUP] No persisted index, doing full backfill...")
            try:
                _build_search_index(force_full=True)
            except Exception as e:
                print(f"[STARTUP] Search index backfill error: {e}")

    def _search_index_refresh_loop():
        # Wait for backfill to finish first
        while not _SEARCH_INDEX["ready"]:
            time.sleep(5)
        while True:
            time.sleep(300)  # 5 minutes
            try:
                _build_search_index(force_full=False)
            except Exception:
                pass

    threading.Thread(target=_search_index_backfill, daemon=True).start()
    threading.Thread(target=_search_index_refresh_loop, daemon=True).start()

    # Workflow completion tracker  keep cache permanently warm
    threading.Thread(target=_workflow_tracker_refresh_loop, daemon=True, name="workflow-tracker-refresh").start()

# -------- VACANT GL DESC Helpers --------
# GL codes that trigger the special VACANT description format
VACANT_GL_CODES = {
    "5705-0000": "VE",  # VACANT ELEC
    "5715-0000": "VG",  # VACANT GAS
    "5720-1000": "VW",  # VACANT WATER
    "5721-1000": "VS",  # VACANT SEWER
}
VACANT_NAME_CODES = {
    "VACANT ELEC": "VE",
    "VACANT ELECTRIC": "VE",
    "VACANT GAS": "VG",
    "VACANT WATER": "VW",
    "VACANT SEWER": "VS",
}

def _parse_service_address(addr: str) -> tuple[str, str, str]:
    """Parse service address into (street_num, street_letter, unit).

    Examples:
        "9436 North St APT 159" -> ("9436", "N", "159")
        "728 Franklin Ave Unit F316" -> ("728", "F", "F316")
        "123 Main Street #4A" -> ("123", "M", "4A")
    """
    import re
    addr = (addr or "").strip().upper()

    # Extract street number (first numeric sequence)
    street_num = ""
    num_match = re.match(r'^(\d+)', addr)
    if num_match:
        street_num = num_match.group(1)

    # Extract first letter of street name (word after street number)
    street_letter = ""
    # Skip the number and find the next word
    after_num = re.sub(r'^\d+\s*', '', addr)
    word_match = re.match(r'([A-Z])', after_num)
    if word_match:
        street_letter = word_match.group(1)

    # Extract unit number (after APT, AP, UNIT, #, STE, SUITE, APARTMENT, BLDG)
    # NOTE: # must be outside \b group because \b doesn't work with non-word chars
    # NOTE: APARTMENT must come before APT/AP to avoid partial matches
    unit = ""
    unit_match = re.search(r'(?:\b(?:APARTMENT|APT|AP|SUITE|STE|UNIT|BLDG)|#)\.?\s*([A-Z0-9-]+)', addr, re.I)
    if unit_match:
        unit = unit_match.group(1)

    return (street_num, street_letter, unit)

def _format_date_compact(date_str: str) -> str:
    """Convert date to compact M/D/YY format.

    Examples:
        "07/24/2025" -> "7/24/25"
        "2025-08-21" -> "8/21/25"
        "7/24/25" -> "7/24/25" (already compact)
    """
    if not date_str:
        return ""
    date_str = str(date_str).strip()

    # Try various date formats
    for fmt in ("%m/%d/%Y", "%Y-%m-%d", "%m/%d/%y", "%Y/%m/%d"):
        try:
            dt_obj = datetime.strptime(date_str, fmt)
            return f"{dt_obj.month}/{dt_obj.day}/{dt_obj.strftime('%y')}"
        except ValueError:
            continue

    # If we can't parse, return as-is
    return date_str

def _get_vacant_code(gl_num: str, gl_name: str) -> str | None:
    """Return the VACANT utility code (VE, VG, VW, VS) if this is a VACANT GL, else None."""
    gl_num = (gl_num or "").strip()
    gl_name = (gl_name or "").strip().upper()

    # Check by GL number first
    if gl_num in VACANT_GL_CODES:
        return VACANT_GL_CODES[gl_num]

    # Check by GL name
    for name, code in VACANT_NAME_CODES.items():
        if name in gl_name:
            return code

    return None

def _build_vacant_gl_desc(rec: dict) -> str:
    """Build the special VACANT GL DESC format.

    Format: (M/D/YY-M/D/YY V[E/G/W/S] Street#Letter@Unit[!])
    Example: (7/24/25-8/21/25 VE 9436N@159)

    Lines with credits/transfers get ! at the end:
    - Balance Transfer, Credit Balance, CA Climate Credit, Credit Adjustment/Transfer
    """
    gl_num = str(rec.get("EnrichedGLAccountNumber") or "").strip()
    gl_name = str(rec.get("EnrichedGLAccountName") or "").strip()

    vacant_code = _get_vacant_code(gl_num, gl_name)
    if not vacant_code:
        return None  # Not a VACANT GL

    # Parse dates
    bps = _format_date_compact(rec.get("Bill Period Start") or "")
    bpe = _format_date_compact(rec.get("Bill Period End") or "")
    date_range = f"{bps}-{bpe}" if (bps or bpe) else ""

    # Parse service address
    addr = str(rec.get("Service Address") or "").strip()
    street_num, street_letter, unit = _parse_service_address(addr)
    addr_part = f"{street_num}{street_letter}@{unit}"

    # Check for credit/transfer line items that get ! suffix
    line_desc = str(rec.get("Line Item Description") or "").strip().upper()
    credit_keywords = [
        "BALANCE TRANSFER",
        "CREDIT BALANCE",
        "CA CLIMATE CREDIT",
        "CLIMATE CREDIT",
        "CREDIT ADJUSTMENT",
        "CREDIT TRANSFER",
    ]
    suffix = "!" if any(kw in line_desc for kw in credit_keywords) else ""

    return f"({date_range} {vacant_code} {addr_part}{suffix})"

# Entrata prototype import
try:
    from .entrata_send_invoices_prototype import (
        build_send_invoices_payload,
        load_vendor_cache,
        do_post,
    )
except Exception:
    # Support running as script
    from entrata_send_invoices_prototype import (
        build_send_invoices_payload,
        load_vendor_cache,
        do_post,
    )

# -------- Auth helpers --------
signer = TimestampSigner(APP_SECRET)

def set_session(resp: Response, username: str):
    token = signer.sign(username.encode("utf-8")).decode("utf-8")
    resp.set_cookie(SESSION_COOKIE, token, max_age=SESSION_MAX_AGE_SECONDS, httponly=True, secure=SECURE_COOKIES, samesite="lax")

def get_current_user(request: Request) -> str | None:
    # Emergency auth bypass - ONLY works when BOTH conditions are met:
    # 1. DISABLE_AUTH=1
    # 2. DISABLE_AUTH_SECRET matches a confirmation phrase (prevents accidental enable)
    # This should NEVER be used in production - only for emergency recovery
    if os.getenv("DISABLE_AUTH", "0") == "1":
        # Require explicit confirmation secret to prevent accidental bypass
        confirm_secret = os.getenv("DISABLE_AUTH_SECRET", "")
        if confirm_secret == "I-UNDERSTAND-THIS-IS-INSECURE":
            print(f"[SECURITY WARNING] Auth bypass active - request from {request.client.host if request.client else 'unknown'}")
            return os.getenv("ADMIN_USER", "admin")
        # If DISABLE_AUTH=1 but secret doesn't match, log and continue with normal auth
        print("[SECURITY] DISABLE_AUTH=1 but DISABLE_AUTH_SECRET not set correctly - auth bypass NOT active")
    token = request.cookies.get(SESSION_COOKIE)
    if not token:
        return None
    try:
        raw = signer.unsign(token, max_age=SESSION_MAX_AGE_SECONDS)
        return raw.decode("utf-8")
    except BadSignature:
        return None

def require_user(request: Request) -> str:
    user = get_current_user(request)
    if not user:
        # Raise 307 to login
        from fastapi import HTTPException
        raise HTTPException(status_code=307, detail="redirect", headers={"Location": "/login"})
    return user


def require_admin(request: Request) -> str:
    """Dependency that requires both authentication AND admin privileges."""
    user = get_current_user(request)
    if not user:
        from fastapi import HTTPException
        raise HTTPException(status_code=307, detail="redirect", headers={"Location": "/login"})
    if user not in ADMIN_USERS:
        from fastapi import HTTPException
        raise HTTPException(status_code=403, detail="Admin access required")
    return user

# -------- Helpers --------

def list_dates() -> List[Dict[str, Any]]:
    # Cache list_dates to avoid expensive S3 pagination on every parse page load
    cache_key = ("list_dates",)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        return ent.get("data", [])

    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=BUCKET, Prefix=ENRICH_PREFIX)
    seen = set()
    for page in pages:
        for obj in page.get("Contents", []):
            key = obj["Key"]
            parts = key.split("/")
            try:
                y = next(p for p in parts if p.startswith("yyyy="))[5:]
                m = next(p for p in parts if p.startswith("mm="))[3:]
                d = next(p for p in parts if p.startswith("dd="))[3:]
                seen.add((y, m, d))
            except StopIteration:
                continue
    dates = sorted([{ "label": f"{y}-{m}-{d}", "tuple": (y,m,d) } for (y,m,d) in seen], key=lambda x: x["label"], reverse=True)

    _CACHE[cache_key] = {"ts": now, "data": dates}
    return dates


def _fetch_s3_file(key: str) -> List[Dict[str, Any]]:
    """Fetch a single S3 JSONL file and parse it. Used for parallel loading."""
    try:
        body = s3.get_object(Bucket=BUCKET, Key=key)["Body"].read().decode("utf-8", errors="ignore")
        rows = []
        for idx, line in enumerate(body.splitlines()):
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                rec["__s3_key__"] = key
                rec["__row_idx__"] = idx
                rec["__id__"] = f"{key}#{idx}"
                rows.append(rec)
            except Exception:
                continue
        return rows
    except Exception:
        return []


def load_day(y: str, m: str, d: str, force_refresh: bool = False) -> List[Dict[str, Any]]:
    # cached by date - use longer TTL for past days
    _k = ("load_day", y, m, d)
    now = time.time()
    ent = _CACHE.get(_k)
    ttl = _get_cache_ttl(y, m, d)
    if not force_refresh and ent and (now - ent.get("ts", 0) < ttl):
        return ent.get("data", [])
    prefix = f"{ENRICH_PREFIX}yyyy={y}/mm={m}/dd={d}/"
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)

    # Collect all keys first
    keys = []
    for page in pages:
        for obj in page.get("Contents", []):
            key = obj["Key"]
            if key.lower().endswith(".jsonl"):
                keys.append(key)

    # Fetch files in parallel (up to 50 concurrent requests)
    rows: List[Dict[str, Any]] = []
    if keys:
        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = {executor.submit(_fetch_s3_file, key): key for key in keys}
            for future in as_completed(futures):
                try:
                    file_rows = future.result()
                    rows.extend(file_rows)
                except Exception:
                    pass

    _CACHE[_k] = {"ts": now, "data": rows}
    return rows


def pdf_id_from_key(key: str) -> str:
    return hashlib.sha1(key.encode("utf-8")).hexdigest()


def line_id_from(key: str, idx: int) -> str:
    return f"{pdf_id_from_key(key)}#{idx}"


def write_overrides(y: str, m: str, d: str, overrides: List[Dict[str, Any]]) -> str | None:
    if not overrides:
        return None
    ts = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    out_prefix = f"{OVERRIDE_PREFIX}yyyy={y}/mm={m}/dd={d}/"
    out_key = f"{out_prefix}overrides_{ts}.jsonl"
    body = "\n".join(json.dumps(o, ensure_ascii=False) for o in overrides) + "\n"
    s3.put_object(Bucket=BUCKET, Key=out_key, Body=body.encode("utf-8"), ContentType="application/x-ndjson")
    return out_key


def _clean_account_number(acct: str) -> str:
    """Strip dashes, spaces, and other punctuation from account numbers to prevent duplicates."""
    if not acct:
        return acct
    return re.sub(r'[-\s\.\(\)]', '', str(acct).strip())


def _write_jsonl(prefix: str, y: str, m: str, d: str, basename: str, rows: List[Dict[str, Any]]) -> str:
    ts = dt.datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    out_prefix = f"{prefix}yyyy={y}/mm={m}/dd={d}/"
    out_key = f"{out_prefix}{basename}_{ts}.jsonl"
    # Clean account numbers: strip dashes and other punctuation to prevent duplicates
    for r in rows:
        for acct_field in ("Account Number", "AccountNumber", "Line Item Account Number"):
            if acct_field in r and r[acct_field]:
                r[acct_field] = _clean_account_number(r[acct_field])
    body = "\n".join(json.dumps(r, ensure_ascii=False) for r in rows) + "\n"
    s3.put_object(Bucket=BUCKET, Key=out_key, Body=body.encode('utf-8'), ContentType='application/x-ndjson')
    return out_key


def put_status(id_: str, status: str, user: str):
    now_iso = dt.datetime.utcnow().isoformat()
    item = {
        "pk": {"S": id_},
        "status": {"S": status},
        "updated_by": {"S": user},
        "updated_utc": {"S": now_iso}
    }
    # Add submitted_at timestamp when marking as Submitted
    if status == "Submitted":
        item["submitted_at"] = {"S": now_iso}
    ddb.put_item(TableName=REVIEW_TABLE, Item=item)


def get_draft(pdf_id: str, line_id: str, user: str) -> Dict[str, Any] | None:
    pk = f"draft#{pdf_id}#{line_id}#{user}"
    resp = ddb.get_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
    item = resp.get("Item")
    if not item:
        return None
    out = {k: list(v.values())[0] for k, v in item.items()}
    if "fields" in out:
        try:
            out["fields"] = json.loads(out["fields"]) if isinstance(out["fields"], str) else out["fields"]
        except Exception:
            out["fields"] = {}
    return out


def get_header_drafts_batch(pdf_ids: List[str], user: str) -> Dict[str, Dict[str, Any]]:
    """Fetch header drafts for multiple pdf_ids in parallel. Returns {pdf_id: draft_fields}."""
    if not pdf_ids:
        return {}

    results: Dict[str, Dict[str, Any]] = {}

    def fetch_one(pid: str) -> tuple:
        pk = f"draft#{pid}#__header__#{user}"
        try:
            resp = ddb.get_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
            item = resp.get("Item")
            if not item:
                return pid, None
            out = {k: list(v.values())[0] for k, v in item.items()}
            if "fields" in out:
                try:
                    out["fields"] = json.loads(out["fields"]) if isinstance(out["fields"], str) else out["fields"]
                except Exception:
                    out["fields"] = {}
            return pid, out
        except Exception:
            return pid, None

    with ThreadPoolExecutor(max_workers=30) as executor:
        futures = {executor.submit(fetch_one, pid): pid for pid in pdf_ids}
        for future in as_completed(futures):
            try:
                pid, draft = future.result()
                if draft:
                    results[pid] = draft
            except Exception:
                pass

    return results


def _extract_ymd_from_key(key: str) -> tuple[str, str, str]:
    # Try yyyy=YYYY/mm=MM/dd=DD pattern first
    parts = key.split('/')
    try:
        y = next(p for p in parts if p.startswith('yyyy='))[5:]
        m = next(p for p in parts if p.startswith('mm='))[3:]
        d = next(p for p in parts if p.startswith('dd='))[3:]
        if y and m and d:
            return y, m, d
    except StopIteration:
        pass
    # Try YYYY/MM/DD pattern under stage root
    for i in range(len(parts) - 3):
        if parts[i].isdigit() and len(parts[i]) == 4 and parts[i+1].isdigit() and parts[i+2].isdigit():
            return parts[i], parts[i+1], parts[i+2]
    today = dt.datetime.utcnow()
    return today.strftime('%Y'), today.strftime('%m'), today.strftime('%d')


def _rewrite_status(rows: list[dict], status: str) -> list[dict]:
    out = []
    for r in rows:
        if isinstance(r, dict):
            r2 = dict(r)
            r2['Status'] = status
            out.append(r2)
    return out


def _basename_from_key(key: str) -> str:
    return os.path.basename(key).rsplit('/', 1)[-1].replace('/', '_')


# Fields added by update-line-item that should NOT affect the hash
# These fields are volatile and added after initial parsing
# Fields that identify a bill structurally (whitelist for exclusion matching).
# Only these fields are used when determining if a Stage 7 bill was already
# assigned in Stage 8. This is robust against re-enrichment changing content.
_BILL_IDENTITY_FIELDS = {
    "Property Name", "Property ID", "Property Code", "Property Address",
    "Vendor Name", "Vendor ID",
    "Account Number",
    "Bill Period Start", "Bill Period End",
    "Bill Date",
    "Utility Type", "Bill Type",
    "Meter Number",
}

# Fields excluded from line-level hashing (used for override/unassign).
_VOLATILE_LINE_FIELDS = {
    "Charge Code", "Charge Code Source", "Charge Code Overridden", "Charge Code Override Reason",
    "Mapped Utility Name", "Current Amount", "Amount Overridden", "Amount Override Reason",
    "Is Excluded From UBI", "Exclusion Reason", "is_excluded_from_ubi", "exclusion_reason",
    "ubi_period", "ubi_amount", "ubi_months_total", "ubi_assigned_by", "ubi_assigned_date",
    "ubi_assignments", "ubi_period_count", "ubi_notes",
    "__stage8_key__", "__s3_key__", "__row_idx__", "__id__", "__manual__", "ubi_auto_suggested",
    "PostedBy", "PostedAt", "Status",
}


def _compute_bill_identity_hash(rec: dict) -> str:
    """Compute a bill-level identity hash using only structural fields.

    Used for exclusion matching between Stage 7 and Stage 8. Robust against
    re-enrichment which changes parsed content (rates, consumption, descriptions).
    """
    import hashlib
    stable_rec = {k: v for k, v in rec.items() if k in _BILL_IDENTITY_FIELDS}
    line_data = json.dumps(stable_rec, sort_keys=True)
    return hashlib.sha256(line_data.encode()).hexdigest()


def _compute_stable_line_hash(rec: dict) -> str:
    """Compute a stable hash for a line item, excluding volatile fields.

    Used for line-level identification in override and unassign operations.
    More specific than bill identity hash  distinguishes individual line items.
    """
    import hashlib
    stable_rec = {k: v for k, v in rec.items() if k not in _VOLATILE_LINE_FIELDS}
    line_data = json.dumps(stable_rec, sort_keys=True)
    return hashlib.sha256(line_data.encode()).hexdigest()


def _try_load_pdf_b64(row: dict) -> tuple[str, str, str] | None:
    """Attempt to locate a PDF in S3 using row fields and return (b64, filename, url?)."""
    # Prefer explicit keys
    cand = row.get("__pdf_s3_key__") or row.get("source_input_key") or row.get("PDF_LINK") or row.get("pdfKey") or ""
    if isinstance(cand, str) and cand:
        key = cand
        # Normalize possible absolute URLs back to key
        if key.startswith("http://") or key.startswith("https://"):
            # We could resolve and parse but most of our links are direct S3 key paths; fall back to path part
            try:
                parsed = urlparse(key)
                key = parsed.path.lstrip('/')
            except Exception:
                key = cand
        try:
            obj = s3.get_object(Bucket=BUCKET, Key=key)
            import base64
            raw = obj["Body"].read()
            b64 = base64.b64encode(raw).decode('ascii')
            fname = os.path.basename(key) or "invoice.pdf"
            return (b64, fname, "")
        except Exception:
            return None
    return None


def _archive_posted_pdf(row: dict) -> str | None:
    """Copy PDF to organized folder structure in S3 after successful POST.

    Structure: Posted_Invoices/{PropertyName}/{Year}/{Month}/{Vendor}_{Account}_{ServiceDates}.pdf
    Returns the archive key if successful, None otherwise.
    """
    try:
        # Get PDF S3 key from row
        pdf_key = row.get("__pdf_s3_key__") or row.get("source_input_key") or row.get("PDF_LINK") or row.get("pdfKey") or ""
        if not pdf_key:
            print("[archive_pdf] No PDF key found in row")
            return None

        # Normalize URL to key if needed
        if pdf_key.startswith("http://") or pdf_key.startswith("https://"):
            try:
                parsed = urlparse(pdf_key)
                pdf_key = parsed.path.lstrip('/')
            except Exception:
                pass

        # Extract metadata from row
        property_name = (
            row.get("EnrichedPropertyName") or
            row.get("Property Name") or
            row.get("EnrichedProperty") or
            row.get("PROPERTY") or
            "UnknownProperty"
        ).strip()

        vendor_name = (
            row.get("EnrichedVendorName") or
            row.get("Vendor Name") or
            row.get("VENDOR") or
            row.get("Vendor") or
            "UnknownVendor"
        ).strip()

        account_number = (
            row.get("Account Number") or
            row.get("AccountNumber") or
            "NoAccount"
        ).strip()

        # Date fields
        service_start = (row.get("Bill Period Start") or row.get("billPeriodStart") or "").strip()
        service_end = (row.get("Bill Period End") or row.get("billPeriodEnd") or "").strip()
        bill_date = (row.get("Bill Date") or row.get("billDate") or row.get("Invoice Date") or "").strip()

        # Determine date for folder structure (prefer service start, fall back to bill date)
        date_for_folder = service_start or bill_date
        if date_for_folder:
            dt_parsed = _parse_date_any(date_for_folder)
            if dt_parsed:
                year = dt_parsed.strftime("%Y")
                month = dt_parsed.strftime("%m")
            else:
                year = datetime.now().strftime("%Y")
                month = datetime.now().strftime("%m")
        else:
            year = datetime.now().strftime("%Y")
            month = datetime.now().strftime("%m")

        # Sanitize names for folder/file paths (remove special chars, limit length)
        def _sanitize(s: str, max_len: int = 50) -> str:
            clean = re.sub(r'[^\w\s-]', '', s).strip()
            clean = re.sub(r'\s+', '_', clean)  # Replace spaces with underscores
            return clean[:max_len] if clean else "Unknown"

        safe_property = _sanitize(property_name, 50)
        safe_vendor = _sanitize(vendor_name, 40)
        safe_account = _sanitize(account_number, 30)

        # Build date part for filename
        if service_start and service_end:
            try:
                svc_start_dt = _parse_date_any(service_start)
                svc_end_dt = _parse_date_any(service_end)
                if svc_start_dt and svc_end_dt:
                    date_part = f"{svc_start_dt.strftime('%Y%m%d')}-{svc_end_dt.strftime('%Y%m%d')}"
                else:
                    date_part = datetime.now().strftime("%Y%m%d")
            except Exception:
                date_part = datetime.now().strftime("%Y%m%d")
        elif bill_date:
            try:
                bill_dt = _parse_date_any(bill_date)
                date_part = bill_dt.strftime("%Y%m%d") if bill_dt else datetime.now().strftime("%Y%m%d")
            except Exception:
                date_part = datetime.now().strftime("%Y%m%d")
        else:
            date_part = datetime.now().strftime("%Y%m%d")

        # Filename: Vendor_Account_Dates.pdf
        filename = f"{safe_vendor}_{safe_account}_{date_part}.pdf"

        # Full archive path: Posted_Invoices/PropertyName/2026/01/Vendor_Account_Dates.pdf
        archive_key = f"Posted_Invoices/{safe_property}/{year}/{month}/{filename}"

        # Copy PDF to archive location (same bucket)
        s3.copy_object(
            Bucket=BUCKET,
            CopySource={'Bucket': BUCKET, 'Key': pdf_key},
            Key=archive_key
        )
        print(f"[archive_pdf] Archived PDF to {archive_key}")
        return archive_key

    except Exception as e:
        print(f"[archive_pdf] Failed to archive PDF: {e}")
        return None


@app.post("/api/post/validate")
async def api_post_validate(request: Request, user: str = Depends(require_user)):
    """Validate invoices before posting: check vendor-property and vendor-GL pairs against history."""
    try:
        body = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    keys = body.get("keys", [])
    if not keys:
        return {"warnings": []}

    # Validate S3 keys are within expected prefixes (prevent reading arbitrary bucket paths)
    valid_prefixes = (STAGE6_PREFIX, STAGE4_PREFIX)
    sanitized_keys = [k for k in keys if isinstance(k, str) and any(k.startswith(p) for p in valid_prefixes)]
    if not sanitized_keys:
        return {"warnings": [], "count": 0}
    keys = sanitized_keys

    # Load historical pairs (cached, 1-hour TTL)
    vp_set, vg_set = _get_cached_vendor_pairs()

    # Read all S3 files in parallel
    def _read_key(s3_key):
        try:
            obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
            content = obj['Body'].read().decode('utf-8')
            records = []
            for line in content.strip().split('\n'):
                if not line.strip():
                    continue
                try:
                    records.append(json.loads(line))
                except Exception:
                    pass
            return s3_key, records
        except Exception as e:
            print(f"[POST VALIDATE] Error reading {s3_key}: {e}")
            return s3_key, []

    futures = [_GLOBAL_EXECUTOR.submit(_read_key, k) for k in keys]

    warnings = []
    seen = set()  # O(1) dedup
    for f in futures:
        try:
            s3_key, records = f.result(timeout=30)
        except Exception:
            continue
        for rec in records:
            vendor_id = str(rec.get("EnrichedVendorID") or "").strip()
            vendor_name = str(rec.get("EnrichedVendorName") or rec.get("Vendor Name") or "").strip()
            prop_id = str(rec.get("EnrichedPropertyID") or "").strip()
            prop_name = str(rec.get("EnrichedPropertyName") or "").strip()
            gl_code = str(rec.get("EnrichedGLAccountNumber") or "").strip()
            gl_name = str(rec.get("EnrichedGLAccountName") or "").strip()

            # Check vendor-property pair
            if vendor_id and prop_id and (vendor_id, prop_id) not in vp_set:
                dedup_key = ("vp", vendor_id, prop_id)
                if dedup_key not in seen:
                    seen.add(dedup_key)
                    warnings.append({
                        "key": s3_key,
                        "type": "vendor_property",
                        "vendor_id": vendor_id,
                        "vendor_name": vendor_name,
                        "property_id": prop_id,
                        "property_name": prop_name,
                        "message": f"Vendor \"{vendor_name}\" (ID: {vendor_id}) has never been posted to property \"{prop_name}\" (ID: {prop_id}) in the past year.",
                    })

            # Check vendor-GL pair
            if vendor_id and gl_code and (vendor_id, gl_code) not in vg_set:
                dedup_key = ("vg", vendor_id, gl_code)
                if dedup_key not in seen:
                    seen.add(dedup_key)
                    warnings.append({
                        "key": s3_key,
                        "type": "vendor_gl",
                        "vendor_id": vendor_id,
                        "vendor_name": vendor_name,
                        "gl_code": gl_code,
                        "gl_code_name": gl_name,
                        "message": f"Vendor \"{vendor_name}\" (ID: {vendor_id}) has never used GL code {gl_code} ({gl_name}) in the past year.",
                    })

    return {"warnings": warnings, "count": len(warnings)}


# -------- Distributed post lock (prevents duplicate Entrata submissions) --------
# Thread-local storage for lock nonces (to verify we own the lock when updating)
_POST_LOCK_NONCES: dict[str, str] = {}  # s3_key -> nonce


def _acquire_post_lock(s3_key: str, user: str) -> bool:
    """Atomically acquire a posting lock for an S3 key using DynamoDB conditional put.
    Returns True if lock acquired, False if another request already holds it.

    Lock can be acquired when:
    - No lock exists (first time)
    - Lock status is FAILED or POSTED (previous attempt completed)
    - Lock status is POSTING but stale (> 30 sec, likely crashed or stuck)
    - Lock status is POSTING and same user is retrying (allows retry after failed release)
    """
    import uuid
    sk = hashlib.sha1(s3_key.encode()).hexdigest()
    now_epoch = int(time.time())
    now_iso = dt.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
    stale_cutoff = dt.datetime.utcfromtimestamp(now_epoch - 30).isoformat() + "Z"  # 30 sec stale timeout
    nonce = uuid.uuid4().hex[:12]  # Unique ID for this lock acquisition
    try:
        ddb.put_item(
            TableName=CONFIG_TABLE,
            Item={
                "PK": {"S": "POST_LOCK"},
                "SK": {"S": sk},
                "s3_key": {"S": s3_key},
                "status": {"S": "POSTING"},
                "locked_by": {"S": user},
                "locked_at": {"S": now_iso},
                "nonce": {"S": nonce},  # Track which request owns this lock
                "ttl_epoch": {"N": str(now_epoch + 86400)},
            },
            ConditionExpression=(
                "attribute_not_exists(PK) "
                "OR #st = :failed "
                "OR #st = :posted "
                "OR (#st = :posting AND locked_at < :stale) "
                "OR (#st = :posting AND locked_by = :user)"  # Same user can retry
            ),
            ExpressionAttributeNames={"#st": "status"},
            ExpressionAttributeValues={
                ":failed": {"S": "FAILED"},
                ":posted": {"S": "POSTED"},
                ":posting": {"S": "POSTING"},
                ":stale": {"S": stale_cutoff},
                ":user": {"S": user},
            },
        )
        _POST_LOCK_NONCES[s3_key] = nonce  # Store nonce for later verification
        return True
    except ddb.exceptions.ConditionalCheckFailedException:
        return False
    except Exception as e:
        print(f"[POST LOCK] Error acquiring lock for {s3_key}: {e}")
        return True  # fail-open: if DDB is down, allow the post rather than blocking


def _update_post_lock(s3_key: str, status: str, force: bool = False):
    """Update a posting lock to POSTED or FAILED after the Entrata call completes.

    Uses nonce verification to ensure we're updating OUR lock, not one that was
    re-acquired by another request. This prevents race conditions.

    Args:
        s3_key: The S3 key to update the lock for
        status: The new status (POSTED or FAILED)
        force: If True, skip nonce verification and do unconditional update.
               Use this for vendor-location retry flows where the lock MUST be
               released to allow the user to retry with location selection.
    """
    sk = hashlib.sha1(s3_key.encode()).hexdigest()
    file_name = s3_key.split('/')[-1] if '/' in s3_key else s3_key
    our_nonce = _POST_LOCK_NONCES.get(s3_key)

    # Force mode: unconditional update (like clear_post_locks)
    if force:
        try:
            ddb.update_item(
                TableName=CONFIG_TABLE,
                Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
                UpdateExpression="SET #st = :new_st, completed_at = :at",
                ExpressionAttributeNames={"#st": "status"},
                ExpressionAttributeValues={
                    ":new_st": {"S": status},
                    ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                },
            )
            print(f"[POST LOCK] Force-updated {file_name} to {status}")
        except Exception as e:
            print(f"[POST LOCK] ERROR force-updating {file_name}: {e}")
        _POST_LOCK_NONCES.pop(s3_key, None)
        return

    if not our_nonce:
        # No nonce stored - legacy code path or lock wasn't acquired by us
        # Still try to update but log the inconsistency
        print(f"[POST LOCK] WARNING: No nonce for {file_name}, attempting unconditional update to {status}")
        try:
            ddb.update_item(
                TableName=CONFIG_TABLE,
                Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
                UpdateExpression="SET #st = :new_st, completed_at = :at",
                ExpressionAttributeNames={"#st": "status"},
                ExpressionAttributeValues={
                    ":new_st": {"S": status},
                    ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                },
            )
            print(f"[POST LOCK] Updated {file_name} to {status} (no nonce)")
        except Exception as e:
            print(f"[POST LOCK] ERROR updating {file_name}: {e}")
        return

    try:
        ddb.update_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
            UpdateExpression="SET #st = :new_st, completed_at = :at",
            ConditionExpression="#st = :posting AND nonce = :nonce",  # Verify we own this lock
            ExpressionAttributeNames={"#st": "status"},
            ExpressionAttributeValues={
                ":new_st": {"S": status},
                ":posting": {"S": "POSTING"},
                ":nonce": {"S": our_nonce},
                ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
            },
        )
        print(f"[POST LOCK] Updated {file_name} to {status} (nonce verified)")
        # Clean up stored nonce
        _POST_LOCK_NONCES.pop(s3_key, None)
    except ddb.exceptions.ConditionalCheckFailedException:
        # Either status changed or nonce doesn't match (another request owns the lock)
        # For FAILED status, force the update anyway to prevent stuck locks
        if status == "FAILED":
            print(f"[POST LOCK] Condition failed for {file_name}, forcing update to FAILED")
            try:
                ddb.update_item(
                    TableName=CONFIG_TABLE,
                    Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
                    UpdateExpression="SET #st = :new_st, completed_at = :at",
                    ExpressionAttributeNames={"#st": "status"},
                    ExpressionAttributeValues={
                        ":new_st": {"S": "FAILED"},
                        ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                    },
                )
                print(f"[POST LOCK] Force-updated {file_name} to FAILED after condition failure")
            except Exception as e2:
                print(f"[POST LOCK] ERROR force-updating {file_name}: {e2}")
        else:
            try:
                verify = ddb.get_item(
                    TableName=CONFIG_TABLE,
                    Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
                    ProjectionExpression="#st, nonce",
                    ExpressionAttributeNames={"#st": "status"},
                )
                actual_status = verify.get("Item", {}).get("status", {}).get("S", "UNKNOWN")
                actual_nonce = verify.get("Item", {}).get("nonce", {}).get("S", "UNKNOWN")
                if actual_nonce != our_nonce:
                    print(f"[POST LOCK] Skipped {file_name}: lock re-acquired by another request (nonce mismatch)")
                else:
                    print(f"[POST LOCK] Skipped {file_name}: status already {actual_status} (wanted {status})")
            except Exception:
                print(f"[POST LOCK] Skipped {file_name}: condition failed")
        _POST_LOCK_NONCES.pop(s3_key, None)
    except Exception as e:
        import traceback
        print(f"[POST LOCK] ERROR updating {file_name} to {status}: {type(e).__name__}: {e}")
        print(f"[POST LOCK] Traceback: {traceback.format_exc()}")
        _POST_LOCK_NONCES.pop(s3_key, None)


@app.post("/api/clear_post_locks")
def api_clear_post_locks(keys: str = Form(...), user: str = Depends(require_user)):
    """Force-clear posting locks for the given S3 keys. Use when locks are stuck."""
    sel = [k.strip() for k in (keys or '').split('|||') if k.strip()]
    cleared = 0
    for key in sel:
        try:
            sk = hashlib.sha1(key.encode()).hexdigest()
            ddb.update_item(
                TableName=CONFIG_TABLE,
                Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
                UpdateExpression="SET #st = :st, completed_at = :at, cleared_by = :cb",
                ExpressionAttributeNames={"#st": "status"},
                ExpressionAttributeValues={
                    ":st": {"S": "FAILED"},
                    ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                    ":cb": {"S": user},
                },
            )
            cleared += 1
            print(f"[POST LOCK] Force-cleared by {user}: {key.split('/')[-1]}")
        except Exception as e:
            print(f"[POST LOCK] Error clearing lock for {key}: {e}")
    return {"ok": True, "cleared": cleared}


@app.get("/api/test_post_lock")
def api_test_post_lock(user: str = Depends(require_user)):
    """Diagnostic endpoint to test if post lock updates work correctly with nonce verification."""
    import uuid
    test_key = f"__TEST_LOCK_{uuid.uuid4().hex[:8]}__"
    sk = hashlib.sha1(test_key.encode()).hexdigest()
    nonce = uuid.uuid4().hex[:12]
    results = {"test_key": test_key, "sk": sk, "nonce": nonce, "steps": []}

    # Step 1: Create a test lock (POSTING) with nonce
    try:
        ddb.put_item(
            TableName=CONFIG_TABLE,
            Item={
                "PK": {"S": "POST_LOCK"},
                "SK": {"S": sk},
                "s3_key": {"S": test_key},
                "status": {"S": "POSTING"},
                "nonce": {"S": nonce},
                "locked_by": {"S": user},
                "locked_at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
                "ttl_epoch": {"N": str(int(time.time()) + 60)},  # Expire in 1 min
            },
        )
        results["steps"].append({"step": "create_lock", "status": "ok"})
    except Exception as e:
        results["steps"].append({"step": "create_lock", "status": "error", "error": str(e)})
        return results

    # Step 2: Verify lock was created with correct nonce
    try:
        resp = ddb.get_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}})
        status = resp.get("Item", {}).get("status", {}).get("S", "NOT_FOUND")
        actual_nonce = resp.get("Item", {}).get("nonce", {}).get("S", "NOT_FOUND")
        nonce_ok = actual_nonce == nonce
        results["steps"].append({
            "step": "verify_create",
            "status": "ok" if status == "POSTING" and nonce_ok else "error",
            "actual_status": status,
            "actual_nonce": actual_nonce,
            "nonce_match": nonce_ok
        })
    except Exception as e:
        results["steps"].append({"step": "verify_create", "status": "error", "error": str(e)})

    # Step 3: Update lock to POSTED with nonce verification (simulating _update_post_lock)
    try:
        ddb.update_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
            UpdateExpression="SET #st = :new_st, completed_at = :at",
            ConditionExpression="#st = :posting AND nonce = :nonce",
            ExpressionAttributeNames={"#st": "status"},
            ExpressionAttributeValues={
                ":new_st": {"S": "POSTED"},
                ":posting": {"S": "POSTING"},
                ":nonce": {"S": nonce},
                ":at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
            },
        )
        results["steps"].append({"step": "update_to_posted", "status": "ok"})
    except ddb.exceptions.ConditionalCheckFailedException as e:
        results["steps"].append({"step": "update_to_posted", "status": "error", "error": "ConditionalCheckFailed - nonce mismatch or status changed"})
        return results
    except Exception as e:
        results["steps"].append({"step": "update_to_posted", "status": "error", "error": str(e)})
        return results

    # Step 4: Verify lock was updated
    try:
        resp = ddb.get_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}})
        status = resp.get("Item", {}).get("status", {}).get("S", "NOT_FOUND")
        results["steps"].append({"step": "verify_update", "status": "ok" if status == "POSTED" else "error", "actual": status})
        results["final_status"] = status
    except Exception as e:
        results["steps"].append({"step": "verify_update", "status": "error", "error": str(e)})

    # Step 5: Test that wrong nonce fails (simulating race condition)
    wrong_nonce = uuid.uuid4().hex[:12]
    try:
        ddb.update_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}},
            UpdateExpression="SET #st = :new_st",
            ConditionExpression="#st = :posted AND nonce = :nonce",
            ExpressionAttributeNames={"#st": "status"},
            ExpressionAttributeValues={
                ":new_st": {"S": "FAILED"},
                ":posted": {"S": "POSTED"},
                ":nonce": {"S": wrong_nonce},
            },
        )
        # If we get here, wrong nonce was accepted - that's bad!
        results["steps"].append({"step": "wrong_nonce_rejected", "status": "error", "error": "Wrong nonce was accepted!"})
    except ddb.exceptions.ConditionalCheckFailedException:
        # Expected - wrong nonce should be rejected
        results["steps"].append({"step": "wrong_nonce_rejected", "status": "ok", "detail": "Correctly rejected update with wrong nonce"})
    except Exception as e:
        results["steps"].append({"step": "wrong_nonce_rejected", "status": "error", "error": str(e)})

    # Step 6: Clean up test lock
    try:
        ddb.delete_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}})
        results["steps"].append({"step": "cleanup", "status": "ok"})
    except Exception as e:
        results["steps"].append({"step": "cleanup", "status": "error", "error": str(e)})

    # Summary
    all_ok = all(s.get("status") == "ok" for s in results["steps"])
    results["all_ok"] = all_ok
    results["message"] = "Post lock mechanism with nonce verification is working correctly" if all_ok else "Post lock mechanism has issues - check steps"

    return results


@app.post("/api/verify_entrata_sync")
def api_verify_entrata_sync(
    entrata_data: str = Form(...),
    date_from: str = Form(None),
    date_to: str = Form(None),
    user: str = Depends(require_user)
):
    """Compare our posted invoices (Stage 7) with Entrata GL data.

    entrata_data: JSON array of Entrata invoices with at least:
        - InvoiceNumber (format: "AccountNumber MM/DD/YYYY")
        - InvoiceTotal
        - PropertyId (optional)
        - VendorId (optional)
    date_from/date_to: Optional date range (YYYY-MM-DD) to filter our Stage 7 data
    """
    try:
        entrata_invoices = json.loads(entrata_data)
        if not isinstance(entrata_invoices, list):
            return JSONResponse({"error": "entrata_data must be a JSON array"}, status_code=400)
    except json.JSONDecodeError as e:
        return JSONResponse({"error": f"Invalid JSON: {e}"}, status_code=400)

    # Build lookup from Entrata data by invoice number
    entrata_lookup = {}
    for inv in entrata_invoices:
        inv_num = str(inv.get("InvoiceNumber") or inv.get("invoiceNumber") or inv.get("invoice_number") or "").strip()
        if inv_num:
            total = inv.get("InvoiceTotal") or inv.get("invoiceTotal") or inv.get("invoice_total") or inv.get("Amount") or inv.get("amount") or 0
            try:
                total = float(str(total).replace("$", "").replace(",", ""))
            except:
                total = 0.0
            entrata_lookup[inv_num] = {
                "total": total,
                "property_id": str(inv.get("PropertyId") or inv.get("propertyId") or ""),
                "vendor_id": str(inv.get("VendorId") or inv.get("vendorId") or ""),
                "raw": inv,
            }

    # Determine date range
    from datetime import datetime, timedelta
    if date_from:
        try:
            start = datetime.strptime(date_from, "%Y-%m-%d").date()
        except:
            start = (datetime.utcnow() - timedelta(days=30)).date()
    else:
        start = (datetime.utcnow() - timedelta(days=30)).date()

    if date_to:
        try:
            end = datetime.strptime(date_to, "%Y-%m-%d").date()
        except:
            end = datetime.utcnow().date()
    else:
        end = datetime.utcnow().date()

    # Load our Stage 7 data
    our_invoices = []
    current = start
    while current <= end:
        prefix = f"{POST_ENTRATA_PREFIX}yyyy={current.year}/mm={current.month:02d}/dd={current.day:02d}/"
        try:
            paginator = s3.get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get("Contents", []):
                    key = obj["Key"]
                    if key.endswith(".jsonl"):
                        try:
                            resp = s3.get_object(Bucket=BUCKET, Key=key)
                            content = resp["Body"].read().decode("utf-8")
                            lines = [json.loads(l) for l in content.strip().split("\n") if l.strip()]
                            if lines:
                                first = lines[0]
                                acct = str(first.get("Account Number") or "").strip()
                                bill_date = str(first.get("Bill Date") or first.get("Invoice Date") or "").strip()
                                # Build invoice number same way as build_send_invoices_payload
                                inv_num = f"{acct} {bill_date}" if acct else bill_date
                                total = sum(float(str(l.get("Line Item Charge", "0")).replace("$", "").replace(",", "")) for l in lines)
                                our_invoices.append({
                                    "invoice_number": inv_num,
                                    "account_number": acct,
                                    "bill_date": bill_date,
                                    "total": round(total, 2),
                                    "line_count": len(lines),
                                    "posted_by": first.get("PostedBy", ""),
                                    "posted_at": first.get("PostedAt", ""),
                                    "s3_key": key,
                                    "vendor": first.get("EnrichedVendorName", first.get("Vendor Name", "")),
                                    "property": first.get("EnrichedPropertyName", first.get("Property Name", "")),
                                })
                        except:
                            pass
        except:
            pass
        current += timedelta(days=1)

    # Compare
    results = {
        "date_range": {"from": str(start), "to": str(end)},
        "our_invoice_count": len(our_invoices),
        "entrata_invoice_count": len(entrata_lookup),
        "matched": [],
        "missing_in_entrata": [],
        "amount_mismatch": [],
        "extra_in_entrata": list(entrata_lookup.keys()),  # Start with all, remove as we match
    }

    for inv in our_invoices:
        inv_num = inv["invoice_number"]
        if inv_num in entrata_lookup:
            entrata_inv = entrata_lookup[inv_num]
            results["extra_in_entrata"].remove(inv_num)

            our_total = inv["total"]
            entrata_total = entrata_inv["total"]
            diff = abs(our_total - entrata_total)

            if diff < 0.01:  # Allow 1 cent tolerance
                results["matched"].append({
                    "invoice_number": inv_num,
                    "our_total": our_total,
                    "entrata_total": entrata_total,
                })
            else:
                results["amount_mismatch"].append({
                    "invoice_number": inv_num,
                    "our_total": our_total,
                    "entrata_total": entrata_total,
                    "difference": round(our_total - entrata_total, 2),
                    "our_data": inv,
                })
        else:
            results["missing_in_entrata"].append(inv)

    # Summary
    results["summary"] = {
        "matched_count": len(results["matched"]),
        "missing_in_entrata_count": len(results["missing_in_entrata"]),
        "amount_mismatch_count": len(results["amount_mismatch"]),
        "extra_in_entrata_count": len(results["extra_in_entrata"]),
    }

    return results


@app.post("/api/post_to_entrata")
def api_post_to_entrata(request: Request, keys: str = Form(...), vendor_overrides: str | None = Form(None), post_month: str | None = Form(None), post_month_date: str | None = Form(None), repost_suffixes: str | None = Form(None), user: str = Depends(require_user)):
    try:
        # Use ||| as delimiter to support filenames with commas (e.g., "San Francisco Water, Power and Sewer")
        # dict.fromkeys deduplicates while preserving order (prevents same key posted twice in one request)
        sel = list(dict.fromkeys(k.strip() for k in (keys or '').split('|||') if k.strip()))
        if not sel:
            return JSONResponse({"error": "no_keys"}, status_code=400)
        print(f"[POST] user={user}, keys={len(sel)}, vendor_overrides={vendor_overrides!r}")
        # Use cached vendor data to avoid S3 read on every POST
        _vc = _POST_HELPER_CACHE.get("vendor_cache")
        if _vc and (time.time() - _vc["ts"] < _POST_HELPER_TTL):
            cache = _vc["data"]
        else:
            cache = load_vendor_cache()
            _POST_HELPER_CACHE["vendor_cache"] = {"ts": time.time(), "data": cache}
        # Parse optional vendor overrides { vendorId: locationId }
        overrides: dict[str, str] = {}
        try:
            if vendor_overrides:
                ov = json.loads(vendor_overrides)
                if isinstance(ov, dict):
                    overrides = {str(k): str(v) for k, v in ov.items() if v}
        except Exception:
            pass
        if overrides:
            print(f"[POST] Parsed overrides: {overrides}")
        # Parse optional repost suffixes { s3_key: suffix } for duplicate invoice reposting
        suffix_map: dict[str, str] = {}
        try:
            if repost_suffixes:
                sm = json.loads(repost_suffixes)
                if isinstance(sm, dict):
                    suffix_map = {str(k): str(v) for k, v in sm.items() if v}
        except Exception:
            pass
        updated = 0
        errors: list[dict] = []
        unresolved: list[dict] = []
        results: list[dict] = []
        _post_lock_held: str | None = None  # Track active lock for safety-net release on crash
        # Preload GL override helpers once
        gl_num_to_id = _load_gl_number_to_id_map()
        gl_name_to_id = _load_gl_name_to_id_map()
        att_index = _index_accounts_to_track_by_key()
        for key in sel:
            _post_lock_held = None  # Reset at start of each iteration
            file_name = key.split('/')[-1] if '/' in key else key
            # --- Distributed lock: prevent concurrent/duplicate posts ---
            if not _acquire_post_lock(key, user):
                # Log the current lock state for debugging
                try:
                    sk = hashlib.sha1(key.encode()).hexdigest()
                    lock_item = ddb.get_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POST_LOCK"}, "SK": {"S": sk}}).get("Item", {})
                    lock_status = lock_item.get("status", {}).get("S", "?")
                    lock_by = lock_item.get("locked_by", {}).get("S", "?")
                    lock_at = lock_item.get("locked_at", {}).get("S", "?")
                    print(f"[POST LOCK] BLOCKED for {file_name}: status={lock_status}, by={lock_by}, at={lock_at}")
                except Exception:
                    print(f"[POST LOCK] BLOCKED for {file_name} (could not read lock state)")
                errors.append({"key": key, "error": f"Already being posted by another request: {file_name}", "code": "lock_held"})
                continue
            print(f"[POST] Lock acquired for {file_name}")
            _post_lock_held = key  # Track which key holds an active lock

            # Read rows from S3
            rows = _read_json_records_from_s3([key])
            file_name = key.split('/')[-1] if '/' in key else key
            if not rows:
                # Check if file exists to give better error message
                try:
                    s3.head_object(Bucket=BUCKET, Key=key)
                    _update_post_lock(key, "FAILED", force=True)
                    errors.append({"key": key, "error": f"File has no data: {file_name}", "code": "empty_file"})
                except Exception:
                    _update_post_lock(key, "FAILED", force=True)
                    errors.append({"key": key, "error": f"Already posted or not found: {file_name}", "code": "not_found"})
                continue
            # Attach PDF (best-effort) to the first row for inclusion in payload
            try:
                pdf_trip = _try_load_pdf_b64(rows[0]) if isinstance(rows[0], dict) else None
                if pdf_trip:
                    b64, fname, url = pdf_trip
                    rows[0]["__pdf_b64__"] = b64
                    rows[0]["__pdf_filename__"] = fname
                    if url:
                        rows[0]["__pdf_url__"] = url
            except Exception:
                pass
            # Apply GL override for this (propertyId,vendorId,accountNumber) if present in Accounts-To-Track
            try:
                if isinstance(rows[0], dict):
                    pid = str(rows[0].get("EnrichedPropertyID") or rows[0].get("Property Id") or rows[0].get("PropertyID") or rows[0].get("PropertyId") or "").strip()
                    vid = str(rows[0].get("EnrichedVendorID") or rows[0].get("Vendor ID") or rows[0].get("VendorID") or "").strip()
                    acct = str(rows[0].get("Account Number") or rows[0].get("AccountNumber") or "").strip()
                    cfg = att_index.get((pid, vid, acct))
                    if cfg and cfg.get("glAccountNumber"):
                        wanted_num = str(cfg.get("glAccountNumber") or "").strip()
                        gid = gl_num_to_id.get(wanted_num)
                        if gid:
                            for r in rows:
                                if isinstance(r, dict):
                                    r["EnrichedGLAccountID"] = gid
            except Exception:
                pass

            # Recompute GL DESC_NEW with the latest format to avoid stale descriptions from older Stage 6 files
            try:
                def _norm(v: Any) -> str:
                    return (str(v or "").strip())
                def _rebuild_desc(rec: dict) -> str:
                    # Check for VACANT GL - use special format
                    vacant_desc = _build_vacant_gl_desc(rec)
                    if vacant_desc:
                        return vacant_desc

                    # HOUSE format (standard)
                    addr = _norm(rec.get("Service Address")).upper()
                    acct = _norm(rec.get("Account Number"))
                    li_acct = _norm(rec.get("Line Item Account Number"))
                    meter = _norm(rec.get("Meter Number"))
                    desc = _norm(rec.get("Line Item Description")).upper()
                    cons = _norm(rec.get("ENRICHED CONSUMPTION") or rec.get("Consumption Amount"))
                    uom = _norm(rec.get("ENRICHED UOM") or rec.get("Unit of Measure")).upper()
                    bps = _norm(rec.get("Bill Period Start"))
                    bpe = _norm(rec.get("Bill Period End"))
                    rng = f"{bps}-{bpe}" if (bps or bpe) else ""
                    parts = [desc, rng, addr, acct, li_acct, meter, cons, uom]
                    return " | ".join(parts)
                for r in rows:
                    if isinstance(r, dict):
                        r["GL DESC_NEW"] = _rebuild_desc(r)
            except Exception:
                pass

            # Honor user edits from Parse: prefer GL Account Name over Number, then fallback to Number
            # This ensures explicit Name changes win even if the Number was not updated.
            try:
                for r in rows:
                    if not isinstance(r, dict):
                        continue
                    glname = str(r.get("EnrichedGLAccountName") or r.get("GL Account Name") or "").strip()
                    glnum = str(r.get("EnrichedGLAccountNumber") or r.get("GL Account Number") or "").strip()
                    # 1) Try by Name first (user-facing control in Parse)
                    if glname:
                        gid2 = gl_name_to_id.get(glname.upper())
                        if gid2:
                            r["EnrichedGLAccountID"] = gid2
                    # 2) If still no ID, try by Number
                    if not r.get("EnrichedGLAccountID") and glnum:
                        gid = gl_num_to_id.get(glnum)
                        if gid:
                            r["EnrichedGLAccountID"] = gid
            except Exception:
                pass

            # Resolver: must have exactly one location for vendor
            def resolver(vendor_id: str) -> str:
                vid = str(vendor_id)
                # 1) explicit override wins
                if vid in overrides and overrides[vid]:
                    return overrides[vid]
                locs = cache.get(vid, []) if isinstance(cache, dict) else []
                # 0 locations: vendor ID not found in cache - user needs to check their vendor selection
                if len(locs) == 0:
                    errors.append({"key": key, "error": f"Vendor ID '{vid}' not found in vendor cache. Please verify the correct vendor is selected in the Review/Parse page."})
                    raise ValueError(f"vendor_not_found:{vid}")
                # 1 location: auto-assign it
                if len(locs) == 1:
                    # Handle both old format (string) and new format (dict with id/name)
                    loc = locs[0]
                    return loc["id"] if isinstance(loc, dict) else str(loc)
                # 2+ locations: prompt user to select
                # locs is now [{id, name}, ...] for frontend to display
                unresolved.append({
                    "key": key,
                    "vendorId": vid,
                    "choices": locs
                })
                # raise to break out of build
                raise ValueError(f"vendor_location_unresolved:{vid}:{len(locs)}")

            try:
                # Prefer MM/YYYY if provided; otherwise accept a date (YYYY-MM-DD) and extract month
                pm_arg = None
                if post_month and isinstance(post_month, str) and '/' in post_month:
                    # Convert MM/YYYY -> pseudo date as first of month for parser
                    try:
                        mm, yyyy = post_month.split('/')
                        pm_arg = f"{yyyy}-{mm}-01"
                    except Exception:
                        pm_arg = None
                if not pm_arg and post_month_date:
                    pm_arg = post_month_date
                inv_suffix = suffix_map.get(key, "")
                payload = build_send_invoices_payload(rows, resolver, post_month_date=pm_arg, invoice_suffix=inv_suffix)
                print(f"[POST] Payload built for {file_name}: {len(payload) if payload else 0} chars")
            except Exception as e:
                # If unresolved locations exist, we will return a promptable response instead of erroring out
                if any(isinstance(u, dict) and u.get("key") == key for u in unresolved):
                    print(f"[POST] Unresolved vendor location for {file_name}, setting lock FAILED")
                    # Use force=True to ensure lock is released for retry (multi-location vendor flow)
                    _update_post_lock(key, "FAILED", force=True)
                    continue
                print(f"[POST] Payload build FAILED for {file_name}: {e}")
                _update_post_lock(key, "FAILED", force=True)
                errors.append({"key": key, "error": f"Could not prepare invoice for Entrata: {e}", "code": "build_failed", "hint": "Check that all required fields (vendor, property, GL account) are filled in."}); continue

            print(f"[POST] Calling do_post for {file_name}...")
            ok, text = do_post(payload, dry_run=False)
            print(f"[POST] do_post result for {file_name}: ok={ok}, response_len={len(text) if isinstance(text, str) else '?'}")
            # Parse body to detect silent failures (e.g., duplicates) even on HTTP 200
            succ, reason = _entrata_post_succeeded(text if isinstance(text, str) else str(text)) if ok else (False, "http_error")
            if not succ:
                _update_post_lock(key, "FAILED", force=True)
                error_hints = {
                    "duplicate": "This invoice was already posted to Entrata.",
                    "invalid_vendor": "The vendor ID is not valid in Entrata.",
                    "invalid_property": "The property ID is not valid in Entrata.",
                    "invalid_gl": "The GL account is not valid in Entrata.",
                    "http_error": "Entrata did not respond in time. Check Entrata for this invoice before retrying.",
                }
                hint = error_hints.get(reason, "Check the invoice details and try again.")
                err_entry: dict = {"key": key, "error": f"Entrata rejected invoice: {reason}", "code": "post_failed", "hint": hint, "response": (text[:500] if isinstance(text, str) else "")}
                # If duplicate, flag as repostable with current suffix so frontend can escalate
                if reason == "duplicate":
                    acct_num = str(rows[0].get("Account Number") or rows[0].get("AccountNumber") or "").strip() if rows else ""
                    bill_date_str = str(rows[0].get("Bill Date") or rows[0].get("Invoice Date") or "").strip() if rows else ""
                    err_entry["repostable"] = True
                    err_entry["account_number"] = acct_num
                    err_entry["bill_date"] = bill_date_str
                    err_entry["current_suffix"] = suffix_map.get(key, "")
                errors.append(err_entry)
                results.append({"key": key, "posted": False, "moved": False})
                continue
            # Count successful Entrata post immediately
            print(f"[POST] SUCCESS for {file_name} - calling _update_post_lock(POSTED)...")
            _update_post_lock(key, "POSTED")
            print(f"[POST] _update_post_lock returned for {file_name}")
            updated += 1

            # Archive PDF to organized folder structure in S3 (for sync to file server)
            try:
                archive_result = _archive_posted_pdf(rows[0])
                if archive_result:
                    print(f"[post_to_entrata] Archived PDF: {archive_result}")
            except Exception as archive_err:
                print(f"[post_to_entrata] PDF archive failed (non-fatal): {archive_err}")

            # Move the JSONL to POST_ENTRATA_PREFIX with Status=Posted, PostedBy, PostedAt
            try:
                y, m, d = _extract_ymd_from_key(key)
                base = _basename_from_key(key)
                posted_at = dt.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')
                posted_rows = _rewrite_status(rows, 'Posted')
                for row in posted_rows:
                    row["PostedBy"] = user
                    row["PostedAt"] = posted_at
                new_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl',''), posted_rows)
                # Write invoice metadata to DynamoDB for fast CHECK REVIEW loading
                try:
                    _pid = hashlib.sha1(new_key.encode()).hexdigest()
                    _write_posted_invoice_metadata(_pid, new_key, posted_rows)
                except Exception:
                    pass  # Non-fatal  CHECK REVIEW falls back to S3
                # Delete original from source stage (data already written to Stage 7)
                s3.delete_object(Bucket=BUCKET, Key=key)
                results.append({"key": key, "posted": True, "moved": True, "newKey": new_key})
            except Exception as e:
                errors.append({"key": key, "error": f"Posted to Entrata but failed to archive: {e}", "code": "move_failed", "hint": "The invoice WAS posted successfully. Refresh the page - it may appear in the correct stage."})
                results.append({"key": key, "posted": True, "moved": False})
                continue
        # If any unresolved vendor locations were discovered and not satisfied by overrides, prompt client
        if unresolved and not overrides:
            print(f"[POST] Returning vendor_locations_needed: {len(unresolved)} unresolved, {len(errors)} errors")
            return JSONResponse({
                "ok": False,
                "message": "vendor_locations_needed",
                "unresolved": unresolved,
                "errors": errors
            }, status_code=400)
        print(f"[POST] DONE: updated={updated}, errors={len(errors)}, results={len(results)}, unresolved={len(unresolved)}")
        # Invalidate TRACK cache so POSTED appears immediately after posting
        try:
            _TRACK_CACHE.clear(); _TRACK_CACHE_TS.clear()
        except Exception:
            pass
        # Invalidate workflow tracker cache so new bill shows immediately
        _CACHE.pop(("workflow_tracker",), None)
        return {"ok": True, "updated": updated, "errors": errors, "unresolved": unresolved, "results": results}
    except Exception as e:
        # Safety net: release post lock if we crashed mid-post (e.g., AppRunner timeout)
        if _post_lock_held:
            try:
                _update_post_lock(_post_lock_held, "FAILED", force=True)
                print(f"[POST LOCK] Safety net released lock for {_post_lock_held.split('/')[-1]}")
            except Exception:
                pass
        import traceback
        return JSONResponse({"error": _sanitize_error(e, "API request")}, status_code=500)


@app.post("/api/advance_to_post_stage")
def api_advance_to_post_stage(keys: str = Form(...), user: str = Depends(require_user)):
    """Move selected pre-Entrata merged JSONL files to Post-Entrata stage WITHOUT posting.
    Does not modify Status; simply re-writes the JSONL under POST_ENTRATA_PREFIX and deletes the source.
    Records PostedBy and PostedAt for metrics tracking.
    """
    try:
        # Use ||| as delimiter to support filenames with commas (e.g., "San Francisco Water, Power and Sewer")
        sel = [k.strip() for k in (keys or '').split('|||') if k.strip()]
        if not sel:
            return JSONResponse({"error": "no_keys"}, status_code=400)
        results: list[dict] = []
        errors: list[dict] = []
        posted_at = dt.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')
        for key in sel:
            try:
                rows = _read_json_records_from_s3([key])
                if not rows:
                    file_name = key.split('/')[-1] if '/' in key else key
                    errors.append({"key": key, "error": f"File not found or empty: {file_name}", "code": "empty"}); continue
                y, m, d = _extract_ymd_from_key(key)
                base = _basename_from_key(key)
                # Add PostedBy and PostedAt to each row for metrics tracking
                for row in rows:
                    row["PostedBy"] = user
                    row["PostedAt"] = posted_at
                new_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl',''), rows)
                # Write invoice metadata to DynamoDB for fast CHECK REVIEW loading
                try:
                    _pid = hashlib.sha1(new_key.encode()).hexdigest()
                    _write_posted_invoice_metadata(_pid, new_key, rows)
                except Exception:
                    pass  # Non-fatal  CHECK REVIEW falls back to S3
                s3.delete_object(Bucket=BUCKET, Key=key)
                results.append({"key": key, "moved": True, "newKey": new_key})
            except Exception as e:
                errors.append({"key": key, "error": str(e)})
        return {"ok": True, "results": results, "errors": errors}
    except Exception as e:
        import traceback
        return JSONResponse({"error": _sanitize_error(e, "API request")}, status_code=500)


@app.post("/api/archive_parsed")
def api_archive_parsed(keys: str = Form(...), user: str = Depends(require_user)):
    """Move selected merged JSONL files to Historical Archive, partitioned by yyyy/mm/dd.
    Works for any source stage. Content is preserved. Source is deleted after archive write.
    """
    try:
        # Use ||| as delimiter to support filenames with commas (e.g., "San Francisco Water, Power and Sewer")
        sel = [k.strip() for k in (keys or '').split('|||') if k.strip()]
        if not sel:
            return JSONResponse({"error": "no_keys"}, status_code=400)
        results: list[dict] = []
        errors: list[dict] = []
        for key in sel:
            try:
                # Read entire object text, split into JSON lines
                body = _read_s3_text(BUCKET, key)
                rows = []
                for ln in body.splitlines():
                    ln = (ln or '').strip()
                    if not ln:
                        continue
                    try:
                        rows.append(json.loads(ln))
                    except Exception:
                        pass
                if not rows:
                    file_name = key.split('/')[-1] if '/' in key else key
                    errors.append({"key": key, "error": f"File not found or empty: {file_name}", "code": "empty"}); continue
                y, m, d = _extract_ymd_from_key(key)
                base = _basename_from_key(key)
                new_key = _write_jsonl(HIST_ARCHIVE_PREFIX, y, m, d, base.replace('.jsonl',''), rows)
                s3.delete_object(Bucket=BUCKET, Key=key)
                results.append({"key": key, "archived": True, "newKey": new_key})
            except Exception as e:
                errors.append({"key": key, "error": str(e)})
        return {"ok": True, "results": results, "errors": errors}
    except Exception as e:
        import traceback
        return JSONResponse({"error": _sanitize_error(e, "API request")}, status_code=500)

# Month-level listing: list once per month prefix rather than per day
def _iter_stage_objects_by_month(prefix_root: str, months: List[dt.date]):
    seen: set[str] = set()
    for md in months:
        y = md.year; m = md.month
        prefixes = [
            f"{prefix_root}yyyy={y}/mm={m:02d}/",
            f"{prefix_root}{y}/{m:02d}/",
        ]
        for p in prefixes:
            try:
                # Use paginator to get ALL files (not just first 2000)
                paginator = s3.get_paginator("list_objects_v2")
                for page in paginator.paginate(Bucket=BUCKET, Prefix=p):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", "")
                        if k and k not in seen:
                            seen.add(k); yield k
            except Exception:
                pass


def put_draft(pdf_id: str, line_id: str, user: str, fields: Dict[str, Any], date: str, invoice: str):
    pk = f"draft#{pdf_id}#{line_id}#{user}"
    ddb.put_item(
        TableName=DRAFTS_TABLE,
        Item={
            "pk": {"S": pk},
            "pdf_id": {"S": pdf_id},
            "line_id": {"S": line_id},
            "user": {"S": user},
            "date": {"S": date},
            "invoice": {"S": str(invoice)},
            "fields": {"S": json.dumps(fields, ensure_ascii=False)},
            "updated_utc": {"S": dt.datetime.utcnow().isoformat()}
        }
    )


def _parse_s3_from_url(url: str) -> tuple[str, str] | None:
    """Try to extract (bucket, key) from a presigned S3 URL.
    Supports both bucket.s3.amazonaws.com/key and s3.amazonaws.com/bucket/key formats.
    """
    try:
        p = urlparse(url)
        host = p.netloc.lower()
        path = p.path.lstrip('/')
        if host.endswith('.s3.amazonaws.com'):
            # bucket.s3.amazonaws.com/key
            bucket = host.split('.s3.amazonaws.com')[0]
            key = unquote(path)
            return bucket, key
        if host == 's3.amazonaws.com':
            # s3.amazonaws.com/bucket/key
            parts = path.split('/', 1)
            if len(parts) == 2:
                return parts[0], unquote(parts[1])
        # S3 virtual-hosted-style in regional endpoints e.g. bucket.s3.us-east-1.amazonaws.com
        if '.s3.' in host and host.endswith('.amazonaws.com'):
            bucket = host.split('.s3.')[0]
            return bucket, unquote(path)
    except Exception:
        pass
    return None


def _resolve_final_url(url: str) -> str:
    """Follow redirects (e.g., short links) to the final URL. Use GET for better compatibility."""
    try:
        r = requests.get(url, allow_redirects=True, timeout=8, stream=True)
        return r.url
    except Exception:
        return url


# -------- Routes (HTML) --------
@app.get("/login", response_class=HTMLResponse)
def login_form(request: Request):
    return templates.TemplateResponse("login.html", {"request": request, "prefill_user": ""})


@app.post("/login")
def login(request: Request, username: str = Form(...), password: str = Form(...)):
    """Login with new role-based authentication system."""
    try:
        user = auth.authenticate(username, password)
        if not user:
            return templates.TemplateResponse("login.html", {
                "request": request,
                "error": "Invalid email or password",
                "prefill_user": username
            }, status_code=401)

        # Track successful login
        try:
            login_time = dt.datetime.utcnow().isoformat()
            ddb.put_item(
                TableName=DRAFTS_TABLE,
                Item={
                    "pk": {"S": f"login#{username}#{login_time}"},
                    "user": {"S": username},
                    "login_utc": {"S": login_time},
                    "ip": {"S": request.client.host if request.client else "unknown"},
                }
            )
        except Exception as e:
            print(f"[LOGIN_TRACK] Failed to track login: {e}")

        # Check if user must change password
        if user.get("must_change_password"):
            # Redirect to change password page
            resp = RedirectResponse(url="/change-password", status_code=302)
            set_session(resp, username)
            return resp

        resp = RedirectResponse(url="/", status_code=302)
        set_session(resp, username)
        return resp
    except Exception as e:
        print(f"[LOGIN] Error: {e}")
        return templates.TemplateResponse("login.html", {
            "request": request,
            "error": "Invalid email or password",
            "prefill_user": username
        }, status_code=401)

@app.get("/change-password", response_class=HTMLResponse)
def change_password_form(request: Request, user: str = Depends(require_user)):
    """Show change password form."""
    return templates.TemplateResponse("change_password.html", {"request": request, "user": user})


@app.post("/change-password")
async def change_password(request: Request, current_password: str = Form(...), new_password: str = Form(...), confirm_password: str = Form(...)):
    """Change user password."""
    user_id = get_current_user(request)
    if not user_id:
        return RedirectResponse("/login", status_code=302)

    if new_password != confirm_password:
        return templates.TemplateResponse("change_password.html", {
            "request": request,
            "user": user_id,
            "error": "Passwords do not match"
        }, status_code=400)

    if len(new_password) < 8:
        return templates.TemplateResponse("change_password.html", {
            "request": request,
            "user": user_id,
            "error": "Password must be at least 8 characters"
        }, status_code=400)

    # Verify current password
    user = auth.authenticate(user_id, current_password)
    if not user:
        return templates.TemplateResponse("change_password.html", {
            "request": request,
            "user": user_id,
            "error": "Current password is incorrect"
        }, status_code=401)

    # Update password
    if auth.update_password(user_id, new_password):
        return RedirectResponse("/", status_code=302)
    else:
        return templates.TemplateResponse("change_password.html", {
            "request": request,
            "user": user_id,
            "error": "Failed to update password"
        }, status_code=500)


@app.get("/config/users", response_class=HTMLResponse)
def users_page(request: Request, user: str = Depends(require_user)):
    """User management page (System Admins only)."""
    user_data = auth.get_user(user)
    if not user_data or user_data.get("role") != "System_Admins":
        return templates.TemplateResponse("error.html", {
            "request": request,
            "user": user,
            "error": "Access denied. System Admins only."
        }, status_code=403)

    return templates.TemplateResponse("users.html", {
        "request": request,
        "user": user,
        "user_role": user_data.get("role"),
        "roles": auth.ROLES
    })


@app.get("/api/users")
def api_list_users(user: str = Depends(require_user)):
    """API endpoint to list all users."""
    user_data = auth.get_user(user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    users = auth.list_users()
    return {"users": users}


@app.post("/api/users")
async def api_create_user(request: Request, admin_user: str = Depends(require_user)):
    """API endpoint to create a new user."""
    user_data = auth.get_user(admin_user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    user_id = payload.get("user_id", "").strip()
    password = payload.get("password", "").strip()
    role = payload.get("role", "").strip()
    full_name = payload.get("full_name", "").strip()

    if not user_id or not password or not role or not full_name:
        return JSONResponse({"error": "All fields required"}, status_code=400)

    if role not in auth.ROLES:
        return JSONResponse({"error": "Invalid role"}, status_code=400)

    if len(password) < 8:
        return JSONResponse({"error": "Password must be at least 8 characters"}, status_code=400)

    if auth.create_user(user_id, password, role, full_name, created_by=admin_user):
        return {"ok": True}
    else:
        return JSONResponse({"error": "User already exists"}, status_code=400)


@app.post("/api/users/{user_id}/disable")
def api_disable_user(user_id: str, admin_user: str = Depends(require_user)):
    """API endpoint to disable a user."""
    user_data = auth.get_user(admin_user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    if auth.disable_user(user_id):
        return {"ok": True}
    else:
        return JSONResponse({"error": "Failed to disable user"}, status_code=500)


@app.post("/api/users/{user_id}/enable")
def api_enable_user(user_id: str, admin_user: str = Depends(require_user)):
    """API endpoint to enable a user."""
    user_data = auth.get_user(admin_user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    if auth.enable_user(user_id):
        return {"ok": True}
    else:
        return JSONResponse({"error": "Failed to enable user"}, status_code=500)


@app.post("/api/users/{user_id}/reset-password")
async def api_reset_password(user_id: str, request: Request, admin_user: str = Depends(require_user)):
    """API endpoint to reset a user's password."""
    user_data = auth.get_user(admin_user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    try:
        payload = await request.json()
        new_password = payload.get("new_password", "").strip()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    if not new_password or len(new_password) < 8:
        return JSONResponse({"error": "Password must be at least 8 characters"}, status_code=400)

    if auth.update_password(user_id, new_password):
        # Set must_change_password flag
        try:
            ddb.update_item(
                TableName=auth.USERS_TABLE,
                Key={"user_id": {"S": user_id}},
                UpdateExpression="SET must_change_password = :must_change",
                ExpressionAttributeValues={":must_change": {"BOOL": True}}
            )
        except Exception as e:
            print(f"[API] Error setting must_change_password for {user_id}: {e}")
        return {"ok": True}
    else:
        return JSONResponse({"error": "Failed to reset password"}, status_code=500)


@app.post("/api/users/{user_id}/role")
async def api_change_role(user_id: str, request: Request, admin_user: str = Depends(require_user)):
    """API endpoint to change a user's role."""
    user_data = auth.get_user(admin_user)
    if not user_data or user_data.get("role") != "System_Admins":
        return JSONResponse({"error": "Access denied"}, status_code=403)

    try:
        payload = await request.json()
        new_role = payload.get("role", "").strip()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    # Validate role
    valid_roles = {"System_Admins", "UBI_Admins", "Utility_APs"}
    if new_role not in valid_roles:
        return JSONResponse({"error": f"Invalid role. Must be one of: {', '.join(valid_roles)}"}, status_code=400)

    try:
        ddb.update_item(
            TableName=auth.USERS_TABLE,
            Key={"user_id": {"S": user_id}},
            UpdateExpression="SET #r = :role",
            ExpressionAttributeNames={"#r": "role"},
            ExpressionAttributeValues={":role": {"S": new_role}}
        )
        return {"ok": True}
    except Exception as e:
        print(f"[API] Error changing role for {user_id}: {e}")
        return JSONResponse({"error": "Failed to change role"}, status_code=500)


@app.get("/health")
def health():
    """Simple healthcheck endpoint for App Runner HTTP checks."""
    return {"ok": True}


@app.post("/logout")
def logout():
    resp = RedirectResponse(url="/login", status_code=302)
    # clear cookie
    resp.delete_cookie(SESSION_COOKIE)
    return resp
@app.get("/", response_class=HTMLResponse)
def home(request: Request):
    user = get_current_user(request)
    if not user:
        return RedirectResponse("/login", status_code=302)
    is_admin = user in ADMIN_USERS
    return templates.TemplateResponse("landing.html", {"request": request, "user": user, "is_admin": is_admin})


@app.get("/parse", response_class=HTMLResponse)
def parse_dashboard(request: Request, user: str = Depends(require_user)):
    """Parse dashboard with pagination. Shows last 8 days by default with 'Load More' option.

    Optimized to only compute status counts for displayed days (not all historical days).
    """
    from concurrent.futures import ThreadPoolExecutor, as_completed

    refresh = request.query_params.get("refresh") == "1"

    # Pagination params - default to first 8 days
    limit = int(request.query_params.get("limit", 8))
    offset = int(request.query_params.get("offset", 0))
    # Date range filter (optional)
    start_date = request.query_params.get("start", "")
    end_date = request.query_params.get("end", "")

    # Get all available dates (this is just S3 listing, relatively cheap and cached)
    all_dates = list_dates()

    # Apply date range filter first (before expensive status computation)
    filtered_dates = all_dates
    if start_date:
        filtered_dates = [d for d in filtered_dates if d["label"] >= start_date]
    if end_date:
        filtered_dates = [d for d in filtered_dates if d["label"] <= end_date]

    total_count = len(filtered_dates)
    has_more = (offset + limit) < total_count

    # Only fetch status counts for the days we're actually displaying
    dates_to_display = filtered_dates[offset:offset + limit]

    def fetch_day_counts(d):
        """Fetch status counts for a single day (uses per-day caching)."""
        y, m, dd = d["tuple"]
        counts = day_status_counts(y, m, dd)
        return {
            "date": f"{y}-{m}-{dd}",
            "label": d["label"],
            "review": counts["REVIEW"],
            "partial": counts["PARTIAL"],
            "complete": counts["COMPLETE"],
        }

    # Fetch only displayed days in parallel (up to 10 concurrent)
    day_cards = []
    if dates_to_display:
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {executor.submit(fetch_day_counts, d): d for d in dates_to_display}
            for future in as_completed(futures):
                try:
                    card = future.result()
                    day_cards.append(card)
                except Exception as e:
                    print(f"[PARSE] Error fetching day counts: {e}")

        # Sort by date descending (newest first)
        day_cards.sort(key=lambda x: x["date"], reverse=True)

    return templates.TemplateResponse("index.html", {
        "request": request,
        "days": day_cards,
        "user": user,
        "total_count": total_count,
        "has_more": has_more,
        "current_limit": limit,
        "current_offset": offset,
        "start_date": start_date,
        "end_date": end_date,
    })


@app.get("/input", response_class=HTMLResponse)
def input_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("input.html", {"request": request, "user": user})


@app.get("/search", response_class=HTMLResponse)
def search_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("search.html", {"request": request, "user": user})


def _calc_invoice_total(data_rows):
    """Calculate total amount for an invoice, excluding summary rows."""
    total = 0.0
    for r in data_rows:
        try:
            desc = str(r.get("Line Item Description", "")).upper().strip()
            summary_patterns = [
                r'\bSUBTOTAL\b', r'\bGRAND TOTAL\b', r'\bBALANCE DUE\b', r'\bAMOUNT DUE\b',
                r'\bTOTAL DUE\b', r'\bTOTAL CHARGES?\b', r'\bTOTAL AMOUNT\b'
            ]
            exact_summary = desc in ['TOTAL', 'SUBTOTAL', 'TAX', 'TAXES', 'BALANCE', 'AMOUNT DUE', 'TOTAL DUE']
            is_summary_row = exact_summary or any(re.search(p, desc) for p in summary_patterns)
            if not is_summary_row:
                amt = r.get("Line Item Charge") or r.get("AMOUNT") or r.get("amount") or r.get("Amount") or r.get("LINE_AMOUNT") or 0
                charge_str = str(amt).replace("$", "").replace(",", "").strip()
                total += float(charge_str) if charge_str else 0.0
        except (ValueError, TypeError):
            pass
    return total


# -------- Search Index Builder --------

def _index_one_day(y: str, m: str, d: str) -> List[Dict]:
    """Load a single day from S3, extract per-invoice metadata for the search index."""
    entries = []
    try:
        rows = load_day(y, m, d)
    except Exception:
        return entries

    by_pdf: dict = {}
    for r in rows:
        key = r.get("__s3_key__", "")
        if not key:
            continue
        pid = pdf_id_from_key(key)
        if pid not in by_pdf:
            by_pdf[pid] = {"rows": [], "key": key}
        by_pdf[pid]["rows"].append(r)

    date_str = f"{y}-{m}-{d}"
    for pid, data in by_pdf.items():
        first = data["rows"][0]
        acct = str(
            first.get("Account Number") or first.get("ACCOUNT_ID") or first.get("account_id") or
            first.get("Account ID") or first.get("ACCOUNT_NUMBER") or ""
        )
        vend = str(
            first.get("EnrichedVendorName") or first.get("Vendor Name") or first.get("VENDOR") or
            first.get("vendor") or first.get("Vendor") or first.get("VENDOR_NAME") or ""
        )
        prop = str(
            first.get("EnrichedPropertyName") or first.get("EnrichedProperty") or first.get("PROPERTY") or
            first.get("property") or first.get("Property") or first.get("PROPERTY_NAME") or ""
        )
        total = _calc_invoice_total(data["rows"])
        entries.append({
            "pdf_id": pid,
            "date": date_str,
            "account": acct,
            "account_l": acct.lower(),
            "vendor": vend,
            "vendor_l": vend.lower(),
            "property": prop,
            "property_l": prop.lower(),
            "amount": total,
        })
    return entries


_SEARCH_INDEX_S3_KEY = "Bill_Parser_Config/search_index.json.gz"


def _save_search_index_to_s3():
    """Persist the search index to S3 as compressed JSON so it survives restarts."""
    import gzip
    idx = _SEARCH_INDEX
    # Strip the _l (lowercase) fields to save space  we rebuild them on load
    slim_entries = [
        {"pdf_id": e["pdf_id"], "date": e["date"], "account": e["account"],
         "vendor": e["vendor"], "property": e["property"], "amount": e["amount"]}
        for e in idx["entries"]
    ]
    payload = {
        "entries": slim_entries,
        "dates_indexed": sorted(idx["dates_indexed"]),
        "saved_at": dt.datetime.utcnow().isoformat() + "Z",
    }
    raw = json.dumps(payload, ensure_ascii=False).encode("utf-8")
    compressed = gzip.compress(raw)
    s3.put_object(Bucket=BUCKET, Key=_SEARCH_INDEX_S3_KEY, Body=compressed,
                  ContentType="application/gzip")
    print(f"[SEARCH INDEX] Saved to S3: {len(slim_entries)} entries, {len(compressed)//1024}KB compressed")


def _load_search_index_from_s3() -> bool:
    """Load persisted search index from S3. Returns True if loaded successfully."""
    import gzip
    try:
        obj = s3.get_object(Bucket=BUCKET, Key=_SEARCH_INDEX_S3_KEY)
        compressed = obj["Body"].read()
        raw = gzip.decompress(compressed)
        payload = json.loads(raw)
        entries = payload.get("entries", [])
        dates_indexed = set(payload.get("dates_indexed", []))
        # Rebuild lowercase fields for fast search
        for e in entries:
            e["account_l"] = e["account"].lower()
            e["vendor_l"] = e["vendor"].lower()
            e["property_l"] = e["property"].lower()
        with _SEARCH_INDEX_LOCK:
            _SEARCH_INDEX["entries"] = entries
            _SEARCH_INDEX["dates_indexed"] = dates_indexed
            _SEARCH_INDEX["by_date"] = {d: 0 for d in dates_indexed}
            _SEARCH_INDEX["ready"] = True
            _SEARCH_INDEX["entry_count"] = len(entries)
            _SEARCH_INDEX["last_refresh"] = time.time()
        print(f"[SEARCH INDEX] Loaded from S3: {len(entries)} entries across {len(dates_indexed)} dates (saved {payload.get('saved_at', '?')})")
        return True
    except s3.exceptions.NoSuchKey:
        print("[SEARCH INDEX] No persisted index found in S3, will do full backfill")
        return False
    except Exception as e:
        print(f"[SEARCH INDEX] Error loading from S3: {e}, will do full backfill")
        return False


def _build_search_index(force_full: bool = False):
    """Build or refresh the in-memory search index from Stage 4 data.
    - On startup: loads persisted index from S3, then indexes only new dates.
    - force_full=True: re-indexes everything from scratch.
    - Incremental: only re-indexes today + any new date folders.
    - Persists to S3 after every build so restarts are instant.
    """
    idx = _SEARCH_INDEX
    if idx["loading"]:
        return
    with _SEARCH_INDEX_LOCK:
        idx["loading"] = True

    try:
        dates = list_dates()
        already = idx["dates_indexed"]

        if force_full:
            to_index = dates
            new_entries = []
            new_by_date = {}
        else:
            # Incremental: index new dates + always re-index today
            today_str = dt.date.today().strftime("%Y-%m-%d")
            to_index = [d for d in dates if d["label"] not in already or d["label"] == today_str]
            new_entries = [e for e in idx["entries"] if e["date"] != today_str]
            new_by_date = {k: v for k, v in idx["by_date"].items() if k != today_str}

        if not to_index:
            with _SEARCH_INDEX_LOCK:
                idx["loading"] = False
                idx["last_refresh"] = time.time()
            return

        total_dates = len(to_index)
        indexed = 0
        print(f"[SEARCH INDEX] Indexing {total_dates} date(s)...")

        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = {
                executor.submit(_index_one_day, d["tuple"][0], d["tuple"][1], d["tuple"][2]): d["label"]
                for d in to_index
            }
            for future in as_completed(futures):
                date_label = futures[future]
                try:
                    day_entries = future.result(timeout=120)
                    new_entries.extend(day_entries)
                    new_by_date[date_label] = len(day_entries)
                    indexed += 1
                    if indexed % 10 == 0:
                        print(f"[SEARCH INDEX] ... {indexed}/{total_dates} dates done ({len(new_entries)} invoices)")
                except Exception as e:
                    print(f"[SEARCH INDEX] Error indexing {date_label}: {e}")

        with _SEARCH_INDEX_LOCK:
            idx["entries"] = new_entries
            idx["by_date"] = new_by_date
            idx["dates_indexed"] = {d["label"] for d in dates}
            idx["ready"] = True
            idx["loading"] = False
            idx["last_refresh"] = time.time()
            idx["entry_count"] = len(new_entries)

        print(f"[SEARCH INDEX] Done. {len(new_entries)} invoices indexed across {len(idx['dates_indexed'])} dates.")

        # Persist to S3 so next startup is instant
        try:
            _save_search_index_to_s3()
        except Exception as e:
            print(f"[SEARCH INDEX] Warning: failed to persist to S3: {e}")

    except Exception as e:
        print(f"[SEARCH INDEX] Fatal error: {e}")
        with _SEARCH_INDEX_LOCK:
            idx["loading"] = False


@app.get("/api/search")
def api_search(
    request: Request,
    account: str = "",
    vendor: str = "",
    property: str = "",
    start_date: str = "",
    end_date: str = "",
    user: str = Depends(require_user)
):
    """Instant search against the precomputed in-memory index."""
    account = (account or "").strip().lower()
    vendor = (vendor or "").strip().lower()
    prop = (property or "").strip().lower()

    if not account and not vendor and not prop:
        return JSONResponse({"error": "At least one search term required"}, status_code=400)

    idx = _SEARCH_INDEX
    if not idx["ready"]:
        return JSONResponse({
            "error": "Search index is still building. Please wait a few minutes and try again.",
            "indexing": True,
        }, status_code=503)

    results = []
    max_results = 500

    for entry in idx["entries"]:
        # Date range filter
        if start_date and entry["date"] < start_date:
            continue
        if end_date and entry["date"] > end_date:
            continue
        # Search term filters (contains match)
        if account and account not in entry["account_l"]:
            continue
        if vendor and vendor not in entry["vendor_l"]:
            continue
        if prop and prop not in entry["property_l"]:
            continue

        results.append({
            "date": entry["date"],
            "pdf_id": entry["pdf_id"],
            "account_id": entry["account"],
            "vendor": entry["vendor"],
            "property": entry["property"],
            "amount": entry["amount"],
        })
        if len(results) >= max_results:
            break

    results.sort(key=lambda x: x["date"], reverse=True)
    return {
        "results": results,
        "truncated": len(results) >= max_results,
        "index_size": idx["entry_count"],
        "index_dates": len(idx["dates_indexed"]),
    }


@app.get("/api/search/index-status")
def api_search_index_status(user: str = Depends(require_user)):
    """Check search index status."""
    idx = _SEARCH_INDEX
    return {
        "ready": idx["ready"],
        "loading": idx["loading"],
        "entry_count": idx["entry_count"],
        "dates_indexed": len(idx["dates_indexed"]),
        "last_refresh": idx["last_refresh"],
    }


@app.post("/api/search/rebuild-index")
def api_search_rebuild_index(user: str = Depends(require_user)):
    """Force full rebuild of search index."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin only"}, status_code=403)
    if _SEARCH_INDEX["loading"]:
        return {"status": "already_loading"}
    threading.Thread(target=_build_search_index, args=(True,), daemon=True).start()
    return {"status": "rebuild_started"}


# Default to the Pending_Parsing prefix to match the S3 event rule
INPUT_PREFIX = os.getenv("INPUT_PREFIX", "Bill_Parser_1_Pending_Parsing/")


@app.post("/api/upload_input")
def api_upload_input(file: UploadFile = File(...), user: str = Depends(require_user)):
    """Upload a PDF to Bill Parser 1 input location."""
    try:
        # Validate
        fname = file.filename or "uploaded.pdf"
        if not fname.lower().endswith(".pdf"):
            return JSONResponse({"error": "file must be a .pdf"}, status_code=400)
        # time + sanitize filename (no path separators)
        now = dt.datetime.utcnow()
        y = now.strftime('%Y'); m = now.strftime('%m'); d = now.strftime('%d')
        ts = now.strftime('%Y%m%dT%H%M%SZ')
        base = os.path.basename(fname).replace("\\", "_").replace("/", "_")
        # Always write flat (no yyyy/mm/dd) to trigger Bill Parser 1 S3 event rules
        key = f"{INPUT_PREFIX}{ts}_{base}"
        body = file.file.read()
        if not body:
            return JSONResponse({"error": "empty file"}, status_code=400)
        s3.put_object(Bucket=BUCKET, Key=key, Body=body, ContentType=file.content_type or 'application/pdf')
        return {"ok": True, "key": key}
    except Exception as e:
        import traceback
        return JSONResponse({"error": _sanitize_error(e, "API request")}, status_code=500)


# -------- Scraper Import APIs --------
# Cached mappings for scraper bucket (integration_uuid -> provider, account_uuid -> account_id)
_scraper_integration_map: dict[str, str] = {}  # integration_uuid -> provider_name
_scraper_account_map: dict[str, dict] = {}  # account_uuid -> {account_id, provider}

def _load_scraper_mappings():
    """Load scraper UUID mappings from CSV files in the app directory."""
    global _scraper_integration_map, _scraper_account_map

    if _scraper_integration_map:
        return  # Already loaded

    import csv
    import os as _os

    # Find the CSV files relative to main.py
    base_dir = _os.path.dirname(_os.path.abspath(__file__))

    # Load integration UUID -> provider mapping
    integration_csv = _os.path.join(base_dir, "integration_uuid_provider_map.csv")
    if _os.path.exists(integration_csv):
        with open(integration_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                uuid = row.get('integration_uuid', '').strip()
                provider = row.get('provider', '').strip()
                if uuid and provider:
                    _scraper_integration_map[uuid] = provider
        print(f"[SCRAPER] Loaded {len(_scraper_integration_map)} integration mappings")

    # Load account UUID -> account_id mapping
    account_csv = _os.path.join(base_dir, "account_uuid_provider_map.csv")
    if _os.path.exists(account_csv):
        with open(account_csv, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                uuid = row.get('account_uuid', '').strip()
                account_id = row.get('account_id', '').strip()
                provider = row.get('provider', '').strip()
                if uuid and account_id:
                    _scraper_account_map[uuid] = {'account_id': account_id, 'provider': provider}
        print(f"[SCRAPER] Loaded {len(_scraper_account_map)} account mappings")


@app.get("/api/scraper/providers")
def api_scraper_providers(user: str = Depends(require_user)):
    """List all providers in the scraper bucket with friendly names."""
    _load_scraper_mappings()

    try:
        # List top-level prefixes (integration UUIDs or provider names)
        paginator = s3.get_paginator('list_objects_v2')
        prefixes = set()

        for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Delimiter='/'):
            for prefix in page.get('CommonPrefixes', []):
                p = prefix.get('Prefix', '').rstrip('/')
                if p and p not in ('traces', 'unknown'):
                    prefixes.add(p)

        # Build provider list with friendly names
        providers = []
        for folder in sorted(prefixes):
            # Check if it's a UUID that maps to a provider name
            provider_name = _scraper_integration_map.get(folder, folder)
            providers.append({
                'folder': folder,
                'name': provider_name.replace('_', ' ').title(),
                'is_uuid': folder != provider_name
            })

        # Sort by friendly name
        providers.sort(key=lambda x: x['name'].lower())

        return {"ok": True, "providers": providers}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/scraper/accounts/{provider_folder}")
def api_scraper_accounts(provider_folder: str, user: str = Depends(require_user)):
    """List accounts for a provider in the scraper bucket."""
    _load_scraper_mappings()

    try:
        # The structure is: {integration_uuid}/bills/{provider_name}/{account_id}/
        # First check if there's a bills subfolder
        accounts = []

        # Try listing under /bills/{provider_name}/
        provider_name = _scraper_integration_map.get(provider_folder, provider_folder)
        bills_prefix = f"{provider_folder}/bills/{provider_name}/"

        paginator = s3.get_paginator('list_objects_v2')
        try:
            for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=bills_prefix, Delimiter='/'):
                for prefix in page.get('CommonPrefixes', []):
                    account_folder = prefix.get('Prefix', '').rstrip('/').split('/')[-1]
                    if account_folder:
                        accounts.append({
                            'folder': account_folder,
                            'account_id': account_folder,
                            'path': f"{provider_folder}/bills/{provider_name}/{account_folder}"
                        })
        except Exception:
            pass

        # If no accounts found, try direct listing under provider folder
        if not accounts:
            for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=f"{provider_folder}/", Delimiter='/'):
                for prefix in page.get('CommonPrefixes', []):
                    subfolder = prefix.get('Prefix', '').rstrip('/').split('/')[-1]
                    if subfolder and subfolder != 'bills':
                        accounts.append({
                            'folder': subfolder,
                            'account_id': subfolder,
                            'path': f"{provider_folder}/{subfolder}"
                        })

        # Filter out accounts with no PDFs (parallel check)
        if accounts:
            from concurrent.futures import ThreadPoolExecutor, as_completed

            def _has_pdfs(acct):
                try:
                    prefix = acct['path'] + '/'
                    resp = s3.list_objects_v2(Bucket=SCRAPER_BUCKET, Prefix=prefix, MaxKeys=20)
                    for obj in resp.get('Contents', []):
                        if obj['Key'].lower().endswith('.pdf'):
                            return True
                    return False
                except Exception:
                    return False

            with ThreadPoolExecutor(max_workers=20) as executor:
                futures = {executor.submit(_has_pdfs, a): a for a in accounts}
                filtered = []
                for f in as_completed(futures):
                    if f.result():
                        filtered.append(futures[f])
                accounts = sorted(filtered, key=lambda a: a['account_id'])

        return {"ok": True, "accounts": accounts, "provider": provider_name}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/scraper/pdfs/{provider_folder}/{account_folder:path}")
def api_scraper_pdfs(provider_folder: str, account_folder: str, user: str = Depends(require_user)):
    """List PDFs for an account in the scraper bucket."""
    _load_scraper_mappings()

    try:
        provider_name = _scraper_integration_map.get(provider_folder, provider_folder)

        # Build the full path
        if '/' in account_folder:
            # Path already includes intermediate folders
            prefix = f"{provider_folder}/{account_folder}/"
        else:
            # Try the standard path structure
            prefix = f"{provider_folder}/bills/{provider_name}/{account_folder}/"

        pdfs = []
        paginator = s3.get_paginator('list_objects_v2')

        for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=prefix):
            for obj in page.get('Contents', []):
                key = obj.get('Key', '')
                if key.lower().endswith('.pdf'):
                    filename = key.split('/')[-1]
                    size = obj.get('Size', 0)
                    last_mod = obj.get('LastModified')
                    pdfs.append({
                        'key': key,
                        'filename': filename,
                        'size': size,
                        'size_kb': round(size / 1024, 1),
                        'last_modified': last_mod.isoformat() if last_mod else '',
                        'last_modified_human': last_mod.strftime('%b %d, %Y %I:%M %p') if last_mod else ''
                    })

        # Sort by last modified (newest first)
        pdfs.sort(key=lambda x: x.get('last_modified', ''), reverse=True)

        # Inline cached service dates
        if pdfs:
            date_cache = _batch_get_cached_pdf_dates([p['key'] for p in pdfs])
            for p in pdfs:
                cached = date_cache.get(p['key'])
                if cached:
                    p['service_start'] = cached.get('service_start', '')
                    p['service_end'] = cached.get('service_end', '')

        return {"ok": True, "pdfs": pdfs, "provider": provider_name, "account": account_folder}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/scraper/all-pdfs/{provider_folder}")
def api_scraper_all_pdfs(provider_folder: str, user: str = Depends(require_user)):
    """List ALL PDFs for a provider across ALL accounts."""
    _load_scraper_mappings()

    try:
        provider_name = _scraper_integration_map.get(provider_folder, provider_folder)

        # First get all accounts
        accounts = []
        bills_prefix = f"{provider_folder}/bills/{provider_name}/"

        paginator = s3.get_paginator('list_objects_v2')
        try:
            for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=bills_prefix, Delimiter='/'):
                for prefix in page.get('CommonPrefixes', []):
                    account_folder = prefix.get('Prefix', '').rstrip('/').split('/')[-1]
                    if account_folder:
                        accounts.append({
                            'account_id': account_folder,
                            'prefix': f"{bills_prefix}{account_folder}/"
                        })
        except Exception:
            pass

        # Fallback: try direct listing
        if not accounts:
            for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=f"{provider_folder}/", Delimiter='/'):
                for prefix in page.get('CommonPrefixes', []):
                    subfolder = prefix.get('Prefix', '').rstrip('/').split('/')[-1]
                    if subfolder and subfolder != 'bills':
                        accounts.append({
                            'account_id': subfolder,
                            'prefix': f"{provider_folder}/{subfolder}/"
                        })

        # Now get all PDFs from all accounts
        all_pdfs = []
        for acct in accounts:
            try:
                for page in paginator.paginate(Bucket=SCRAPER_BUCKET, Prefix=acct['prefix']):
                    for obj in page.get('Contents', []):
                        key = obj.get('Key', '')
                        if key.lower().endswith('.pdf'):
                            filename = key.split('/')[-1]
                            size = obj.get('Size', 0)
                            last_mod = obj.get('LastModified')
                            all_pdfs.append({
                                'key': key,
                                'filename': filename,
                                'account_id': acct['account_id'],
                                'size': size,
                                'size_kb': round(size / 1024, 1),
                                'last_modified': last_mod.isoformat() if last_mod else '',
                                'last_modified_human': last_mod.strftime('%b %d, %Y %I:%M %p') if last_mod else ''
                            })
            except Exception:
                continue

        # Sort by last modified (newest first)
        all_pdfs.sort(key=lambda x: x.get('last_modified', ''), reverse=True)

        # Inline cached service dates (batch DDB read  eliminates separate round trip)
        if all_pdfs:
            date_cache = _batch_get_cached_pdf_dates([p['key'] for p in all_pdfs])
            for p in all_pdfs:
                cached = date_cache.get(p['key'])
                if cached:
                    p['service_start'] = cached.get('service_start', '')
                    p['service_end'] = cached.get('service_end', '')

        return {
            "ok": True,
            "pdfs": all_pdfs,
            "provider": provider_name,
            "total_accounts": len(accounts),
            "total_pdfs": len(all_pdfs)
        }
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/scraper/import")
async def api_scraper_import(request: Request, user: str = Depends(require_user)):
    """Import selected PDFs from scraper bucket to bill parser input."""
    try:
        payload = await request.json()
        keys = payload.get('keys', [])

        if not keys:
            return JSONResponse({"error": "No files selected"}, status_code=400)

        if len(keys) > 50:
            return JSONResponse({"error": "Maximum 50 files per import"}, status_code=400)

        imported = []
        errors = []

        for src_key in keys:
            try:
                # Get the PDF from scraper bucket
                obj = s3.get_object(Bucket=SCRAPER_BUCKET, Key=src_key)
                body = obj['Body'].read()

                # Generate destination key in bill parser input
                now = dt.datetime.utcnow()
                ts = now.strftime('%Y%m%dT%H%M%SZ')
                # Use original filename from the key
                orig_filename = src_key.split('/')[-1]
                dest_key = f"{INPUT_PREFIX}{ts}_scraper_{orig_filename}"

                # Upload to bill parser input bucket
                s3.put_object(
                    Bucket=BUCKET,
                    Key=dest_key,
                    Body=body,
                    ContentType='application/pdf'
                )

                imported.append({
                    'source': src_key,
                    'destination': dest_key
                })
            except Exception as e:
                errors.append({
                    'key': src_key,
                    'error': str(e)
                })

        return {
            "ok": True,
            "imported": len(imported),
            "failed": len(errors),
            "details": imported,
            "errors": errors
        }
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


# -------- Gemini PDF Date Extraction --------
_gemini_configured = False
_gemini_model = None

def _init_gemini():
    """Initialize Gemini API with key from Secrets Manager."""
    global _gemini_configured, _gemini_model
    if _gemini_configured:
        return _gemini_model is not None

    _gemini_configured = True
    try:
        # Get API key from Secrets Manager
        sm = boto3.client('secretsmanager', region_name=AWS_REGION)
        secret = sm.get_secret_value(SecretId=GEMINI_SECRET_NAME)
        secret_str = secret['SecretString']

        # Handle malformed JSON (JS object notation without quotes)
        # Extract API keys using regex - they start with AIza
        api_keys = re.findall(r'AIza[A-Za-z0-9_-]+', secret_str)
        if not api_keys:
            # Try standard JSON parsing as fallback
            try:
                secret_data = json.loads(secret_str)
                api_keys = secret_data.get('keys', [])
            except:
                pass

        if not api_keys:
            print("[GEMINI] No API keys found in secret")
            return False

        # Use first available key
        genai.configure(api_key=api_keys[0])
        _gemini_model = genai.GenerativeModel("gemini-2.0-flash")
        print(f"[GEMINI] Initialized successfully with key ending ...{api_keys[0][-4:]}")
        return True
    except Exception as e:
        print(f"[GEMINI] Failed to initialize: {e}")
        return False


def _get_cached_pdf_dates(s3_key: str) -> dict | None:
    """Get cached service dates from DynamoDB."""
    try:
        # Use hash of S3 key as ID
        key_hash = hashlib.sha256(s3_key.encode()).hexdigest()[:16]
        resp = ddb.get_item(
            TableName=DRAFTS_TABLE,
            Key={'pk': {'S': f'scraper_dates#{key_hash}'}}
        )
        item = resp.get('Item')
        if item:
            return {
                'service_start': item.get('service_start', {}).get('S', ''),
                'service_end': item.get('service_end', {}).get('S', ''),
                'extracted_at': item.get('extracted_at', {}).get('S', '')
            }
    except Exception:
        pass
    return None


def _batch_get_cached_pdf_dates(s3_keys: list) -> dict:
    """Batch-fetch cached service dates from DynamoDB using batch_get_item.
    Returns {s3_key: {'service_start': ..., 'service_end': ...}}."""
    if not s3_keys:
        return {}
    results = {}
    pk_to_key = {}
    for key in s3_keys:
        key_hash = hashlib.sha256(key.encode()).hexdigest()[:16]
        pk_to_key[f'scraper_dates#{key_hash}'] = key

    pk_list = list(pk_to_key.keys())
    for i in range(0, len(pk_list), 100):
        batch = pk_list[i:i + 100]
        try:
            resp = ddb.batch_get_item(
                RequestItems={
                    DRAFTS_TABLE: {
                        'Keys': [{'pk': {'S': pk}} for pk in batch],
                        'ProjectionExpression': 'pk, service_start, service_end',
                    }
                }
            )
            for item in resp.get('Responses', {}).get(DRAFTS_TABLE, []):
                pk = item['pk']['S']
                s3_key = pk_to_key.get(pk, '')
                if s3_key:
                    results[s3_key] = {
                        'service_start': item.get('service_start', {}).get('S', ''),
                        'service_end': item.get('service_end', {}).get('S', ''),
                    }
            # Handle unprocessed keys (DDB throttling)
            unprocessed = resp.get('UnprocessedKeys', {}).get(DRAFTS_TABLE, {}).get('Keys', [])
            if unprocessed:
                retry_resp = ddb.batch_get_item(
                    RequestItems={DRAFTS_TABLE: {'Keys': unprocessed, 'ProjectionExpression': 'pk, service_start, service_end'}}
                )
                for item in retry_resp.get('Responses', {}).get(DRAFTS_TABLE, []):
                    pk = item['pk']['S']
                    s3_key = pk_to_key.get(pk, '')
                    if s3_key:
                        results[s3_key] = {
                            'service_start': item.get('service_start', {}).get('S', ''),
                            'service_end': item.get('service_end', {}).get('S', ''),
                        }
        except Exception as e:
            print(f"[SCRAPER CACHE] Batch get error: {e}")
    return results


def _cache_pdf_dates(s3_key: str, service_start: str, service_end: str, raise_on_error: bool = False):
    """Cache extracted service dates to DynamoDB."""
    try:
        key_hash = hashlib.sha256(s3_key.encode()).hexdigest()[:16]
        pk = f'scraper_dates#{key_hash}'
        print(f"[SCRAPER CACHE] Saving dates for {s3_key[:80]} -> pk={pk}, start={service_start}, end={service_end}")
        ddb.put_item(
            TableName=DRAFTS_TABLE,
            Item={
                'pk': {'S': pk},
                's3_key': {'S': s3_key},
                'service_start': {'S': service_start or ''},
                'service_end': {'S': service_end or ''},
                'extracted_at': {'S': dt.datetime.utcnow().isoformat()}
            }
        )
        print(f"[SCRAPER CACHE] Successfully saved pk={pk}")
    except Exception as e:
        import traceback
        print(f"[SCRAPER CACHE] FAILED to save dates for {s3_key}: {e}")
        traceback.print_exc()
        if raise_on_error:
            raise


def _extract_dates_from_pdf(s3_key: str) -> dict:
    """Extract service dates from a PDF using Gemini."""
    # Check cache first
    cached = _get_cached_pdf_dates(s3_key)
    if cached:
        return {'ok': True, 'cached': True, **cached}

    # Initialize Gemini
    if not _init_gemini() or _gemini_model is None:
        return {'ok': False, 'error': 'Gemini not configured'}

    try:
        # Download PDF from S3
        obj = s3.get_object(Bucket=SCRAPER_BUCKET, Key=s3_key)
        pdf_bytes = obj['Body'].read()

        # Limit file size (Gemini has limits)
        if len(pdf_bytes) > 20 * 1024 * 1024:  # 20MB limit
            return {'ok': False, 'error': 'PDF too large'}

        # Send to Gemini
        prompt = """Analyze this utility bill PDF and extract the service period dates.
Return ONLY a JSON object in this exact format (no other text):
{"service_start": "YYYY-MM-DD", "service_end": "YYYY-MM-DD"}

If you cannot find the dates, use empty strings for the values.
Look for phrases like "Service Period", "Bill Period", "From/To dates", "Service From", etc."""

        # Upload PDF as inline data
        response = _gemini_model.generate_content([
            prompt,
            {"mime_type": "application/pdf", "data": pdf_bytes}
        ])

        # Parse response
        text = response.text.strip()
        # Extract JSON from response (may have markdown code blocks)
        if '```' in text:
            text = text.split('```')[1]
            if text.startswith('json'):
                text = text[4:]
            text = text.strip()

        result = json.loads(text)
        service_start = result.get('service_start', '')
        service_end = result.get('service_end', '')

        # Normalize dates to MM/DD/YYYY
        def _normalize_extracted_date(d):
            if not d:
                return d
            d = d.strip()
            # YYYY-MM-DD  MM/DD/YYYY
            if len(d) == 10 and d[4] == '-' and d[7] == '-':
                try:
                    return f"{int(d[5:7]):02d}/{int(d[8:10]):02d}/{d[0:4]}"
                except (ValueError, IndexError):
                    pass
            # Already MM/DD/YYYY or other format  try normalizing
            return _normalize_date_mmddyyyy(d)

        service_start = _normalize_extracted_date(service_start)
        service_end = _normalize_extracted_date(service_end)

        # Cache the results
        _cache_pdf_dates(s3_key, service_start, service_end)

        return {
            'ok': True,
            'cached': False,
            'service_start': service_start,
            'service_end': service_end,
            'extracted_at': dt.datetime.utcnow().isoformat()
        }
    except json.JSONDecodeError as e:
        return {'ok': False, 'error': f'Invalid JSON response: {str(e)}'}
    except Exception as e:
        return {'ok': False, 'error': str(e)}


@app.post("/api/scraper/extract-dates")
async def api_scraper_extract_dates(request: Request, user: str = Depends(require_user)):
    """Extract service dates from PDFs using Gemini AI. Results are cached."""
    try:
        payload = await request.json()
        keys = payload.get('keys', [])

        if not keys:
            return JSONResponse({"error": "No PDF keys provided"}, status_code=400)

        if len(keys) > 20:
            return JSONResponse({"error": "Maximum 20 PDFs per request"}, status_code=400)

        results = {}
        for key in keys:
            results[key] = _extract_dates_from_pdf(key)

        return {"ok": True, "results": results}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/scraper/save-dates")
async def api_scraper_save_dates(request: Request, user: str = Depends(require_user)):
    """Explicitly save extracted service dates to DynamoDB cache."""
    try:
        payload = await request.json()
        dates_to_save = payload.get('dates', {})  # {s3_key: {service_start, service_end}}
        if not dates_to_save:
            return {"ok": True, "saved": 0}

        saved = 0
        errors = []
        for s3_key, date_info in dates_to_save.items():
            try:
                svc_start = date_info.get('service_start', '')
                svc_end = date_info.get('service_end', '')
                _cache_pdf_dates(s3_key, svc_start, svc_end, raise_on_error=True)
                saved += 1
            except Exception as e:
                errors.append(f"{s3_key}: {str(e)}")

        print(f"[SCRAPER SAVE] Saved {saved}/{len(dates_to_save)} date entries, {len(errors)} errors")
        return {"ok": True, "saved": saved, "errors": errors if errors else None}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/scraper/get-cached-dates")
async def api_scraper_get_cached_dates(request: Request, user: str = Depends(require_user)):
    """Get cached service dates for PDFs (no Gemini calls, just cache lookup)."""
    try:
        payload = await request.json()
        keys = payload.get('keys', [])

        if not keys:
            return {"ok": True, "results": {}}

        results = {}
        for key in keys[:500]:  # Limit to 500 keys
            cached = _get_cached_pdf_dates(key)
            if cached:
                results[key] = {'ok': True, 'cached': True, **cached}

        return {"ok": True, "results": results}
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/post", response_class=HTMLResponse)
def post_view(request: Request, user: str = Depends(require_user)):
    """List merged pre-Entrata files (Bill_Parser_6_PreEntrata_Submission) with Title and Status.

    Totals loaded lazily via AJAX to speed up initial render.
    """
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=BUCKET, Prefix=PRE_ENTRATA_PREFIX)
    items = []
    for page in pages:
        for obj in page.get("Contents", []) or []:
            k = obj.get("Key", "")
            if not k.lower().endswith('.jsonl'):
                continue
            size = int(obj.get('Size', 0) or 0)
            lm = obj.get('LastModified')
            last_mod = lm.isoformat() if lm else ''
            last_mod_ts = lm.timestamp() if lm else 0
            # Friendly PT time
            last_mod_human = ''
            try:
                if lm:
                    pac = lm.astimezone(ZoneInfo('America/Los_Angeles'))
                    last_mod_human = pac.strftime('%b %d, %Y %I:%M %p %Z')
            except Exception:
                last_mod_human = last_mod
            # PERF: Only read first line for title/status - total loaded lazily via AJAX
            title = ""
            status = ""
            total_amount = None  # None = not loaded yet, will be loaded via AJAX
            try:
                # Use S3 Select or range read to get just first line (much faster)
                # Read enough bytes to capture large first lines with enriched data
                obj_data = s3.get_object(Bucket=BUCKET, Key=k, Range='bytes=0-16384')
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                first_line = txt.split('\n')[0].strip() if txt else ''

                submitter = ""
                submitted_at = ""
                if first_line:
                    rec = json.loads(first_line)
                    acct = (rec.get('Account Number') or rec.get('AccountNumber') or '').strip()
                    prop = (rec.get('EnrichedPropertyName') or rec.get('Property Name') or '').strip()
                    vend = (rec.get('EnrichedVendorName') or rec.get('Vendor Name') or rec.get('Vendor') or '').strip()
                    parts = k[len(PRE_ENTRATA_PREFIX):].split('/')
                    y = parts[0].split('=')[1] if len(parts)>0 and '=' in parts[0] else ''
                    m = parts[1].split('=')[1] if len(parts)>1 and '=' in parts[1] else ''
                    d = parts[2].split('=')[1] if len(parts)>2 and '=' in parts[2] else ''
                    date_str = f"{y}-{m}-{d}" if y and m and d else ''
                    # Use pre-computed Title if available, otherwise build from fields
                    # Template expects 4 parts: Account | Property | Vendor | Date
                    existing_title = str(rec.get("Title", "")).strip()
                    if existing_title and " | " in existing_title:
                        title = existing_title
                    else:
                        title = f"{acct} | {prop} | {vend} | {date_str}"
                    status = str(rec.get("Status", "")).strip() or "Not Posted"
                    if not status:
                        status = "Not Posted"
                    submitter = str(rec.get("Submitter", "")).strip()
                    submitted_at = str(rec.get("SubmittedAt", "")).strip()
            except Exception:
                submitter = ""
                submitted_at = ""
                # Fallback: parse from filename (format: Property-Vendor-Account-dates_timestamp.jsonl)
                try:
                    basename = k.rsplit('/', 1)[-1].replace('.jsonl', '')
                    parts = k[len(PRE_ENTRATA_PREFIX):].split('/')
                    y = parts[0].split('=')[1] if len(parts) > 0 and '=' in parts[0] else ''
                    m = parts[1].split('=')[1] if len(parts) > 1 and '=' in parts[1] else ''
                    d = parts[2].split('=')[1] if len(parts) > 2 and '=' in parts[2] else ''
                    date_str = f"{y}-{m}-{d}" if y and m and d else ''
                    # Remove timestamp suffix and use filename as property fallback
                    name_part = basename.rsplit('_', 1)[0] if '_' in basename and len(basename.rsplit('_', 1)) > 1 else basename
                    # Always 4 parts: Account | Property | Vendor | Date
                    title = f" | {name_part} |  | {date_str}"
                except Exception:
                    title = f" | {k.rsplit('/', 1)[-1]} |  | "
            items.append({
                "key": k,
                "size": size,
                "last_modified": last_mod,
                "last_modified_ts": last_mod_ts,
                "last_modified_human": last_mod_human,
                "title": title,
                "status": status or "Not Posted",
                "total_amount": total_amount,  # Loaded lazily
                "submitter": submitter,
                "submitted_at": submitted_at
            })
    items.sort(key=lambda x: x.get("last_modified_ts") or 0, reverse=True)
    return templates.TemplateResponse("post.html", {"request": request, "items": items, "user": user})


@app.get("/api/post/total")
def api_post_total(key: str, user: str = Depends(require_user)):
    """PERF: Lazy-load total for a single POST file. Called via AJAX after page renders."""
    if not key:
        return {"total": 0}

    # Check per-file cache
    cache_key = f"post_total_{key}"
    now = time.time()
    ent = _CACHE.get(cache_key)
    if ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        return {"total": ent.get("data", 0)}

    total_amount = 0.0
    try:
        obj_data = s3.get_object(Bucket=BUCKET, Key=key)
        txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
        for line in txt.splitlines():
            line = line.strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                total_amount += float(charge_str) if charge_str else 0.0
            except (ValueError, TypeError, json.JSONDecodeError):
                pass
    except Exception as e:
        print(f"[POST TOTAL] Error reading {key}: {e}")

    _CACHE[cache_key] = {"ts": time.time(), "data": total_amount}
    return {"total": total_amount}


@app.get("/ubi", response_class=HTMLResponse)
def ubi_view(request: Request, user: str = Depends(require_user)):
    """Stub page to classify posted records for UBI with a selectable post date (1st of month)."""
    # Next few first-of-month options
    today = dt.date.today().replace(day=1)
    options = []
    for i in range(0, 6):
        mo = (today.month - 1 + i) % 12 + 1
        yr = today.year + ((today.month - 1 + i) // 12)
        options.append(f"{yr:04d}-{mo:02d}-01")
    return templates.TemplateResponse("ubi.html", {"request": request, "options": options, "user": user})


@app.get("/billback", response_class=HTMLResponse)
def billback_view(request: Request, user: str = Depends(require_user)):
    """BILLBACK page - manage invoices for tenant billback after posting to Entrata."""
    return templates.TemplateResponse("billback.html", {"request": request, "user": user})


@app.get("/billback/summary", response_class=HTMLResponse)
def billback_summary_view(request: Request, user: str = Depends(require_user)):
    """BILLBACK Summary page - view aggregated billback data by property, vendor, charge code, and month."""
    return templates.TemplateResponse("billback_summary.html", {"request": request, "user": user})


@app.get("/master-bills", response_class=HTMLResponse)
def master_bills_view(request: Request, user: str = Depends(require_user)):
    """UBI Master Bills page - view aggregated UBI billback master bills."""
    return templates.TemplateResponse("master-bills.html", {"request": request, "user": user})


@app.get("/ubi-batch", response_class=HTMLResponse)
def ubi_batch_view(request: Request, user: str = Depends(require_user)):
    """UBI Batch Management page - create and manage UBI billback batches for Snowflake export."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("ubi-batch.html", {"request": request, "user": user})


@app.get("/history", response_class=HTMLResponse)
def history_view(request: Request, user: str = Depends(require_user)):
    """HISTORY page - view archived bills and posting history."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("history.html", {"request": request, "user": user})


@app.get("/api/history/archived")
def api_history_archived(request: Request, user: str = Depends(require_user), start: str = "", end: str = ""):
    """Load archived line items from Historical Archive for history view."""
    if not start or not end:
        return JSONResponse({"error": "start and end dates required"}, status_code=400)

    try:
        # Parse dates
        start_date = dt.datetime.strptime(start, "%Y-%m-%d").date()
        end_date = dt.datetime.strptime(end, "%Y-%m-%d").date()

        # List all JSONL files in Historical Archive within date range
        line_items = []
        prefix = HIST_ARCHIVE_PREFIX

        # Iterate through dates in range
        current_date = start_date
        while current_date <= end_date:
            year = current_date.year
            month = current_date.month
            day = current_date.day

            # Build prefix for this day
            day_prefix = f"{prefix}yyyy={year}/mm={month:02d}/dd={day:02d}/"

            try:
                # List objects for this day
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                    if 'Contents' not in page:
                        continue

                    for obj in page['Contents']:
                        key = obj['Key']
                        if not key.endswith('.jsonl'):
                            continue

                        # Download and parse JSONL
                        try:
                            response = s3.get_object(Bucket=BUCKET, Key=key)
                            content = response['Body'].read().decode('utf-8')

                            for line in content.strip().split('\n'):
                                if not line.strip():
                                    continue
                                try:
                                    item = json.loads(line)
                                    item['__s3_key__'] = key
                                    item['__archived_date__'] = f"{year}-{month:02d}-{day:02d}"
                                    line_items.append(item)
                                except Exception as e:
                                    print(f"Error parsing line from {key}: {e}")
                        except Exception as e:
                            print(f"Error reading {key}: {e}")
            except Exception as e:
                print(f"Error listing {day_prefix}: {e}")

            # Move to next day
            current_date += dt.timedelta(days=1)

        return {"line_items": line_items, "count": len(line_items)}

    except Exception as e:
        print(f"Error in api_history_archived: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/archive")
async def api_billback_archive(request: Request, user: str = Depends(require_user)):
    """Archive selected billback line items from Stage 7 (PostEntrata) to Historical Archive."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    items = payload.get("items", [])
    if not items:
        return JSONResponse({"error": "No items provided"}, status_code=400)

    try:
        # Group items by S3 key
        from collections import defaultdict
        items_by_key = defaultdict(list)

        for item in items:
            s3_key = item.get("__s3_key__")
            if s3_key:
                items_by_key[s3_key].append(item)

        archived_count = 0
        errors = []

        # Archive each file
        for key, key_items in items_by_key.items():
            try:
                # Read the entire file
                body = _read_s3_text(BUCKET, key)
                all_rows = []
                remaining_rows = []

                # Parse all lines
                for ln in body.splitlines():
                    ln = (ln or '').strip()
                    if not ln:
                        continue
                    try:
                        row = json.loads(ln)
                        all_rows.append(row)
                    except Exception:
                        pass

                # Filter out the items to archive (match by account number and bill period)
                archived_items = []
                for row in all_rows:
                    should_archive = False
                    for item in key_items:
                        if (row.get('Account Number') == item.get('Account Number') and
                            row.get('Bill Period Start') == item.get('Bill Period Start') and
                            row.get('Bill Period End') == item.get('Bill Period End')):
                            should_archive = True
                            break

                    if should_archive:
                        archived_items.append(row)
                        archived_count += 1
                    else:
                        remaining_rows.append(row)

                # Archive the selected items
                if archived_items:
                    y, m, d = _extract_ymd_from_key(key)
                    base = _basename_from_key(key)
                    archive_key = _write_jsonl(HIST_ARCHIVE_PREFIX, y, m, d, base.replace('.jsonl',''), archived_items)
                    print(f"[BILLBACK ARCHIVE] Archived {len(archived_items)} items to {archive_key}")

                # Rewrite the original file with remaining items (or delete if empty)
                if remaining_rows:
                    y, m, d = _extract_ymd_from_key(key)
                    base = _basename_from_key(key)
                    new_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl',''), remaining_rows)
                    # Delete the ORIGINAL file since we wrote remaining items to a new file
                    if new_key != key:
                        s3.delete_object(Bucket=BUCKET, Key=key)
                        print(f"[BILLBACK ARCHIVE] Deleted original file {key}, remaining items in {new_key}")
                else:
                    s3.delete_object(Bucket=BUCKET, Key=key)
                    print(f"[BILLBACK ARCHIVE] Deleted empty file {key}")

            except Exception as e:
                print(f"[BILLBACK ARCHIVE] Error archiving {key}: {e}")
                errors.append({"key": key, "error": str(e)})

        if errors:
            return {"ok": True, "archived": archived_count, "errors": errors}
        else:
            return {"ok": True, "archived": archived_count}

    except Exception as e:
        print(f"Error in api_billback_archive: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/posted")
def api_billback_posted(request: Request, user: str = Depends(require_user), start: str = "", end: str = ""):
    """Load posted line items from Stage 7 (PostEntrata) for billback review."""
    if not start or not end:
        return JSONResponse({"error": "start and end dates required"}, status_code=400)

    try:
        # Parse dates
        start_date = dt.datetime.strptime(start, "%Y-%m-%d").date()
        end_date = dt.datetime.strptime(end, "%Y-%m-%d").date()

        # List all JSONL files in Stage 7 within date range
        line_items = []
        prefix = POST_ENTRATA_PREFIX

        # Iterate through dates in range
        current_date = start_date
        while current_date <= end_date:
            year = current_date.year
            month = current_date.month
            day = current_date.day

            # Build prefix for this day
            day_prefix = f"{prefix}yyyy={year}/mm={month:02d}/dd={day:02d}/"

            try:
                # List objects for this day
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                    if 'Contents' not in page:
                        continue

                    for obj in page['Contents']:
                        key = obj['Key']
                        if not key.endswith('.jsonl'):
                            continue

                        # Download and parse JSONL
                        try:
                            response = s3.get_object(Bucket=BUCKET, Key=key)
                            content = response['Body'].read().decode('utf-8')

                            for line in content.strip().split('\n'):
                                if not line.strip():
                                    continue
                                try:
                                    item = json.loads(line)
                                    item['__s3_key__'] = key
                                    line_items.append(item)
                                except Exception as e:
                                    print(f"Error parsing line from {key}: {e}")
                        except Exception as e:
                            print(f"Error reading {key}: {e}")
            except Exception as e:
                print(f"Error listing {day_prefix}: {e}")

            # Move to next day
            current_date += dt.timedelta(days=1)

        return {"line_items": line_items, "count": len(line_items)}

    except Exception as e:
        print(f"Error in api_billback_posted: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/save")
async def api_billback_save(request: Request, user: str = Depends(require_user)):
    """Save billback line items with notes and period assignments."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    line_items = payload.get("line_items", [])
    if not line_items:
        return JSONResponse({"error": "No line items provided"}, status_code=400)

    try:
        import uuid
        from decimal import Decimal

        for item in line_items:
            line_item_id = item.get("line_item_id") or str(uuid.uuid4())
            billback_period = item.get("billback_period")

            if not billback_period:
                continue  # Skip items without assigned period

            # Save to DynamoDB
            ddb.put_item(
                TableName="jrk-bill-billback-master",
                Item={
                    "billback_period": {"S": billback_period},
                    "line_item_id": {"S": line_item_id},
                    "property_id": {"S": str(item.get("property_id", ""))},
                    "property_name": {"S": str(item.get("property_name", ""))},
                    "vendor_id": {"S": str(item.get("vendor_id", ""))},
                    "vendor_name": {"S": str(item.get("vendor_name", ""))},
                    "account_number": {"S": str(item.get("account_number", ""))},
                    "service_address": {"S": str(item.get("service_address", ""))},
                    "utility_type": {"S": str(item.get("utility_type", ""))},
                    "charge_code": {"S": str(item.get("charge_code", ""))},
                    "bill_period_start": {"S": str(item.get("bill_period_start", ""))},
                    "bill_period_end": {"S": str(item.get("bill_period_end", ""))},
                    "line_charge": {"N": str(float(item.get("line_charge", 0)))},
                    "billback_amount": {"N": str(float(item.get("billback_amount", 0)))},
                    "notes": {"S": str(item.get("notes", ""))},
                    "status": {"S": "draft"},
                    "created_by": {"S": user},
                    "created_utc": {"S": dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z")},
                    "updated_by": {"S": user},
                    "updated_utc": {"S": dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z")},
                }
            )

        return {"ok": True, "saved": len([i for i in line_items if i.get("billback_period")])}
    except Exception as e:
        print(f"Error in api_billback_save: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/submit")
async def api_billback_submit(request: Request, user: str = Depends(require_user)):
    """Submit billback line items to master bills (mark as submitted)."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "Invalid JSON"}, status_code=400)

    line_item_ids = payload.get("line_item_ids", [])
    if not line_item_ids:
        return JSONResponse({"error": "No line items provided"}, status_code=400)

    try:
        # First, query for all items to update
        submitted_count = 0
        for line_item_id in line_item_ids:
            # We need to find the item by line_item_id across all periods
            # This is inefficient, but works for now. Consider adding a GSI in production.

            # For now, we'll assume the client sends both line_item_id and billback_period
            # Let's update the API to require billback_period as well
            pass

        return {"ok": True, "submitted": submitted_count}
    except Exception as e:
        print(f"Error in api_billback_submit: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/summary")
def api_billback_summary(user: str = Depends(require_user), group_by: str = "month"):
    """Get billback summary grouped by property, vendor, charge_code, or month."""
    if group_by not in ["property", "vendor", "charge_code", "month"]:
        return JSONResponse({"error": "Invalid group_by parameter"}, status_code=400)

    try:
        # Scan the billback master table (paginated to handle large tables)
        items = []
        scan_kwargs = {"TableName": "jrk-bill-billback-master"}
        while True:
            response = ddb.scan(**scan_kwargs)
            items.extend(response.get("Items", []))
            if "LastEvaluatedKey" not in response:
                break
            scan_kwargs["ExclusiveStartKey"] = response["LastEvaluatedKey"]

        print(f"[BILLBACK SUMMARY] Found {len(items)} items in table, grouping by {group_by}")

        # Process and group items
        from collections import defaultdict
        summary = defaultdict(lambda: {"total_amount": 0, "count": 0, "items": []})

        for item in items:
            try:
                key = ""
                if group_by == "property":
                    key = item.get("property_name", {}).get("S", "Unknown")
                elif group_by == "vendor":
                    key = item.get("vendor_name", {}).get("S", "Unknown")
                elif group_by == "charge_code":
                    key = item.get("charge_code", {}).get("S", "Unknown")
                elif group_by == "month":
                    key = item.get("billback_period", {}).get("S", "Unknown")

                amount = float(item.get("billback_amount", {}).get("N", "0"))
                summary[key]["total_amount"] += amount
                summary[key]["count"] += 1
                summary[key]["items"].append({
                    "line_item_id": item.get("line_item_id", {}).get("S", ""),
                    "property_name": item.get("property_name", {}).get("S", ""),
                    "vendor_name": item.get("vendor_name", {}).get("S", ""),
                    "charge_code": item.get("charge_code", {}).get("S", ""),
                    "billback_period": item.get("billback_period", {}).get("S", ""),
                    "billback_amount": amount,
                    "notes": item.get("notes", {}).get("S", ""),
                })
            except Exception as item_error:
                print(f"[BILLBACK SUMMARY] Error processing item: {item_error}")
                continue

        # Convert to list
        result = []
        for key, data in summary.items():
            result.append({
                "group_key": key,
                "total_amount": round(data["total_amount"], 2),
                "count": data["count"],
                "items": data["items"]
            })

        # Sort by total_amount descending
        result.sort(key=lambda x: x["total_amount"], reverse=True)

        print(f"[BILLBACK SUMMARY] Returning {len(result)} groups")
        return {"summary": result, "group_by": group_by}
    except Exception as e:
        print(f"[BILLBACK SUMMARY] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/ubi/filter-options")
def api_billback_ubi_filter_options(
    user: str = Depends(require_user),
    days_back: int = 60
):
    """Return unique properties, vendors, and GL codes for the filter drawer.

    This endpoint scans Stage 7 files and returns unique values for filtering.
    Results are cached for 5 minutes to avoid repeated scanning.
    """
    try:
        import hashlib
        from datetime import datetime, timedelta

        cache_key = f"filter_options_{days_back}"
        cached = getattr(api_billback_ubi_filter_options, '_cache', {})
        cache_time = getattr(api_billback_ubi_filter_options, '_cache_time', {})

        # Check cache (5 minute TTL)
        if cache_key in cached and cache_key in cache_time:
            if (datetime.now() - cache_time[cache_key]).total_seconds() < 300:
                return cached[cache_key]

        start_time = datetime.now()

        # Load UBI accounts (must be both is_ubi and is_tracked)
        accounts_to_track = _get_accounts_to_track()
        ubi_account_keys = set()
        for acct in accounts_to_track:
            if acct.get("is_ubi") == True and acct.get("is_tracked", True):
                prop_id = str(acct.get("propertyId", "")).strip()
                vendor_id = str(acct.get("vendorId", "")).strip()
                acct_num = str(acct.get("accountNumber", "")).strip()
                if prop_id and acct_num:
                    ubi_account_keys.add(f"{prop_id}|{vendor_id}|{acct_num}")
                    ubi_account_keys.add(f"{prop_id}||{acct_num}")

        # Scan Stage 7 files
        properties = set()
        vendors = set()
        gl_codes = set()

        today = datetime.now()
        dates_to_scan = []
        for i in range(days_back):
            d = today - timedelta(days=i)
            dates_to_scan.append(d)

        def scan_date(d):
            local_props = set()
            local_vendors = set()
            local_gls = set()

            # Scan both Stage 7 (PostEntrata) and Stage 8 (UBI Assigned) for filter options
            prefixes_to_scan = [
                f"{POST_ENTRATA_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/",
                f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            ]

            for prefix in prefixes_to_scan:
                try:
                    paginator = s3.get_paginator("list_objects_v2")
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get("Contents", []):
                            key = obj["Key"]
                            if not key.endswith(".jsonl"):
                                continue
                            try:
                                resp = s3.get_object(Bucket=BUCKET, Key=key)
                                for line in resp["Body"].read().decode("utf-8").strip().split("\n"):
                                    if not line.strip():
                                        continue
                                    row = json.loads(line)

                                    # Check if this is a UBI account
                                    prop_id = str(row.get("EnrichedPropertyID") or row.get("propertyId") or "").strip()
                                    vendor_id = str(row.get("EnrichedVendorID") or row.get("vendorId") or "").strip()
                                    acct_num = str(row.get("Account Number") or "").strip()

                                    key1 = f"{prop_id}|{vendor_id}|{acct_num}"
                                    key2 = f"{prop_id}||{acct_num}"
                                    if key1 not in ubi_account_keys and key2 not in ubi_account_keys:
                                        continue

                                    # Collect unique values
                                    prop_name = row.get("EnrichedPropertyName") or row.get("Property Name") or ""
                                    if prop_name:
                                        local_props.add(prop_name)

                                    vendor_name = row.get("EnrichedVendorName") or row.get("Vendor Name") or ""
                                    if vendor_name:
                                        local_vendors.add(vendor_name)

                                    gl_code = row.get("EnrichedGLAccountNumber") or row.get("GL Account Number") or ""
                                    if gl_code:
                                        local_gls.add(gl_code)
                            except Exception as e:
                                pass
                except Exception as e:
                    pass

            return local_props, local_vendors, local_gls

        futures = {_GLOBAL_EXECUTOR.submit(scan_date, d): d for d in dates_to_scan}
        try:
            for future in as_completed(futures, timeout=100):
                try:
                    local_props, local_vendors, local_gls = future.result()
                    properties.update(local_props)
                    vendors.update(local_vendors)
                    gl_codes.update(local_gls)
                except Exception:
                    pass
        except TimeoutError:
            print(f"[FILTER OPTIONS] Timeout after 100s  returning partial results ({len(properties)} props, {len(vendors)} vendors, {len(gl_codes)} GLs)")

        elapsed = (datetime.now() - start_time).total_seconds()

        # Also include ALL properties from the dimension table so filtering works
        # even for properties without recent Stage 7 data
        try:
            dim_properties = _load_dim_records(DIM_PROPERTY_PREFIX)
            for r in dim_properties:
                name = (
                    r.get("name") or r.get("NAME") or r.get("propertyName") or
                    r.get("Property Name") or r.get("PROPERTY_NAME") or
                    r.get("Property") or r.get("PROPERTY")
                )
                if name and str(name).strip():
                    properties.add(str(name).strip())
        except Exception as e:
            print(f"[FILTER OPTIONS] Error loading dim properties: {e}")

        # Also include ALL GL codes from the dimension table so filtering works
        # even for GL codes not seen in recent invoice data
        try:
            dim_gls = _load_dim_records(DIM_GL_PREFIX)
            for r in dim_gls:
                # Try formatted first (e.g., "5190-0000"), then raw account number
                gl_num = (
                    r.get("FORMATTED_ACCOUNT_NUMBER") or r.get("formattedAccountNumber") or
                    r.get("glAccountNumber") or r.get("GL_ACCOUNT_NUMBER") or
                    r.get("accountNumber") or r.get("ACCOUNT_NUMBER") or
                    r.get("glNumber") or r.get("GL_NUMBER") or ""
                )
                if gl_num and str(gl_num).strip():
                    gl_codes.add(str(gl_num).strip())
        except Exception as e:
            print(f"[FILTER OPTIONS] Error loading dim GL codes: {e}")

        result = {
            "properties": sorted(list(properties)),
            "vendors": sorted(list(vendors)),
            "gl_codes": sorted(list(gl_codes)),
            "scan_time_seconds": round(elapsed, 2)
        }

        # Cache the result
        if not hasattr(api_billback_ubi_filter_options, '_cache'):
            api_billback_ubi_filter_options._cache = {}
            api_billback_ubi_filter_options._cache_time = {}
        api_billback_ubi_filter_options._cache[cache_key] = result
        api_billback_ubi_filter_options._cache_time[cache_key] = datetime.now()

        return result

    except Exception as e:
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/ubi/unassigned")
def api_billback_ubi_unassigned(
    user: str = Depends(require_user),
    page: int = 1,
    page_size: int = 50,
    days_back: int = 60,
    sort: str = "amount_desc",
    property_filter: str = "",
    vendor_filter: str = "",
    gl_filter: str = ""
):
    """Load line items from Stage 7 that haven't been assigned to UBI periods.

    Pagination, sorting, and filtering to handle large datasets:
    - page: Page number (1-indexed)
    - page_size: Number of bills per page (default 50)
    - days_back: Only look at files from the last N days (default 60)
    - sort: Sort order (amount_desc, amount_asc, modified_desc, modified_asc, vendor_asc, property_asc)
    - property_filter: Filter by property name (partial match)
    - vendor_filter: Filter by vendor name (partial match)
    - gl_filter: Filter by GL code (exact match)

    IMPORTANT: Only shows bills for accounts marked as is_ubi=true in accounts_to_track.
    """
    try:
        import hashlib
        from datetime import datetime, timedelta
        from concurrent.futures import ThreadPoolExecutor, as_completed

        start_time = datetime.now()

        # Load UBI accounts - only show bills for accounts marked is_ubi=true AND is_tracked=true
        accounts_to_track = _get_accounts_to_track()
        ubi_account_keys = set()
        for acct in accounts_to_track:
            if acct.get("is_ubi") == True and acct.get("is_tracked", True):
                prop_id = str(acct.get("propertyId", "")).strip()
                vendor_id = str(acct.get("vendorId", "")).strip()
                acct_num = str(acct.get("accountNumber", "")).strip()
                if prop_id and acct_num:
                    # Build key with and without vendor for flexible matching
                    ubi_account_keys.add(f"{prop_id}|{vendor_id}|{acct_num}")
                    ubi_account_keys.add(f"{prop_id}||{acct_num}")  # Fallback without vendor
        print(f"[UBI UNASSIGNED] Found {len(ubi_account_keys)} UBI account keys")

        # Use cached exclusion hashes (refreshed every 5 minutes)
        excluded_hashes = _get_cached_exclusion_hashes(days_back)
        print(f"[UBI UNASSIGNED] Using {len(excluded_hashes)} cached exclusion hashes")

        # Get last UBI periods from Stage 8 (scans actual assigned bills)
        last_ubi_periods = _get_last_ubi_periods_from_stage8()
        print(f"[UBI UNASSIGNED] Got {len(last_ubi_periods)} accounts with UBI history from Stage 8")

        # Build date-partitioned prefixes for the last N days to avoid scanning everything
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(days_back):
            d = today - timedelta(days=i)
            prefix = f"{POST_ENTRATA_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        print(f"[UBI UNASSIGNED] Scanning {len(prefixes_to_scan)} date partitions (last {days_back} days)")

        # Collect all S3 keys first
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                s3_pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)
                for s3_page in s3_pages:
                    for obj in s3_page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception:
                continue

        print(f"[UBI UNASSIGNED] Found {len(all_keys)} JSONL files to process")

        # Helper function to safely parse charge values
        def safe_parse_charge(charge_val):
            """Safely parse a charge value, handling dates and invalid data."""
            if charge_val is None:
                return 0.0
            charge_str = str(charge_val).replace("$", "").replace(",", "").strip()
            if not charge_str:
                return 0.0
            # Skip if it looks like a date (contains /)
            if "/" in charge_str or "-" in charge_str:
                # Try to detect date-like patterns
                import re
                if re.match(r'^\d{1,2}[/-]\d{1,2}[/-]\d{2,4}$', charge_str):
                    return 0.0
            try:
                return float(charge_str)
            except (ValueError, TypeError):
                return 0.0

        # Process a single S3 file
        def process_file(key):
            try:
                obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]

                if not lines:
                    return None

                try:
                    first_rec = json.loads(lines[0])
                except json.JSONDecodeError as e:
                    print(f"[UBI UNASSIGNED] Skipping corrupted file {key}: {e}")
                    return None

                # Compute pdf_id from s3_key and extract review date from path
                computed_pdf_id = pdf_id_from_key(key)
                import re
                date_match = re.search(r'yyyy=(\d{4})/mm=(\d{2})/dd=(\d{2})', key)
                review_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}" if date_match else ""

                # Use PostedAt from record if available, then SubmittedAt, finally S3 LastModified
                # Important: Use SubmittedAt before S3 LastModified since GL mapping refresh can change LastModified
                posted_at_str = first_rec.get("PostedAt", "") or first_rec.get("SubmittedAt", "")
                posted_at_ts = 0
                submitter = first_rec.get("Submitter", "") or first_rec.get("SubmittedBy", "")
                if posted_at_str:
                    try:
                        from datetime import datetime
                        posted_at_dt = datetime.fromisoformat(posted_at_str.replace('Z', '+00:00'))
                        posted_at_ts = posted_at_dt.timestamp()
                    except Exception:
                        pass
                if not posted_at_str:
                    # Fall back to S3 LastModified only if no record timestamps exist
                    s3_last_mod = obj_data.get('LastModified')
                    if s3_last_mod:
                        posted_at_str = s3_last_mod.strftime("%Y-%m-%dT%H:%M:%S")
                        posted_at_ts = s3_last_mod.timestamp()

                # Build account key for history lookup
                property_id = first_rec.get("EnrichedPropertyID", "")
                vendor_id = first_rec.get("EnrichedVendorID", "")
                account_number = str(first_rec.get("Account Number", "")).strip()
                account_key = f"{property_id}|{vendor_id}|{account_number}"
                account_key_no_vendor = f"{property_id}||{account_number}"

                # Check if this is a UBI account (for suggestion purposes only - still show ALL bills)
                is_ubi_account = account_key in ubi_account_keys or account_key_no_vendor in ubi_account_keys

                # Look up suggestion from Stage 8 history (based on service periods)
                # ONLY suggest for UBI accounts - but still show ALL bills
                history = last_ubi_periods.get(account_key) if is_ubi_account else None
                suggested_period = None
                last_ubi_period = None
                last_service_month_str = None

                last_service_dates = None
                if history and is_ubi_account:
                    last_service_month = history.get("last_service_month")  # (year, month) tuple
                    last_ubi_period = history.get("last_ubi_period")
                    last_service_start = history.get("last_service_start", "")
                    last_service_end = history.get("last_service_end", "")

                    if last_service_month:
                        last_service_month_str = f"{last_service_month[1]:02d}/{last_service_month[0]}"
                        # Build full date range string
                        if last_service_start and last_service_end:
                            last_service_dates = f"{last_service_start} - {last_service_end}"
                        elif last_service_start:
                            last_service_dates = last_service_start

                        # Get this bill's service period
                        bill_service_start = first_rec.get("Bill Period Start", "")
                        bill_service_month = _parse_service_period_to_month(bill_service_start)

                        if bill_service_month and last_ubi_period:
                            # Check if this bill's service month is AFTER the last assigned
                            last_year, last_month = last_service_month
                            bill_year, bill_month = bill_service_month

                            # Calculate expected next service month
                            if last_month == 12:
                                expected_year, expected_month = last_year + 1, 1
                            else:
                                expected_year, expected_month = last_year, last_month + 1

                            # If bill is the NEXT sequential month, suggest next UBI period
                            if bill_year == expected_year and bill_month == expected_month:
                                suggested_period = _get_next_ubi_period(last_ubi_period)
                            # If bill is AFTER the last assigned (not exact next), still suggest next period
                            # This handles cases where bills come in out of order
                            elif (bill_year, bill_month) > (last_year, last_month):
                                suggested_period = _get_next_ubi_period(last_ubi_period)
                        elif last_ubi_period:
                            # No service dates but have history - suggest next period
                            suggested_period = _get_next_ubi_period(last_ubi_period)

                # --- Duplicate detection & prior period suggestion ---
                duplicate_warning = None     # "Possible duplicate of <period>"
                prior_period_suggestion = None  # "Suggest <period> (prior to <period>)"
                bill_svc_start_raw = first_rec.get("Bill Period Start", "")
                bill_svc_end_raw = first_rec.get("Bill Period End", "")

                if history and is_ubi_account and bill_svc_start_raw:
                    all_assignments = history.get("all_assignments", [])
                    bill_start_dt = _parse_date_any(bill_svc_start_raw)
                    bill_end_dt = _parse_date_any(bill_svc_end_raw)

                    if bill_start_dt and all_assignments:
                        # 1) DUPLICATE CHECK: same service dates (within 5 days) already assigned
                        for asgn in all_assignments:
                            asgn_start = _parse_date_any(asgn.get("service_start", ""))
                            asgn_end = _parse_date_any(asgn.get("service_end", ""))
                            if asgn_start:
                                start_diff = abs((bill_start_dt - asgn_start).days)
                                # Only confirm match if both end dates available; skip if either missing
                                if bill_end_dt and asgn_end:
                                    end_diff = abs((bill_end_dt - asgn_end).days)
                                else:
                                    end_diff = 999
                                if start_diff <= 5 and end_diff <= 5:
                                    duplicate_warning = asgn.get("ubi_period", "")
                                    break

                        # 2) PRIOR PERIOD CHECK: this bill's service month is the month
                        #    BEFORE an assigned bill  suggest the prior UBI period
                        if not suggested_period and not duplicate_warning:
                            bill_svc_month = _parse_service_period_to_month(bill_svc_start_raw)
                            if bill_svc_month:
                                bill_y, bill_m = bill_svc_month
                                for asgn in all_assignments:
                                    asgn_month = asgn.get("service_month")  # (year, month) tuple
                                    if not asgn_month:
                                        continue
                                    asgn_y, asgn_m = asgn_month
                                    # Calculate what month is one BEFORE the assigned service month
                                    if asgn_m == 1:
                                        prev_y, prev_m = asgn_y - 1, 12
                                    else:
                                        prev_y, prev_m = asgn_y, asgn_m - 1
                                    if bill_y == prev_y and bill_m == prev_m:
                                        prior_ubi = _get_prev_ubi_period(asgn.get("ubi_period", ""))
                                        if prior_ubi:
                                            prior_period_suggestion = prior_ubi
                                            # Also set as the main suggested_period for Accept button
                                            suggested_period = prior_ubi
                                            break

                bill_info = {
                    "s3_key": key,
                    "vendor": first_rec.get("EnrichedVendorName", "") or first_rec.get("Vendor Name", ""),
                    "account": first_rec.get("Account Number", ""),
                    "account_key": account_key,
                    "property_name": first_rec.get("EnrichedPropertyName", ""),
                    "pdf_id": computed_pdf_id,  # Computed from s3_key
                    "review_date": review_date,  # For /review URL
                    "invoice_no": first_rec.get("Invoice Number", ""),
                    "total_amount": 0.0,
                    "line_count": 0,
                    "unassigned_lines": [],
                    "last_modified": posted_at_str,
                    "last_modified_ts": posted_at_ts,
                    "submitter": submitter,
                    "suggested_period": suggested_period,
                    "last_assigned_period": last_ubi_period,
                    "last_assigned_service": last_service_dates or last_service_month_str,
                    "is_ubi_account": is_ubi_account,
                    "duplicate_warning": duplicate_warning,
                    "prior_period_suggestion": prior_period_suggestion,
                }

                for line in lines:
                    try:
                        rec = json.loads(line)
                        line_hash = _compute_stable_line_hash(rec)

                        # Skip if already assigned (line_hash from DDB)
                        if line_hash in excluded_hashes:
                            continue

                        charge = safe_parse_charge(rec.get("Line Item Charge", "0"))

                        # Only include essential fields to reduce payload size
                        # This drastically improves load time for large datasets
                        ESSENTIAL_FIELDS = {
                            'EnrichedPropertyName', 'EnrichedPropertyID', 'Property Name',
                            'EnrichedVendorName', 'EnrichedVendorID', 'Vendor Name',
                            'Account Number', 'Bill Period Start', 'Bill Period End',
                            'EnrichedGLAccountNumber', 'EnrichedGLAccountName', 'GL Account Number',
                            'Line Item Description', 'Line Item Charge', 'Charge Code',
                            'source_input_key', 'PDF_LINK', 'Invoice Number',
                            'Service Address', 'Meter Number', 'Consumption Amount', 'Unit of Measure',
                            'Current Amount', 'Amount Overridden', 'Charge Code Source',
                        }
                        sanitized_rec = {}
                        for k, v in rec.items():
                            if k not in ESSENTIAL_FIELDS:
                                continue  # Skip non-essential fields
                            if isinstance(v, str):
                                # Remove any null bytes or other problematic characters
                                sanitized_rec[k] = v.replace('\x00', '').replace('\ufffd', '')
                            else:
                                sanitized_rec[k] = v

                        bill_info["unassigned_lines"].append({
                            "line_hash": line_hash,
                            "line_data": sanitized_rec,
                            "charge": charge
                        })
                        bill_info["total_amount"] += charge
                        bill_info["line_count"] += 1
                    except (json.JSONDecodeError, ValueError, TypeError):
                        continue

                if bill_info["unassigned_lines"]:
                    return bill_info
                return None
            except Exception as e:
                print(f"[UBI UNASSIGNED] Error processing {key}: {e}")
                return None

        # Process files concurrently with ThreadPoolExecutor
        unassigned_bills = []
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    unassigned_bills.append(result)

        # Apply server-side filtering
        prop_filter_lower = property_filter.lower().strip() if property_filter else ""
        vendor_filter_lower = vendor_filter.lower().strip() if vendor_filter else ""
        gl_filter_clean = gl_filter.strip() if gl_filter else ""

        if prop_filter_lower or vendor_filter_lower or gl_filter_clean:
            filtered_bills = []
            for bill in unassigned_bills:
                # Get first line data for filtering
                first_line = bill.get("unassigned_lines", [{}])[0] if bill.get("unassigned_lines") else {}
                ld = first_line.get("line_data", {})

                # Property filter
                if prop_filter_lower:
                    prop_name = (ld.get("EnrichedPropertyName") or ld.get("Property Name") or "").lower()
                    if prop_filter_lower not in prop_name:
                        continue

                # Vendor filter
                if vendor_filter_lower:
                    vendor_name = (ld.get("EnrichedVendorName") or ld.get("Vendor Name") or bill.get("vendor", "")).lower()
                    if vendor_filter_lower not in vendor_name:
                        continue

                # GL filter (check if ANY line has this GL code)
                if gl_filter_clean:
                    has_gl = any(
                        (line.get("line_data", {}).get("EnrichedGLAccountNumber") or
                         line.get("line_data", {}).get("GL Account Number") or "") == gl_filter_clean
                        for line in bill.get("unassigned_lines", [])
                    )
                    if not has_gl:
                        continue

                filtered_bills.append(bill)
            unassigned_bills = filtered_bills

        # Apply server-side sorting
        def get_sort_key(bill):
            first_line = bill.get("unassigned_lines", [{}])[0] if bill.get("unassigned_lines") else {}
            ld = first_line.get("line_data", {})
            if sort == "amount_asc":
                return bill.get("total_amount", 0)
            elif sort == "amount_desc":
                return -bill.get("total_amount", 0)
            elif sort == "modified_asc":
                return bill.get("last_modified_ts", 0)
            elif sort == "modified_desc":
                return -bill.get("last_modified_ts", 0)
            elif sort == "vendor_asc":
                return (ld.get("EnrichedVendorName") or ld.get("Vendor Name") or bill.get("vendor", "")).lower()
            elif sort == "property_asc":
                return (ld.get("EnrichedPropertyName") or ld.get("Property Name") or "").lower()
            else:
                return -bill.get("total_amount", 0)  # Default to amount_desc

        unassigned_bills.sort(key=get_sort_key)

        # Calculate pagination
        total_bills = len(unassigned_bills)
        total_pages = (total_bills + page_size - 1) // page_size if total_bills > 0 else 1
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size

        # Get the requested page
        paginated_bills = unassigned_bills[start_idx:end_idx]

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"[UBI UNASSIGNED] Returning page {page}/{total_pages} ({len(paginated_bills)} of {total_bills} bills) in {elapsed:.1f}s")

        return {
            "bills": paginated_bills,
            "total_bills": total_bills,
            "page": page,
            "page_size": page_size,
            "total_pages": total_pages,
            "has_more": page < total_pages,
            "processing_time_seconds": round(elapsed, 1)
        }

    except Exception as e:
        print(f"[UBI UNASSIGNED] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/assign")
async def api_billback_ubi_assign(request: Request, user: str = Depends(require_user)):
    """Assign line items to UBI period(s) - moves items from Stage 7 to Stage 8 (UBI Assigned).

    Supports multi-period assignment: pass ubi_periods as comma-separated list (e.g., "08/2025,09/2025").
    Falls back to ubi_period for single-period backward compatibility.
    """
    try:
        import uuid
        from datetime import datetime

        form = await request.form()
        # Support both single period (ubi_period) and multiple periods (ubi_periods)
        ubi_periods_str = form.get("ubi_periods", "").strip()
        ubi_period_single = form.get("ubi_period", "").strip()
        line_hashes_str = form.get("line_hashes", "")
        s3_key = form.get("s3_key", "").strip()
        amounts_str = form.get("amounts", "")
        notes_str = form.get("notes", "")
        months_total = form.get("months_total", "1")

        # Parse periods - prefer ubi_periods if provided, otherwise use ubi_period
        if ubi_periods_str:
            ubi_periods = [p.strip() for p in ubi_periods_str.split(",") if p.strip()]
        elif ubi_period_single:
            ubi_periods = [ubi_period_single]
        else:
            ubi_periods = []

        if not ubi_periods:
            return JSONResponse({"error": "UBI period is required"}, status_code=400)

        if not line_hashes_str:
            return JSONResponse({"error": "No line items selected"}, status_code=400)

        if not s3_key:
            return JSONResponse({"error": "S3 key is required"}, status_code=400)

        line_hashes_to_assign = set(h.strip() for h in line_hashes_str.split(",") if h.strip())

        if not line_hashes_to_assign:
            return JSONResponse({"error": "No line items selected"}, status_code=400)

        # Parse amounts (comma-separated) - map by index in original order
        amounts_list = []
        if amounts_str:
            amounts_list = [float(a.strip()) if a.strip() else 0.0 for a in amounts_str.split(",")]
        line_hashes_ordered = [h.strip() for h in line_hashes_str.split(",") if h.strip()]
        hash_to_amount = {h: amounts_list[i] if i < len(amounts_list) else 0.0 for i, h in enumerate(line_hashes_ordered)}

        # Parse notes (||| separated)
        notes_list = notes_str.split("|||") if notes_str else []
        hash_to_notes = {h: notes_list[i] if i < len(notes_list) else "" for i, h in enumerate(line_hashes_ordered)}

        now_utc = datetime.utcnow().isoformat() + "Z"

        print(f"[UBI ASSIGN] Assigning {len(line_hashes_to_assign)} line(s) to {len(ubi_periods)} period(s): {', '.join(ubi_periods)}")
        print(f"[UBI ASSIGN] S3 Key: {s3_key}")
        print(f"[UBI ASSIGN] User: {user}")

        # Read the JSONL file from S3
        body = None
        actual_s3_key = s3_key
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as read_err:
            print(f"[UBI ASSIGN] Original S3 key not found: {read_err}")
            # Fallback: The file may have been rewritten with a new timestamp by another user
            # Try to find the bill in Stage 7 by searching the same date partition
            try:
                y, m, d = _extract_ymd_from_key(s3_key)
                prefix = f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"
                print(f"[UBI ASSIGN] Searching {prefix} for matching line hashes...")

                # List all JSONL files in that date partition
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        candidate_key = obj['Key']
                        if not candidate_key.endswith('.jsonl'):
                            continue
                        try:
                            candidate_body = _read_s3_text(BUCKET, candidate_key)
                            # Check if any of our line hashes exist in this file
                            for line in candidate_body.splitlines():
                                if not line.strip():
                                    continue
                                rec = json.loads(line.strip())
                                line_hash = _compute_stable_line_hash(rec)
                                if line_hash in line_hashes_to_assign:
                                    # Found the bill in a different file!
                                    print(f"[UBI ASSIGN] Found bill in alternate location: {candidate_key}")
                                    body = candidate_body
                                    actual_s3_key = candidate_key
                                    break
                            if body:
                                break
                        except Exception:
                            continue
                    if body:
                        break
            except Exception as search_err:
                print(f"[UBI ASSIGN] Fallback search failed: {search_err}")

        if not body:
            # Bill not found in Stage 7 - it was likely already assigned or archived
            print(f"[UBI ASSIGN] Bill not found after fallback search. Original key: {s3_key}")
            return JSONResponse({
                "error": "Bill not found - it may have been assigned or modified by another user. Please refresh the page to get the latest data."
            }, status_code=404)

        # Use the actual key we found (may differ from original if fallback was used)
        s3_key = actual_s3_key

        # Parse all lines and split into assigned vs remaining
        assigned_items = []
        remaining_items = []
        assigned_count = 0

        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                line_hash = _compute_stable_line_hash(rec)

                if line_hash in line_hashes_to_assign:
                    # Initialize ubi_assignments array if not present
                    if "ubi_assignments" not in rec:
                        rec["ubi_assignments"] = []

                    # Create assignment entries for ALL periods at once
                    line_amount = hash_to_amount.get(line_hash, 0.0)
                    line_notes = hash_to_notes.get(line_hash, "")

                    for period in ubi_periods:
                        new_assignment = {
                            "period": period,
                            "amount": line_amount,
                            "months": int(months_total) if months_total else 1,
                            "assigned_by": user,
                            "assigned_date": now_utc,
                        }
                        if line_notes:
                            new_assignment["notes"] = line_notes
                        rec["ubi_assignments"].append(new_assignment)

                    # Sort assignments by period
                    rec["ubi_assignments"].sort(key=lambda x: x.get("period", ""))

                    # Update legacy single-period fields (use first period for backward compat)
                    first = rec["ubi_assignments"][0]
                    rec["ubi_period"] = first["period"]
                    rec["ubi_amount"] = first["amount"]
                    rec["ubi_months_total"] = first["months"]
                    rec["ubi_assigned_by"] = first["assigned_by"]
                    rec["ubi_assigned_date"] = first["assigned_date"]
                    rec["ubi_period_count"] = len(rec["ubi_assignments"])

                    assigned_items.append(rec)
                    assigned_count += 1
                    print(f"[UBI ASSIGN] Assigned line {line_hash[:16]}... amount={line_amount} periods={', '.join(ubi_periods)}")
                else:
                    remaining_items.append(rec)
            except (json.JSONDecodeError, ValueError, TypeError) as e:
                print(f"[UBI ASSIGN] Error parsing line: {e}")
                remaining_items.append(json.loads(line) if line else {})

        if not assigned_items:
            return JSONResponse({"error": "No matching line items found in the file"}, status_code=400)

        # Extract date parts from source key for writing
        y, m, d = _extract_ymd_from_key(s3_key)
        base = _basename_from_key(s3_key)

        # Write assigned items to Stage 8 (UBI_ASSIGNED_PREFIX)
        assigned_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), assigned_items)
        print(f"[UBI ASSIGN] Wrote {len(assigned_items)} assigned items to {assigned_key}")

        # Also write to Stage 99 (HIST_ARCHIVE_PREFIX) for historical record
        archive_key = _write_jsonl(HIST_ARCHIVE_PREFIX, y, m, d, base.replace('.jsonl', ''), assigned_items)
        print(f"[UBI ASSIGN] Also archived {len(assigned_items)} items to {archive_key}")

        # Write line hashes to DDB exclusion tables for fast duplicate detection
        try:
            for rec in assigned_items:
                line_hash = _compute_stable_line_hash(rec)
                ubi_period = rec.get("ubi_period", "")
                # Use line_hash directly as assignment_id for easy deletion
                # (line_hash is unique per line item)
                record_id = line_hash

                # Write to assignments table (uses assignment_id = line_hash for direct deletion)
                assignment_item = {
                    'assignment_id': {'S': record_id},
                    'line_hash': {'S': line_hash},
                    's3_key': {'S': assigned_key},
                    'ubi_period': {'S': ubi_period},
                    'assigned_by': {'S': user},
                    'assigned_date': {'S': now_utc},
                }
                ddb.put_item(TableName='jrk-bill-ubi-assignments', Item=assignment_item)

                # Write to archived table (uses archive_id)
                archive_item = {
                    'archive_id': {'S': record_id},
                    'line_hash': {'S': line_hash},
                    's3_key': {'S': archive_key},
                    'ubi_period': {'S': ubi_period},
                    'assigned_by': {'S': user},
                    'assigned_date': {'S': now_utc},
                }
                ddb.put_item(TableName='jrk-bill-ubi-archived', Item=archive_item)
            print(f"[UBI ASSIGN] Wrote {len(assigned_items)} hashes to DDB exclusion tables")
        except Exception as ddb_err:
            print(f"[UBI ASSIGN] Warning: Could not write to DDB exclusion tables: {ddb_err}")

        # Update source file: rewrite with remaining items or delete if empty
        if remaining_items:
            new_s7_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_items)
            # Delete original file since _write_jsonl creates a new file with a new timestamp
            if new_s7_key != s3_key:
                s3.delete_object(Bucket=BUCKET, Key=s3_key)
                print(f"[UBI ASSIGN] Deleted original {s3_key}, remaining items in {new_s7_key}")
                # Verify deletion
                try:
                    s3.head_object(Bucket=BUCKET, Key=s3_key)
                    print(f"[UBI ASSIGN] WARNING: Original file STILL EXISTS after delete: {s3_key}")
                    s3.delete_object(Bucket=BUCKET, Key=s3_key)
                    print(f"[UBI ASSIGN] Retried delete for {s3_key}")
                except Exception:
                    pass  # Good - file is gone (404)
            else:
                print(f"[UBI ASSIGN] Rewrote {len(remaining_items)} remaining items to source")
        else:
            s3.delete_object(Bucket=BUCKET, Key=s3_key)
            print(f"[UBI ASSIGN] Deleted empty source file {s3_key}")
            # Verify deletion
            try:
                s3.head_object(Bucket=BUCKET, Key=s3_key)
                print(f"[UBI ASSIGN] WARNING: Source file STILL EXISTS after delete: {s3_key}")
                s3.delete_object(Bucket=BUCKET, Key=s3_key)
            except Exception:
                pass  # Good - file is gone (404)

        print(f"[UBI ASSIGN] COMPLETED: Moved {assigned_count} items to Stage 8 for {len(ubi_periods)} period(s)")

        # Invalidate exclusion cache so new assignments are reflected immediately
        invalidate_exclusion_cache()

        # Update UBI account history for smarter future suggestions
        if assigned_items:
            try:
                first_rec = assigned_items[0]
                property_id = first_rec.get("EnrichedPropertyID", "")
                vendor_id = first_rec.get("EnrichedVendorID", "")
                account_number = str(first_rec.get("Account Number", "")).strip()
                account_key = f"{property_id}|{vendor_id}|{account_number}"

                bill_date = first_rec.get("Bill Date", "")
                service_start = first_rec.get("Bill Period Start", "")
                service_end = first_rec.get("Bill Period End", "")

                # Calculate total amount
                total_amount = sum(float(str(r.get("ubi_amount", 0)).replace("$", "").replace(",", "")) for r in assigned_items)

                _update_ubi_account_history(account_key, bill_date, service_start, service_end, ubi_periods, total_amount)
                print(f"[UBI ASSIGN] Updated account history for {account_key}")
            except Exception as hist_err:
                print(f"[UBI ASSIGN] Warning: Could not update history: {hist_err}")

        # Invalidate the Stage 8 cache so next request gets fresh data
        global _last_ubi_periods_cache
        _last_ubi_periods_cache = {"data": {}, "expires": 0}
        print("[UBI ASSIGN] Invalidated Stage 8 cache")

        return {"ok": True, "assigned": assigned_count, "ubi_periods": ubi_periods, "period_count": len(ubi_periods)}

    except Exception as e:
        print(f"[UBI ASSIGN] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- Smart UBI Suggestion APIs --------

@app.get("/api/billback/ubi/suggestions")
def api_billback_ubi_suggestions(user: str = Depends(require_user), page: int = 1, page_size: int = 50, days_back: int = 90):
    """Get UBI allocation suggestions for unassigned bills with service period dates."""
    try:
        from datetime import datetime, timedelta
        from concurrent.futures import ThreadPoolExecutor, as_completed

        start_time = datetime.now()
        print(f"[UBI SUGGESTIONS] Loading suggestions for unassigned bills")

        # Load UBI accounts - only show bills for accounts marked is_ubi=true AND is_tracked=true
        accounts_to_track = _get_accounts_to_track()
        ubi_account_keys = set()
        for acct in accounts_to_track:
            if acct.get("is_ubi") == True and acct.get("is_tracked", True):
                prop_id = str(acct.get("propertyId", "")).strip()
                vendor_id = str(acct.get("vendorId", "")).strip()
                acct_num = str(acct.get("accountNumber", "")).strip()
                if prop_id and acct_num:
                    ubi_account_keys.add(f"{prop_id}|{vendor_id}|{acct_num}")
                    ubi_account_keys.add(f"{prop_id}||{acct_num}")
        print(f"[UBI SUGGESTIONS] Found {len(ubi_account_keys)} UBI account keys")

        # Load UBI account history for better suggestions
        history_data = _s3_get_ubi_account_history()
        account_histories = history_data.get("accounts", {})

        # Also get Stage 8 history for more accurate suggestions
        stage8_history = _get_last_ubi_periods_from_stage8()
        print(f"[UBI SUGGESTIONS] Got {len(stage8_history)} accounts from Stage 8 history")

        # Use cached exclusion hashes
        excluded_hashes = _get_cached_exclusion_hashes(days_back)

        # Build date-partitioned prefixes
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(days_back):
            d = today - timedelta(days=i)
            prefix = f"{POST_ENTRATA_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        # Collect all S3 keys
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                s3_pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)
                for s3_page in s3_pages:
                    for obj in s3_page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception:
                continue

        print(f"[UBI SUGGESTIONS] Found {len(all_keys)} files to check")

        def process_file_for_suggestions(key):
            """Process a single file and calculate suggestions."""
            try:
                obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]

                if not lines:
                    return None

                try:
                    first_rec = json.loads(lines[0])
                except json.JSONDecodeError:
                    return None

                # Skip if all lines are already assigned
                has_unassigned = False
                for line in lines:
                    try:
                        rec = json.loads(line)
                        _lh = _compute_stable_line_hash(rec)
                        if _lh not in excluded_hashes:
                            has_unassigned = True
                            break
                    except Exception:
                        continue

                if not has_unassigned:
                    return None

                # Parse bill info
                property_id = first_rec.get("EnrichedPropertyID", "")
                vendor_id = first_rec.get("EnrichedVendorID", "")
                account_number = str(first_rec.get("Account Number", "")).strip()
                account_key = f"{property_id}|{vendor_id}|{account_number}"
                account_key_no_vendor = f"{property_id}||{account_number}"

                # Check if this is a UBI account (for suggestion purposes)
                is_ubi_account = account_key in ubi_account_keys or account_key_no_vendor in ubi_account_keys

                # Get account history if exists (only for UBI accounts)
                acct_history = account_histories.get(account_key) if is_ubi_account else None

                # Parse service dates
                service_start_str = first_rec.get("Bill Period Start", "")
                service_end_str = first_rec.get("Bill Period End", "")

                service_start = _parse_date_any(service_start_str) if service_start_str else None
                service_end = _parse_date_any(service_end_str) if service_end_str else None

                # Calculate total amount
                total_amount = 0.0
                unassigned_count = 0
                for line in lines:
                    try:
                        rec = json.loads(line)
                        _lh = _compute_stable_line_hash(rec)
                        if _lh not in excluded_hashes:
                            charge_str = str(rec.get("Line Item Charge", "0")).replace("$", "").replace(",", "").strip()
                            try:
                                total_amount += float(charge_str)
                            except Exception:
                                pass
                            unassigned_count += 1
                    except Exception:
                        continue

                # Calculate suggestion - ONLY for UBI accounts
                suggestion = None
                if is_ubi_account:
                    # Prefer Stage 8 history over ubi_account_history.json
                    stage8_data = stage8_history.get(account_key)
                    if stage8_data and stage8_data.get("last_ubi_period"):
                        # Use Stage 8 history - suggest next period after last assigned
                        last_ubi = stage8_data["last_ubi_period"]
                        next_ubi = _get_next_ubi_period(last_ubi)
                        suggestion = {
                            "suggested_periods": [{
                                "period": next_ubi,
                                "days": 30,
                                "amount": round(total_amount, 2),
                                "pct": 100.0
                            }],
                            "confidence": "high",
                            "reason": f"Next sequential period after {last_ubi} (from Stage 8 history)",
                            "last_period": last_ubi,
                            "spans_months": False
                        }
                    else:
                        # Fall back to ubi_account_history.json calculation
                        suggestion = _calculate_ubi_suggestion(
                            service_start,
                            service_end,
                            total_amount,
                            acct_history
                        )

                # Get posted date
                posted_at = first_rec.get("PostedAt", "")
                if not posted_at:
                    s3_last_mod = obj_data.get('LastModified')
                    if s3_last_mod:
                        posted_at = s3_last_mod.strftime("%Y-%m-%dT%H:%M:%S")

                return {
                    "s3_key": key,
                    "pdf_id": first_rec.get("pdf_id", ""),
                    "vendor": first_rec.get("Vendor Name", ""),
                    "vendor_id": vendor_id,
                    "property": first_rec.get("EnrichedPropertyName", ""),
                    "property_id": property_id,
                    "account": account_number,
                    "account_key": account_key,
                    "invoice_no": first_rec.get("Invoice Number", ""),
                    "bill_date": first_rec.get("Bill Date", ""),
                    "service_start": service_start_str,
                    "service_end": service_end_str,
                    "total_amount": round(total_amount, 2),
                    "unassigned_lines": unassigned_count,
                    "posted_at": posted_at,
                    "suggestion": suggestion,
                    "has_history": acct_history is not None,
                    "avg_service_days": acct_history.get("avgServiceDays") if acct_history else None,
                    "last_ubi_periods": acct_history.get("lastUbiPeriods", []) if acct_history else []
                }
            except Exception as e:
                print(f"[UBI SUGGESTIONS] Error processing {key}: {e}")
                return None

        # Process files concurrently
        bills_with_suggestions = []
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file_for_suggestions, key): key for key in all_keys}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    bills_with_suggestions.append(result)

        # Sort by confidence (high first), then by amount
        confidence_order = {"high": 0, "medium": 1, "low": 2}
        bills_with_suggestions.sort(
            key=lambda x: (confidence_order.get(x["suggestion"]["confidence"], 3), -x["total_amount"])
        )

        # Pagination
        total_bills = len(bills_with_suggestions)
        total_pages = (total_bills + page_size - 1) // page_size if total_bills > 0 else 1
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated = bills_with_suggestions[start_idx:end_idx]

        # Calculate summary stats
        high_confidence = sum(1 for b in bills_with_suggestions if b["suggestion"]["confidence"] == "high")
        medium_confidence = sum(1 for b in bills_with_suggestions if b["suggestion"]["confidence"] == "medium")
        low_confidence = sum(1 for b in bills_with_suggestions if b["suggestion"]["confidence"] == "low")

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"[UBI SUGGESTIONS] Returning {len(paginated)} of {total_bills} bills ({high_confidence} high, {medium_confidence} medium, {low_confidence} low confidence)")

        return {
            "bills": paginated,
            "total_bills": total_bills,
            "page": page,
            "page_size": page_size,
            "total_pages": total_pages,
            "summary": {
                "high_confidence": high_confidence,
                "medium_confidence": medium_confidence,
                "low_confidence": low_confidence
            },
            "processing_time_seconds": round(elapsed, 1)
        }

    except Exception as e:
        print(f"[UBI SUGGESTIONS] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/accept-suggestion")
async def api_billback_ubi_accept_suggestion(request: Request, user: str = Depends(require_user)):
    """Accept a UBI period suggestion and assign the bill."""
    try:
        form = await request.form()
        s3_key = form.get("s3_key", "")
        accept = form.get("accept", "true").lower() == "true"

        if not s3_key:
            return JSONResponse({"error": "s3_key required"}, status_code=400)

        if not accept:
            # User rejected suggestion - just return ok, they'll assign manually
            return {"ok": True, "action": "rejected"}

        # Read the bill file
        obj_data = s3.get_object(Bucket=BUCKET, Key=s3_key)
        txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
        lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]

        if not lines:
            return JSONResponse({"error": "Empty file"}, status_code=400)

        first_rec = json.loads(lines[0])

        # Get service dates and calculate suggestion
        service_start_str = first_rec.get("Bill Period Start", "")
        service_end_str = first_rec.get("Bill Period End", "")
        service_start = _parse_date_any(service_start_str) if service_start_str else None
        service_end = _parse_date_any(service_end_str) if service_end_str else None

        # Calculate total amount
        excluded_hashes = _get_cached_exclusion_hashes(90)
        total_amount = 0.0
        line_hashes = []
        for line in lines:
            try:
                rec = json.loads(line)
                line_hash = _compute_stable_line_hash(rec)
                if line_hash not in excluded_hashes:
                    charge_str = str(rec.get("Line Item Charge", "0")).replace("$", "").replace(",", "").strip()
                    try:
                        total_amount += float(charge_str)
                    except Exception:
                        pass
                    line_hashes.append(line_hash)
            except Exception:
                continue

        if not line_hashes:
            return JSONResponse({"error": "No unassigned lines found"}, status_code=400)

        # Get suggestion
        property_id = first_rec.get("EnrichedPropertyID", "")
        vendor_id = first_rec.get("EnrichedVendorID", "")
        account_number = str(first_rec.get("Account Number", "")).strip()
        account_key = f"{property_id}|{vendor_id}|{account_number}"

        history_data = _s3_get_ubi_account_history()
        acct_history = history_data.get("accounts", {}).get(account_key)

        suggestion = _calculate_ubi_suggestion(service_start, service_end, total_amount, acct_history)

        if not suggestion["suggested_periods"]:
            return JSONResponse({"error": "No suggested periods - assign manually"}, status_code=400)

        # Build form data for the assign endpoint
        ubi_periods = [p["period"] for p in suggestion["suggested_periods"]]
        ubi_periods_str = ",".join(ubi_periods)

        # Call the assign logic directly (simplified version)
        # We'll construct a mock form and call the same logic
        now_utc = dt.datetime.utcnow().isoformat() + "Z"

        # Parse the key to get date parts
        key_parts = s3_key.split("/")
        y = m = d = ""
        for part in key_parts:
            if part.startswith("yyyy="):
                y = part.split("=")[1]
            elif part.startswith("mm="):
                m = part.split("=")[1]
            elif part.startswith("dd="):
                d = part.split("=")[1]

        base = os.path.basename(s3_key)

        # Read and update records
        assigned_items = []
        remaining_items = []
        assigned_count = 0

        for line in lines:
            try:
                rec = json.loads(line)
                line_hash = _compute_stable_line_hash(rec)

                if line_hash in line_hashes:
                    # Calculate amount per line from suggestion
                    line_charge_str = str(rec.get("Line Item Charge", "0")).replace("$", "").replace(",", "").strip()
                    try:
                        line_amount = float(line_charge_str)
                    except Exception:
                        line_amount = 0.0

                    # Calculate proportional amounts for multi-period
                    if "ubi_assignments" not in rec:
                        rec["ubi_assignments"] = []

                    for period_info in suggestion["suggested_periods"]:
                        period = period_info["period"]
                        pct = period_info["pct"] / 100.0 if period_info["pct"] else (1.0 / len(suggestion["suggested_periods"]))
                        amount_for_period = round(line_amount * pct, 2)

                        new_assignment = {
                            "period": period,
                            "amount": amount_for_period,
                            "months": 1,
                            "assigned_by": user,
                            "assigned_date": now_utc,
                            "auto_suggested": True
                        }
                        rec["ubi_assignments"].append(new_assignment)

                    rec["ubi_assignments"].sort(key=lambda x: x.get("period", ""))
                    first = rec["ubi_assignments"][0]
                    rec["ubi_period"] = first["period"]
                    rec["ubi_amount"] = first["amount"]
                    rec["ubi_months_total"] = first["months"]
                    rec["ubi_assigned_by"] = first["assigned_by"]
                    rec["ubi_assigned_date"] = first["assigned_date"]
                    rec["ubi_period_count"] = len(rec["ubi_assignments"])
                    rec["ubi_auto_suggested"] = True

                    assigned_items.append(rec)
                    assigned_count += 1
                else:
                    remaining_items.append(rec)
            except Exception:
                continue

        if not assigned_items:
            return JSONResponse({"error": "No items to assign"}, status_code=400)

        # Write to Stage 8
        assigned_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), assigned_items)
        print(f"[UBI ACCEPT] Wrote {len(assigned_items)} items to {assigned_key}")

        # Archive copy
        archive_key = _write_jsonl(HIST_ARCHIVE_PREFIX, y, m, d, base.replace('.jsonl', ''), assigned_items)

        # Write exclusion hashes to DDB (same as main assign endpoint)
        try:
            now_utc = datetime.utcnow().isoformat() + "Z"
            for rec in assigned_items:
                lh = _compute_stable_line_hash(rec)
                ddb.put_item(TableName='jrk-bill-ubi-assignments', Item={
                    'assignment_id': {'S': lh},
                    'line_hash': {'S': lh},
                    's3_key': {'S': assigned_key},
                    'ubi_period': {'S': rec.get("ubi_period", "")},
                    'assigned_by': {'S': user},
                    'assigned_date': {'S': now_utc},
                })
            print(f"[UBI ACCEPT] Wrote {len(assigned_items)} hashes to DDB exclusion table")
        except Exception as ddb_err:
            print(f"[UBI ACCEPT] Warning: Could not write to DDB exclusion tables: {ddb_err}")

        # Update source file - MUST delete original when writing new file
        if remaining_items:
            new_s7_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_items)
            if new_s7_key != s3_key:
                s3.delete_object(Bucket=BUCKET, Key=s3_key)
                print(f"[UBI ACCEPT] Deleted original {s3_key}, remaining items in {new_s7_key}")
        else:
            s3.delete_object(Bucket=BUCKET, Key=s3_key)
            print(f"[UBI ACCEPT] Deleted empty source file {s3_key}")

        # Update account history
        _update_ubi_account_history(
            account_key,
            first_rec.get("Bill Date", ""),
            service_start_str,
            service_end_str,
            ubi_periods,
            total_amount
        )

        # Invalidate cache
        invalidate_exclusion_cache()

        return {
            "ok": True,
            "assigned": assigned_count,
            "ubi_periods": ubi_periods,
            "suggestion": suggestion
        }

    except Exception as e:
        print(f"[UBI ACCEPT] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/ubi/account-history/{account_key:path}")
def api_billback_ubi_account_history(account_key: str, user: str = Depends(require_user)):
    """Get billing history for a specific account."""
    try:
        history_data = _s3_get_ubi_account_history()
        accounts = history_data.get("accounts", {})

        # URL decode the account key
        import urllib.parse
        decoded_key = urllib.parse.unquote(account_key)

        acct = accounts.get(decoded_key, None)

        if not acct:
            return {
                "found": False,
                "account_key": decoded_key,
                "history": [],
                "avg_service_days": None,
                "last_bill_date": None,
                "last_ubi_periods": []
            }

        return {
            "found": True,
            "account_key": decoded_key,
            "history": acct.get("billHistory", []),
            "avg_service_days": acct.get("avgServiceDays"),
            "last_bill_date": acct.get("lastBillDate"),
            "last_service_end": acct.get("lastServiceEnd"),
            "last_ubi_periods": acct.get("lastUbiPeriods", [])
        }

    except Exception as e:
        print(f"[UBI HISTORY] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/calculate-suggestion")
async def api_billback_ubi_calculate_suggestion(request: Request, user: str = Depends(require_user)):
    """Calculate UBI period suggestion for a specific bill."""
    try:
        form = await request.form()
        service_start_str = form.get("service_start", "")
        service_end_str = form.get("service_end", "")
        amount_str = form.get("amount", "0")
        account_key = form.get("account_key", "")

        # Parse amount
        try:
            amount = float(str(amount_str).replace("$", "").replace(",", "").strip())
        except Exception:
            amount = 0.0

        # Parse dates
        service_start = _parse_date_any(service_start_str) if service_start_str else None
        service_end = _parse_date_any(service_end_str) if service_end_str else None

        # Get account history if key provided
        acct_history = None
        if account_key:
            history_data = _s3_get_ubi_account_history()
            acct_history = history_data.get("accounts", {}).get(account_key)

        suggestion = _calculate_ubi_suggestion(service_start, service_end, amount, acct_history)

        return {
            "ok": True,
            "suggestion": suggestion,
            "account_history": {
                "found": acct_history is not None,
                "avg_service_days": acct_history.get("avgServiceDays") if acct_history else None,
                "last_ubi_periods": acct_history.get("lastUbiPeriods", []) if acct_history else []
            } if account_key else None
        }

    except Exception as e:
        print(f"[UBI CALC] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/ubi/assigned")
def api_billback_ubi_assigned(user: str = Depends(require_user), period: str = "", days_back: int = 90):
    """Load line items assigned to a specific UBI period from Stage 8 (S3)."""
    try:
        from datetime import datetime, timedelta
        from collections import defaultdict
        from concurrent.futures import ThreadPoolExecutor, as_completed

        print(f"[UBI ASSIGNED] Loading assigned items from Stage 8" + (f" for period {period}" if period else ""))

        # Build date-partitioned prefixes for the last N days
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(days_back):
            d = today - timedelta(days=i)
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        # Collect all S3 keys
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception as e:
                print(f"[UBI ASSIGNED] Error listing {prefix}: {e}")

        print(f"[UBI ASSIGNED] Found {len(all_keys)} files in Stage 8")

        # Group by period, then by S3 key
        by_period = defaultdict(lambda: {"total_amount": 0.0, "line_count": 0, "bills": {}})

        def process_file(key):
            """Process a single S3 file."""
            try:
                body = _read_s3_text(BUCKET, key)
                lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
                if not lines:
                    return []

                results = []
                first_rec = json.loads(lines[0])

                # Compute pdf_id from s3_key and extract review date from path
                computed_pdf_id = pdf_id_from_key(key)
                # Extract date from path like Bill_Parser_8_UBI_Assigned/yyyy=2025/mm=12/dd=01/...
                import re
                date_match = re.search(r'yyyy=(\d{4})/mm=(\d{2})/dd=(\d{2})', key)
                review_date = f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}" if date_match else ""

                for line in lines:
                    rec = json.loads(line)
                    line_hash = _compute_stable_line_hash(rec)

                    # Exclude large fields
                    EXCLUDE_FIELDS = {'__pdf_b64__', '__pdf_filename__', 'EnrichedProperty'}
                    filtered_rec = {k: v for k, v in rec.items() if k not in EXCLUDE_FIELDS}

                    # Handle multi-period format (ubi_assignments array)
                    ubi_assignments = rec.get("ubi_assignments", [])
                    if ubi_assignments:
                        # New format: iterate through all assignments
                        for asn in ubi_assignments:
                            asn_period = asn.get("period", "")
                            if period and asn_period != period:
                                continue
                            if not asn_period:
                                continue

                            charge = asn.get("amount", 0.0)
                            if charge == 0.0:
                                charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                                charge = float(charge_str) if charge_str else 0.0

                            results.append({
                                "s3_key": key,
                                "ubi_period": asn_period,
                                "line_hash": line_hash,
                                "assignment_id": f"{key}||{line_hash}",  # Composite ID for unassign
                                "line_data": filtered_rec,
                                "charge": charge,
                                "vendor": rec.get("Vendor Name", ""),
                                "account": rec.get("Account Number", ""),
                                "pdf_id": computed_pdf_id,  # Computed from s3_key
                                "review_date": review_date,  # For /review URL
                                "invoice_no": rec.get("Invoice Number", ""),
                                "ubi_period_count": len(ubi_assignments),
                                "assigned_date": asn.get("assigned_date", ""),
                                "assigned_by": asn.get("assigned_by", "")
                            })
                    else:
                        # Legacy format: single ubi_period field
                        line_ubi_period = rec.get("ubi_period", "")
                        if period and line_ubi_period != period:
                            continue
                        if not line_ubi_period:
                            continue

                        charge = rec.get("ubi_amount", 0.0)
                        if charge == 0.0:
                            charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                            charge = float(charge_str) if charge_str else 0.0

                        results.append({
                            "s3_key": key,
                            "ubi_period": line_ubi_period,
                            "line_hash": line_hash,
                            "assignment_id": f"{key}||{line_hash}",  # Composite ID for unassign
                            "line_data": filtered_rec,
                            "charge": charge,
                            "vendor": rec.get("Vendor Name", ""),
                            "account": rec.get("Account Number", ""),
                            "pdf_id": computed_pdf_id,  # Computed from s3_key
                            "review_date": review_date,  # For /review URL
                            "invoice_no": rec.get("Invoice Number", ""),
                            "ubi_period_count": 1,
                            "assigned_date": rec.get("ubi_assigned_date", ""),
                            "assigned_by": rec.get("ubi_assigned_by", "")
                        })
                return results
            except Exception as e:
                print(f"[UBI ASSIGNED] Error processing {key}: {e}")
                return []

        # Process files concurrently
        all_items = []
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                all_items.extend(future.result())

        # Group results by period and bill
        for item in all_items:
            ubi_period_key = item["ubi_period"]
            s3_key = item["s3_key"]

            if s3_key not in by_period[ubi_period_key]["bills"]:
                by_period[ubi_period_key]["bills"][s3_key] = {
                    "s3_key": s3_key,
                    "vendor": item["vendor"],
                    "account": item["account"],
                    "pdf_id": item["pdf_id"],
                    "invoice_no": item["invoice_no"],
                    "assigned_lines": [],
                    "assigned_date": item.get("assigned_date", ""),
                    "assigned_by": item.get("assigned_by", "")
                }
            else:
                # Track most recent assigned_date for the bill
                existing_date = by_period[ubi_period_key]["bills"][s3_key].get("assigned_date", "")
                new_date = item.get("assigned_date", "")
                if new_date and (not existing_date or new_date > existing_date):
                    by_period[ubi_period_key]["bills"][s3_key]["assigned_date"] = new_date
                    by_period[ubi_period_key]["bills"][s3_key]["assigned_by"] = item.get("assigned_by", "")

            by_period[ubi_period_key]["bills"][s3_key]["assigned_lines"].append({
                "line_hash": item["line_hash"],
                "assignment_id": item["assignment_id"],  # Composite ID for unassign/reassign
                "line_data": item["line_data"],
                "charge": item["charge"],
                "assigned_date": item.get("assigned_date", ""),
                "assigned_by": item.get("assigned_by", "")
            })
            by_period[ubi_period_key]["total_amount"] += item["charge"]
            by_period[ubi_period_key]["line_count"] += 1

        # Convert to list format
        result = []
        for ubi_period_key, data in by_period.items():
            bills_list = list(data["bills"].values())
            result.append({
                "ubi_period": ubi_period_key,
                "total_amount": round(data["total_amount"], 2),
                "line_count": data["line_count"],
                "bills": bills_list
            })

        result.sort(key=lambda x: x["ubi_period"], reverse=True)

        print(f"[UBI ASSIGNED] Returning {len(result)} periods with {sum(r['line_count'] for r in result)} total lines")
        return {"periods": result}

    except Exception as e:
        print(f"[UBI ASSIGNED] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/unassign")
async def api_billback_ubi_unassign(request: Request, user: str = Depends(require_user)):
    """Remove assignments - moves items from Stage 8 back to Stage 7."""
    try:
        from collections import defaultdict

        form = await request.form()
        assignment_ids_str = form.get("assignment_ids", "").strip()

        print(f"[UBI UNASSIGN] Request from {user}")
        print(f"[UBI UNASSIGN] Raw assignment_ids (first 500 chars): {assignment_ids_str[:500]}")

        if not assignment_ids_str:
            print("[UBI UNASSIGN] ERROR: Empty assignment_ids")
            return JSONResponse({"error": "Assignment IDs are required"}, status_code=400)

        # Parse assignment IDs (format: s3_key||line_hash) and group by s3_key
        by_s3_key = defaultdict(set)
        invalid_count = 0
        for aid in assignment_ids_str.split(","):
            aid = aid.strip()
            if not aid:
                continue
            if "||" in aid:
                s3_key, line_hash = aid.split("||", 1)
                by_s3_key[s3_key].add(line_hash)
            else:
                invalid_count += 1
                print(f"[UBI UNASSIGN] Invalid ID (no ||): {aid[:80]}...")

        if invalid_count > 0:
            print(f"[UBI UNASSIGN] WARNING: {invalid_count} invalid assignment IDs")

        if not by_s3_key:
            print("[UBI UNASSIGN] ERROR: No valid assignment IDs after parsing")
            return JSONResponse({"error": "No valid assignment IDs provided. Expected format: s3_key||line_hash"}, status_code=400)

        print(f"[UBI UNASSIGN] Processing {sum(len(v) for v in by_s3_key.values())} line(s) across {len(by_s3_key)} file(s)")

        total_unassigned = 0

        # Process each S3 file
        for s3_key, line_hashes_to_unassign in by_s3_key.items():
            print(f"[UBI UNASSIGN] Processing {s3_key} - {len(line_hashes_to_unassign)} line(s)")

            # Read the JSONL file from Stage 8
            try:
                body = _read_s3_text(BUCKET, s3_key)
            except Exception as read_err:
                print(f"[UBI UNASSIGN] Error reading S3 file {s3_key}: {read_err}")
                continue  # Try other files

            # Parse all lines and split into unassigned vs remaining
            unassigned_items = []
            remaining_items = []

            for line in body.splitlines():
                line = (line or '').strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    line_hash = _compute_stable_line_hash(rec)

                    if line_hash in line_hashes_to_unassign:
                        # Remove all UBI assignment fields (both legacy and multi-period)
                        for field in ["ubi_period", "ubi_assigned_date", "ubi_assigned_by", "ubi_amount",
                                      "ubi_months_total", "ubi_notes", "ubi_assignments", "ubi_period_count"]:
                            rec.pop(field, None)
                        unassigned_items.append(rec)
                        print(f"[UBI UNASSIGN] Unassigned line {line_hash[:16]}...")
                    else:
                        remaining_items.append(rec)
                except (json.JSONDecodeError, ValueError, TypeError) as e:
                    print(f"[UBI UNASSIGN] Error parsing line: {e}")
                    remaining_items.append(json.loads(line) if line else {})

            if not unassigned_items:
                print(f"[UBI UNASSIGN] No matching line items found in {s3_key}")
                continue

            # Extract date parts from source key
            y, m, d = _extract_ymd_from_key(s3_key)
            base = _basename_from_key(s3_key)

            # Write unassigned items back to Stage 7 (POST_ENTRATA_PREFIX)
            unassigned_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), unassigned_items)
            print(f"[UBI UNASSIGN] Wrote {len(unassigned_items)} items back to Stage 7: {unassigned_key}")

            # Update Stage 8 file: rewrite with remaining items or delete if empty
            if remaining_items:
                new_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_items)
                # Delete original file since we wrote to a new file
                if new_key != s3_key:
                    s3.delete_object(Bucket=BUCKET, Key=s3_key)
                    print(f"[UBI UNASSIGN] Deleted original {s3_key}, remaining items in {new_key}")
                else:
                    print(f"[UBI UNASSIGN] Rewrote {len(remaining_items)} remaining items to Stage 8")
            else:
                s3.delete_object(Bucket=BUCKET, Key=s3_key)
                print(f"[UBI UNASSIGN] Deleted empty Stage 8 file {s3_key}")

            total_unassigned += len(unassigned_items)

        if total_unassigned == 0:
            return JSONResponse({"error": "No matching line items found"}, status_code=400)

        # Clean up exclusion sources so unassigned bills reappear in the pool
        all_unassigned_line_hashes = set()
        for hashes_set in by_s3_key.values():
            all_unassigned_line_hashes.update(hashes_set)

        # 1) Delete matching records from jrk-bill-ubi-assignments DDB table
        # New format: assignment_id = line_hash (direct deletion)
        # Also handle old format: assignment_id = line_hash[:32]-date (scan-based)
        deleted_count = 0
        not_found_hashes = set()

        # First: try direct deletion by line_hash (new format)
        for lh in all_unassigned_line_hashes:
            try:
                ddb.delete_item(
                    TableName='jrk-bill-ubi-assignments',
                    Key={'assignment_id': {'S': lh}}
                )
                deleted_count += 1
                print(f"[UBI UNASSIGN] Deleted DDB record for {lh[:24]}...")
            except ddb.exceptions.ResourceNotFoundException:
                not_found_hashes.add(lh)
            except Exception as e:
                # delete_item doesn't error if key doesn't exist, so this is a real error
                print(f"[UBI UNASSIGN] Warning: could not delete DDB assignment {lh[:24]}: {e}")
                not_found_hashes.add(lh)

        # Second: scan for old-format records (assignment_id = line_hash[:32]-date)
        if not_found_hashes:
            print(f"[UBI UNASSIGN] Scanning for {len(not_found_hashes)} old-format records...")
            try:
                ddb_paginator = ddb.get_paginator('scan')
                for page in ddb_paginator.paginate(
                    TableName='jrk-bill-ubi-assignments',
                    ProjectionExpression='assignment_id, line_hash'
                ):
                    for item in page.get('Items', []):
                        lh = item.get('line_hash', {}).get('S', '')
                        if lh in not_found_hashes:
                            aid = item['assignment_id']['S']
                            try:
                                ddb.delete_item(
                                    TableName='jrk-bill-ubi-assignments',
                                    Key={'assignment_id': {'S': aid}}
                                )
                                deleted_count += 1
                                not_found_hashes.discard(lh)
                                print(f"[UBI UNASSIGN] Deleted old-format record {aid}")
                            except Exception as e:
                                print(f"[UBI UNASSIGN] Warning: could not delete {aid}: {e}")
            except Exception as e:
                print(f"[UBI UNASSIGN] Warning: scan for old records failed: {e}")

        print(f"[UBI UNASSIGN] Deleted {deleted_count} DDB assignment records")

        # 2) Invalidate exclusion cache so unassigned bills reappear immediately
        invalidate_exclusion_cache()

        # 3) Invalidate the Stage 8 history cache so BILLBACK shows correct
        # duplicate warnings and suggestions after unassign
        global _last_ubi_periods_cache
        _last_ubi_periods_cache = {"data": {}, "expires": 0}
        print("[UBI UNASSIGN] Invalidated Stage 8 cache")

        print(f"[UBI UNASSIGN] COMPLETED: Moved {total_unassigned} items back to Stage 7")
        return {"ok": True, "unassigned": total_unassigned}

    except Exception as e:
        print(f"[UBI UNASSIGN] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/unassign-account")
async def api_billback_ubi_unassign_account(request: Request, user: str = Depends(require_user)):
    """Unassign ALL line items for a specific account from a specific period.
    Used from the UBI Completion Tracker to quickly unassign an account.
    """
    try:
        form = await request.form()
        s3_key = form.get("s3_key", "").strip()
        account_number = form.get("account_number", "").strip()
        period = form.get("period", "").strip()

        print(f"[UBI UNASSIGN ACCOUNT] User {user} unassigning account={account_number} period={period}")

        if not account_number:
            return JSONResponse({"error": "Account number required"}, status_code=400)
        if not period:
            return JSONResponse({"error": "Period required"}, status_code=400)

        # If we have a specific s3_key, use it; otherwise scan Stage 8 for this account+period
        s3_keys_to_check = []
        if s3_key:
            s3_keys_to_check.append(s3_key)
        else:
            # Scan Stage 8 for files containing this account
            from datetime import timedelta
            end_date = datetime.now()
            start_date = end_date - timedelta(days=365)
            prefixes_to_scan = set()
            current = start_date
            while current <= end_date:
                prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={current.year}/mm={current.month:02d}/"
                prefixes_to_scan.add(prefix)
                if current.month == 12:
                    current = current.replace(year=current.year + 1, month=1, day=1)
                else:
                    current = current.replace(month=current.month + 1, day=1)

            for prefix in prefixes_to_scan:
                try:
                    paginator = s3.get_paginator('list_objects_v2')
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get('Contents', []):
                            key = obj['Key']
                            if key.endswith('.jsonl') and account_number in key:
                                s3_keys_to_check.append(key)
                except Exception:
                    continue

        if not s3_keys_to_check:
            return JSONResponse({"error": f"No Stage 8 files found for account {account_number}"}, status_code=404)

        print(f"[UBI UNASSIGN ACCOUNT] Checking {len(s3_keys_to_check)} S3 file(s)")

        total_unassigned = 0
        line_hashes_unassigned = set()

        for key in s3_keys_to_check:
            try:
                body = _read_s3_text(BUCKET, key)
            except Exception as e:
                print(f"[UBI UNASSIGN ACCOUNT] Error reading {key}: {e}")
                continue

            unassigned_items = []
            remaining_items = []

            for line in body.splitlines():
                line = (line or '').strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    rec_acct = rec.get("Account Number", rec.get("AccountNumber", ""))

                    # Check if this record matches account AND has the specified period
                    has_period_match = False
                    if rec_acct == account_number:
                        ubi_assignments = rec.get("ubi_assignments", [])
                        if ubi_assignments:
                            for asn in ubi_assignments:
                                if asn.get("period", "") == period or period in asn.get("period", ""):
                                    has_period_match = True
                                    break
                        else:
                            rec_period = rec.get("ubi_period", "")
                            if rec_period == period or period in rec_period:
                                has_period_match = True

                    if has_period_match:
                        # Remove UBI assignment fields
                        line_hash = _compute_stable_line_hash(rec)
                        line_hashes_unassigned.add(line_hash)
                        for field in ["ubi_period", "ubi_assigned_date", "ubi_assigned_by", "ubi_amount",
                                      "ubi_months_total", "ubi_notes", "ubi_assignments", "ubi_period_count"]:
                            rec.pop(field, None)
                        unassigned_items.append(rec)
                    else:
                        remaining_items.append(rec)
                except Exception:
                    remaining_items.append(json.loads(line) if line else {})

            if not unassigned_items:
                continue

            # Extract date parts from key
            y, m, d = _extract_ymd_from_key(key)
            base = _basename_from_key(key)

            # Write unassigned items back to Stage 7
            unassigned_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), unassigned_items)
            print(f"[UBI UNASSIGN ACCOUNT] Wrote {len(unassigned_items)} items back to Stage 7: {unassigned_key}")

            # Update or delete Stage 8 file
            if remaining_items:
                new_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_items)
                if new_key != key:
                    s3.delete_object(Bucket=BUCKET, Key=key)
            else:
                s3.delete_object(Bucket=BUCKET, Key=key)

            total_unassigned += len(unassigned_items)

        if total_unassigned == 0:
            return JSONResponse({"error": f"No matching line items found for account {account_number} in period {period}"}, status_code=404)

        # Clean up DDB assignments
        # New format: assignment_id = line_hash (direct deletion)
        # Also handle old format: assignment_id = line_hash[:32]-date (scan-based)
        deleted_count = 0
        not_found_hashes = set()

        # First: try direct deletion by line_hash (new format)
        for lh in line_hashes_unassigned:
            try:
                ddb.delete_item(
                    TableName='jrk-bill-ubi-assignments',
                    Key={'assignment_id': {'S': lh}}
                )
                deleted_count += 1
            except Exception:
                not_found_hashes.add(lh)

        # Second: scan for old-format records (assignment_id = line_hash[:32]-date)
        if not_found_hashes:
            try:
                ddb_paginator = ddb.get_paginator('scan')
                for page in ddb_paginator.paginate(
                    TableName='jrk-bill-ubi-assignments',
                    ProjectionExpression='assignment_id, line_hash'
                ):
                    for item in page.get('Items', []):
                        lh = item.get('line_hash', {}).get('S', '')
                        if lh in not_found_hashes:
                            aid = item['assignment_id']['S']
                            try:
                                ddb.delete_item(
                                    TableName='jrk-bill-ubi-assignments',
                                    Key={'assignment_id': {'S': aid}}
                                )
                                deleted_count += 1
                                not_found_hashes.discard(lh)
                            except Exception:
                                pass
            except Exception as e:
                print(f"[UBI UNASSIGN ACCOUNT] Warning: scan for old records failed: {e}")

        print(f"[UBI UNASSIGN ACCOUNT] Deleted {deleted_count} DDB records")

        # Invalidate caches
        invalidate_exclusion_cache()

        # CRITICAL: Invalidate the Stage 8 history cache so BILLBACK shows correct
        # duplicate warnings and suggestions after unassign
        global _last_ubi_periods_cache
        _last_ubi_periods_cache = {"data": {}, "expires": 0}
        print("[UBI UNASSIGN ACCOUNT] Invalidated Stage 8 cache")

        print(f"[UBI UNASSIGN ACCOUNT] COMPLETED: Unassigned {total_unassigned} items for {account_number} from {period}")
        return {"ok": True, "unassigned": total_unassigned}

    except Exception as e:
        print(f"[UBI UNASSIGN ACCOUNT] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/cleanup-exclusions")
async def api_billback_ubi_cleanup_exclusions(request: Request, user: str = Depends(require_user)):
    """Remove orphaned exclusion records from DDB for a specific S3 file.
    Used to fix cases where unassign failed to clean up exclusion hashes.
    """
    try:
        form = await request.form()
        s3_key = form.get("s3_key", "").strip()

        if not s3_key:
            return JSONResponse({"error": "s3_key required"}, status_code=400)

        print(f"[CLEANUP EXCLUSIONS] User {user} cleaning up for {s3_key}")

        # Read the S3 file and compute line hashes
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as e:
            return JSONResponse({"error": f"Could not read S3 file: {e}"}, status_code=404)

        line_hashes = set()
        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                lh = _compute_stable_line_hash(rec)
                line_hashes.add(lh)
            except Exception:
                pass

        if not line_hashes:
            return JSONResponse({"error": "No line hashes found in file"}, status_code=400)

        print(f"[CLEANUP EXCLUSIONS] Found {len(line_hashes)} line hashes in file")

        # Delete each hash from exclusion table (try both new and old formats)
        deleted_count = 0
        not_found = set()

        # Try direct deletion by line_hash (new format)
        for lh in line_hashes:
            try:
                ddb.delete_item(
                    TableName='jrk-bill-ubi-assignments',
                    Key={'assignment_id': {'S': lh}}
                )
                deleted_count += 1
            except Exception:
                not_found.add(lh)

        # Scan for old-format records
        if not_found:
            try:
                ddb_paginator = ddb.get_paginator('scan')
                for page in ddb_paginator.paginate(
                    TableName='jrk-bill-ubi-assignments',
                    ProjectionExpression='assignment_id, line_hash'
                ):
                    for item in page.get('Items', []):
                        lh = item.get('line_hash', {}).get('S', '')
                        if lh in not_found:
                            aid = item['assignment_id']['S']
                            try:
                                ddb.delete_item(
                                    TableName='jrk-bill-ubi-assignments',
                                    Key={'assignment_id': {'S': aid}}
                                )
                                deleted_count += 1
                                not_found.discard(lh)
                            except Exception:
                                pass
            except Exception as e:
                print(f"[CLEANUP EXCLUSIONS] Warning: scan failed: {e}")

        # Invalidate cache
        invalidate_exclusion_cache()

        print(f"[CLEANUP EXCLUSIONS] Deleted {deleted_count}/{len(line_hashes)} exclusion records")
        return {"ok": True, "deleted": deleted_count, "total_hashes": len(line_hashes)}

    except Exception as e:
        print(f"[CLEANUP EXCLUSIONS] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/reassign-account")
async def api_billback_ubi_reassign_account(request: Request, user: str = Depends(require_user)):
    """Reassign ALL line items for a specific account from one period to another.
    Used from the UBI Completion Tracker for quick reassignment.
    """
    try:
        form = await request.form()
        s3_key = form.get("s3_key", "").strip()
        account_number = form.get("account_number", "").strip()
        old_period = form.get("old_period", "").strip()
        new_period = form.get("new_period", "").strip()

        print(f"[UBI REASSIGN ACCOUNT] User {user} reassigning account={account_number} from {old_period} to {new_period}")

        if not account_number:
            return JSONResponse({"error": "Account number required"}, status_code=400)
        if not old_period or not new_period:
            return JSONResponse({"error": "Both old and new periods required"}, status_code=400)
        if old_period == new_period:
            return JSONResponse({"error": "New period must be different from old period"}, status_code=400)

        # Find S3 files to check
        s3_keys_to_check = []
        if s3_key:
            s3_keys_to_check.append(s3_key)
        else:
            from datetime import timedelta
            end_date = datetime.now()
            start_date = end_date - timedelta(days=365)
            prefixes_to_scan = set()
            current = start_date
            while current <= end_date:
                prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={current.year}/mm={current.month:02d}/"
                prefixes_to_scan.add(prefix)
                if current.month == 12:
                    current = current.replace(year=current.year + 1, month=1, day=1)
                else:
                    current = current.replace(month=current.month + 1, day=1)

            for prefix in prefixes_to_scan:
                try:
                    paginator = s3.get_paginator('list_objects_v2')
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get('Contents', []):
                            key = obj['Key']
                            if key.endswith('.jsonl') and account_number in key:
                                s3_keys_to_check.append(key)
                except Exception:
                    continue

        if not s3_keys_to_check:
            return JSONResponse({"error": f"No Stage 8 files found for account {account_number}"}, status_code=404)

        print(f"[UBI REASSIGN ACCOUNT] Checking {len(s3_keys_to_check)} S3 file(s)")

        total_reassigned = 0

        for key in s3_keys_to_check:
            try:
                body = _read_s3_text(BUCKET, key)
            except Exception as e:
                print(f"[UBI REASSIGN ACCOUNT] Error reading {key}: {e}")
                continue

            modified_items = []
            has_changes = False

            for line in body.splitlines():
                line = (line or '').strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    rec_acct = rec.get("Account Number", rec.get("AccountNumber", ""))

                    # Check if this record matches account AND has the old period
                    if rec_acct == account_number:
                        ubi_assignments = rec.get("ubi_assignments", [])
                        if ubi_assignments:
                            # Multi-period format: update matching period entries
                            changed = False
                            for asn in ubi_assignments:
                                if asn.get("period", "") == old_period or old_period in asn.get("period", ""):
                                    asn["period"] = new_period
                                    changed = True
                            if changed:
                                rec["ubi_assignments"] = ubi_assignments
                                has_changes = True
                                total_reassigned += 1
                        else:
                            # Legacy format
                            rec_period = rec.get("ubi_period", "")
                            if rec_period == old_period or old_period in rec_period:
                                rec["ubi_period"] = new_period
                                has_changes = True
                                total_reassigned += 1

                    modified_items.append(rec)
                except Exception:
                    modified_items.append(json.loads(line) if line else {})

            if has_changes and modified_items:
                # Write back to same file
                y, m, d = _extract_ymd_from_key(key)
                base = _basename_from_key(key)
                new_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), modified_items)
                if new_key != key:
                    s3.delete_object(Bucket=BUCKET, Key=key)
                print(f"[UBI REASSIGN ACCOUNT] Updated {key} with new periods")

        if total_reassigned == 0:
            return JSONResponse({"error": f"No matching line items found for account {account_number} in period {old_period}"}, status_code=404)

        # Invalidate caches
        invalidate_exclusion_cache()

        # CRITICAL: Invalidate the Stage 8 history cache so BILLBACK shows correct
        # duplicate warnings and suggestions after reassign
        global _last_ubi_periods_cache
        _last_ubi_periods_cache = {"data": {}, "expires": 0}
        print("[UBI REASSIGN ACCOUNT] Invalidated Stage 8 cache")

        print(f"[UBI REASSIGN ACCOUNT] COMPLETED: Reassigned {total_reassigned} items for {account_number} from {old_period} to {new_period}")
        return {"ok": True, "reassigned": total_reassigned}

    except Exception as e:
        print(f"[UBI REASSIGN ACCOUNT] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/reassign")
async def api_billback_ubi_reassign(request: Request, user: str = Depends(require_user)):
    """Reassign line items to a different UBI period - updates Stage 8 files."""
    try:
        from collections import defaultdict

        form = await request.form()
        assignment_ids_str = form.get("assignment_ids", "").strip()
        old_period = form.get("old_period", "").strip()
        new_period = form.get("new_period", "").strip()
        new_amount = form.get("new_amount", "")  # Optional: update amount too

        if not assignment_ids_str:
            return JSONResponse({"error": "No line items selected"}, status_code=400)

        if not new_period:
            return JSONResponse({"error": "New period is required"}, status_code=400)

        # Parse assignment IDs (format: s3_key||line_hash) and group by s3_key
        by_s3_key = defaultdict(set)
        for aid in assignment_ids_str.split(","):
            aid = aid.strip()
            if not aid:
                continue
            if "||" in aid:
                s3_key, line_hash = aid.split("||", 1)
                by_s3_key[s3_key].add(line_hash)
            else:
                print(f"[UBI REASSIGN] Invalid assignment ID format: {aid[:50]}...")

        if not by_s3_key:
            return JSONResponse({"error": "No valid assignment IDs provided"}, status_code=400)

        now_utc = dt.datetime.utcnow().isoformat() + "Z"
        total_updated = 0

        print(f"[UBI REASSIGN] Reassigning {sum(len(v) for v in by_s3_key.values())} lines across {len(by_s3_key)} files to {new_period}")

        # Process each S3 file
        for s3_key, line_hashes_to_update in by_s3_key.items():
            print(f"[UBI REASSIGN] Processing {s3_key} - {len(line_hashes_to_update)} line(s)")

            # Read the Stage 8 file
            try:
                body = _read_s3_text(BUCKET, s3_key)
            except Exception as read_err:
                print(f"[UBI REASSIGN] Error reading S3 file {s3_key}: {read_err}")
                continue

            # Parse and update lines
            updated_lines = []
            file_updated_count = 0

            for line in body.splitlines():
                line = (line or '').strip()
                if not line:
                    continue
                try:
                    rec = json.loads(line)
                    line_hash = _compute_stable_line_hash(rec)

                    if line_hash in line_hashes_to_update:
                        # Update ubi_assignments array
                        ubi_assignments = rec.get("ubi_assignments", [])

                        if ubi_assignments:
                            # Find and update matching period, or add new
                            found = False
                            for asn in ubi_assignments:
                                if old_period and asn.get("period") == old_period:
                                    asn["period"] = new_period
                                    asn["reassigned_date"] = now_utc
                                    asn["reassigned_by"] = user
                                    if new_amount:
                                        asn["amount"] = float(new_amount)
                                    found = True
                                    break
                            if not found:
                                # Add as new period
                                new_asn = {
                                    "period": new_period,
                                    "amount": float(new_amount) if new_amount else rec.get("ubi_amount", 0),
                                    "months": 1,
                                    "assigned_by": user,
                                    "assigned_date": now_utc,
                                }
                                ubi_assignments.append(new_asn)

                            # Re-sort and update legacy fields
                            ubi_assignments.sort(key=lambda x: x.get("period", ""))
                            first = ubi_assignments[0]
                            rec["ubi_period"] = first["period"]
                            rec["ubi_amount"] = first["amount"]
                            rec["ubi_period_count"] = len(ubi_assignments)
                        else:
                            # Legacy format - just update the single period
                            rec["ubi_period"] = new_period
                            if new_amount:
                                rec["ubi_amount"] = float(new_amount)
                            rec["ubi_reassigned_date"] = now_utc
                            rec["ubi_reassigned_by"] = user

                        file_updated_count += 1
                        print(f"[UBI REASSIGN] Updated line {line_hash[:16]}...")

                    updated_lines.append(rec)
                except (json.JSONDecodeError, ValueError, TypeError) as e:
                    print(f"[UBI REASSIGN] Error parsing line: {e}")
                    updated_lines.append(json.loads(line) if line else {})

            if file_updated_count > 0:
                # Write back to Stage 8
                y, m, d = _extract_ymd_from_key(s3_key)
                base = _basename_from_key(s3_key)
                new_key = _write_jsonl(UBI_ASSIGNED_PREFIX, y, m, d, base.replace('.jsonl', ''), updated_lines)

                # Delete original if we wrote to a new key
                if new_key != s3_key:
                    s3.delete_object(Bucket=BUCKET, Key=s3_key)
                    print(f"[UBI REASSIGN] Deleted original {s3_key}, updated in {new_key}")

                print(f"[UBI REASSIGN] Wrote {len(updated_lines)} lines back to Stage 8")

                total_updated += file_updated_count

        if total_updated == 0:
            return JSONResponse({"error": "No matching line items found"}, status_code=400)

        # Invalidate the Stage 8 history cache so BILLBACK shows correct
        # duplicate warnings and suggestions after reassign
        global _last_ubi_periods_cache
        _last_ubi_periods_cache = {"data": {}, "expires": 0}
        print("[UBI REASSIGN] Invalidated Stage 8 cache")

        print(f"[UBI REASSIGN] COMPLETED: Updated {total_updated} items to period {new_period}")
        return {"ok": True, "reassigned": total_updated, "new_period": new_period}

    except Exception as e:
        print(f"[UBI REASSIGN] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/ubi/archive")
async def api_billback_ubi_archive(request: Request, user: str = Depends(require_user)):
    """Archive line items - moves items from Stage 7/8 to Stage 99 (Historical Archive)."""
    try:
        from datetime import datetime

        form = await request.form()
        line_hashes_str = form.get("line_hashes", "").strip()
        s3_key = form.get("s3_key", "").strip()

        if not line_hashes_str:
            return JSONResponse({"error": "Line hashes are required"}, status_code=400)

        if not s3_key:
            return JSONResponse({"error": "S3 key is required"}, status_code=400)

        line_hashes_to_archive = set(h.strip() for h in line_hashes_str.split(",") if h.strip())

        if not line_hashes_to_archive:
            return JSONResponse({"error": "No valid line hashes provided"}, status_code=400)

        now_utc = datetime.utcnow().isoformat() + "Z"

        print(f"[UBI ARCHIVE] Archiving {len(line_hashes_to_archive)} line(s) from {s3_key}")
        print(f"[UBI ARCHIVE] User: {user}")

        # Read the JSONL file from S3
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as read_err:
            print(f"[UBI ARCHIVE] Error reading S3 file: {read_err}")
            return JSONResponse({"error": f"Could not read S3 file: {read_err}"}, status_code=500)

        # Parse all lines from file
        all_recs = []
        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                all_recs.append(rec)
            except (json.JSONDecodeError, ValueError, TypeError) as e:
                print(f"[UBI ARCHIVE] Error parsing line: {e}")

        # Try exact hash matching first
        archived_items = []
        remaining_items = []

        for rec in all_recs:
            line_hash = _compute_stable_line_hash(rec)
            if line_hash in line_hashes_to_archive:
                rec["archived_date"] = now_utc
                rec["archived_by"] = user
                archived_items.append(rec)
                print(f"[UBI ARCHIVE] Archived line {line_hash[:16]}...")
            else:
                remaining_items.append(rec)

        # If exact hash match failed but user selected ALL lines in the file,
        # archive everything (the file was likely modified but user's intent is clear)
        if not archived_items and len(line_hashes_to_archive) == len(all_recs):
            print(f"[UBI ARCHIVE] Hash mismatch but count matches ({len(all_recs)} lines) - archiving all")
            archived_items = []
            for rec in all_recs:
                rec["archived_date"] = now_utc
                rec["archived_by"] = user
                archived_items.append(rec)
            remaining_items = []

        if not archived_items:
            # Still no match - file was modified, ask user to reload
            file_hashes = [_compute_stable_line_hash(r)[:16] for r in all_recs]
            requested_hashes = [h[:16] for h in line_hashes_to_archive]
            print(f"[UBI ARCHIVE] HASH MISMATCH - Requested {len(requested_hashes)}: {requested_hashes}")
            print(f"[UBI ARCHIVE] HASH MISMATCH - File has {len(file_hashes)}: {file_hashes}")
            return JSONResponse({
                "error": f"File was modified since loading. Please click 'Load Unassigned Bills' to refresh and try again. (Requested {len(requested_hashes)} lines, file has {len(file_hashes)})"
            }, status_code=400)

        # Extract date parts from source key for writing
        y, m, d = _extract_ymd_from_key(s3_key)
        base = _basename_from_key(s3_key)

        # Write archived items to Stage 99 (HIST_ARCHIVE_PREFIX)
        archive_key = _write_jsonl(HIST_ARCHIVE_PREFIX, y, m, d, base.replace('.jsonl', ''), archived_items)
        print(f"[UBI ARCHIVE] Wrote {len(archived_items)} archived items to {archive_key}")

        # Determine source prefix to rewrite remaining items
        if UBI_ASSIGNED_PREFIX in s3_key:
            source_prefix = UBI_ASSIGNED_PREFIX
        else:
            source_prefix = POST_ENTRATA_PREFIX

        # Update source file: rewrite with remaining items or delete if empty
        if remaining_items:
            _write_jsonl(source_prefix, y, m, d, base.replace('.jsonl', ''), remaining_items)
            print(f"[UBI ARCHIVE] Rewrote {len(remaining_items)} remaining items to source")
        else:
            s3.delete_object(Bucket=BUCKET, Key=s3_key)
            print(f"[UBI ARCHIVE] Deleted empty source file {s3_key}")

        # Invalidate exclusion cache so archived bills are immediately excluded
        invalidate_exclusion_cache()

        print(f"[UBI ARCHIVE] COMPLETED: Moved {len(archived_items)} items to Stage 99")
        return {"ok": True, "archived": len(archived_items)}

    except Exception as e:
        print(f"[UBI ARCHIVE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# ============================================================
# FLAGGED FOR REVIEW MODULE - Quality control for submitted bills
# ============================================================

@app.get("/flagged", response_class=HTMLResponse)
def flagged_review_view(request: Request, user: str = Depends(require_user)):
    """Flagged for Review page - admin only."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("flagged_review.html", {"request": request, "user": user})


@app.post("/api/billback/flag")
async def api_billback_flag(request: Request, user: str = Depends(require_user)):
    """Flag items for review - moves items from Stage 7/8 to Stage 9 (Flagged Review).

    Supports per-line notes via line_notes JSON: {"line_hash": "note text", ...}
    """
    print(f"[FLAG REVIEW] Received flag request from user: {user}")
    try:
        from datetime import datetime

        # Admin only - flagging is a QA function
        if user not in ADMIN_USERS:
            print(f"[FLAG REVIEW] Rejected non-admin user: {user}")
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        form = await request.form()
        line_hashes_str = form.get("line_hashes", "").strip()
        s3_key = form.get("s3_key", "").strip()
        reason = form.get("reason", "").strip() or "Flagged for review"
        submitter = form.get("submitter", "").strip() or "Unknown"
        line_notes_json = form.get("line_notes", "").strip()  # JSON dict of {hash: note}

        # Parse per-line notes
        line_notes = {}
        if line_notes_json:
            try:
                line_notes = json.loads(line_notes_json)
            except (json.JSONDecodeError, ValueError):
                pass

        if not line_hashes_str:
            return JSONResponse({"error": "Line hashes are required"}, status_code=400)

        if not s3_key:
            return JSONResponse({"error": "S3 key is required"}, status_code=400)

        if not _validate_s3_key(s3_key):
            return JSONResponse({"error": "Invalid S3 key"}, status_code=400)

        line_hashes_to_flag = set(h.strip() for h in line_hashes_str.split(",") if h.strip())

        if not line_hashes_to_flag:
            print(f"[FLAG REVIEW] No valid line hashes provided")
            return JSONResponse({"error": "No valid line hashes provided"}, status_code=400)

        now_utc = datetime.utcnow().isoformat() + "Z"

        print(f"[FLAG REVIEW] Processing: {len(line_hashes_to_flag)} line(s) from {s3_key}")
        print(f"[FLAG REVIEW] Flagged by: {user}, Submitter: {submitter}, Reason: {reason}")
        if line_notes:
            print(f"[FLAG REVIEW] Per-line notes provided for {len(line_notes)} lines")

        # Read the JSONL file from S3
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as read_err:
            print(f"[FLAG REVIEW] Error reading S3 file: {read_err}")
            return JSONResponse({"error": "Could not read source file. It may have been moved or deleted."}, status_code=500)

        # Parse all lines from file
        all_recs = []
        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                all_recs.append(rec)
            except (json.JSONDecodeError, ValueError, TypeError) as e:
                print(f"[FLAG REVIEW] Error parsing line: {e}")

        # Check if ANY of the requested hashes match lines in this file
        # If so, we flag the ENTIRE invoice (all lines), not just selected ones
        matched_any = False
        for rec in all_recs:
            line_hash = _compute_stable_line_hash(rec)
            if line_hash in line_hashes_to_flag:
                matched_any = True
                break

        # Fallback: if no hash match but count matches, assume user selected all
        if not matched_any and len(line_hashes_to_flag) == len(all_recs):
            print(f"[FLAG REVIEW] Hash mismatch but count matches ({len(all_recs)} lines) - flagging all")
            matched_any = True

        if not matched_any:
            return JSONResponse({
                "error": "File was modified since loading. Please refresh and try again."
            }, status_code=400)

        # Flag ALL lines in the invoice (move entire invoice to review)
        # Get the invoice-level note (all lines share the same note)
        invoice_note = ""
        for hash_val in line_hashes_to_flag:
            if hash_val in line_notes and line_notes[hash_val]:
                invoice_note = line_notes[hash_val]
                break  # Use the first note found (they should all be the same for invoice-level)

        flagged_items = []
        for rec in all_recs:
            rec["flagged_date"] = now_utc
            rec["flagged_by"] = user
            rec["flagged_reason"] = reason
            rec["original_submitter"] = submitter
            rec["source_s3_key"] = s3_key
            if invoice_note:
                rec["flagged_note"] = invoice_note
            flagged_items.append(rec)

        print(f"[FLAG REVIEW] Flagging ENTIRE invoice: {len(flagged_items)} lines")

        # Extract date parts from source key for writing
        y, m, d = _extract_ymd_from_key(s3_key)
        base = _basename_from_key(s3_key)

        # Write flagged items to Stage 9 (FLAGGED_REVIEW_PREFIX)
        flag_key = _write_jsonl(FLAGGED_REVIEW_PREFIX, y, m, d, base.replace('.jsonl', ''), flagged_items)
        print(f"[FLAG REVIEW] Wrote {len(flagged_items)} flagged items to {flag_key}")

        # Delete the source file (entire invoice moved to review)
        s3.delete_object(Bucket=BUCKET, Key=s3_key)
        print(f"[FLAG REVIEW] Deleted source file {s3_key}")

        print(f"[FLAG REVIEW] COMPLETED: Moved {len(flagged_items)} items to Stage 9")
        return {"ok": True, "flagged": len(flagged_items)}

    except Exception as e:
        print(f"[FLAG REVIEW] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/flagged")
def api_flagged_list(user: str = Depends(require_user), days_back: int = 90):
    """Get all flagged items grouped by submitter."""
    try:
        from datetime import datetime, timedelta
        from collections import defaultdict
        from concurrent.futures import ThreadPoolExecutor, as_completed

        if user not in ADMIN_USERS:
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        today = datetime.utcnow()

        def fetch_day_flagged(d):
            """Fetch flagged items for a single day."""
            items = []
            prefix = f"{FLAGGED_REVIEW_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if not key.endswith('.jsonl'):
                            continue
                        try:
                            body = _read_s3_text(BUCKET, key)
                            for line in body.splitlines():
                                line = (line or '').strip()
                                if not line:
                                    continue
                                rec = json.loads(line)
                                rec['_s3_key'] = key
                                rec['_line_hash'] = _compute_stable_line_hash(rec)
                                items.append(rec)
                        except Exception as e:
                            print(f"[FLAGGED LIST] Error reading {key}: {e}")
            except Exception as e:
                print(f"[FLAGGED LIST] Error listing prefix {prefix}: {e}")
            return items

        # Parallel fetch all days
        all_flagged = []
        dates_to_scan = [today - timedelta(days=i) for i in range(days_back)]

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(fetch_day_flagged, d): d for d in dates_to_scan}
            for future in as_completed(futures):
                try:
                    items = future.result()
                    all_flagged.extend(items)
                except Exception as e:
                    print(f"[FLAGGED LIST] Worker error: {e}")

        # Group by submitter
        by_submitter = defaultdict(list)
        for rec in all_flagged:
            submitter = rec.get('original_submitter', rec.get('Submitter', rec.get('SubmittedBy', 'Unknown')))
            by_submitter[submitter].append(rec)

        # Build response with stats
        submitters = []
        for submitter, items in sorted(by_submitter.items(), key=lambda x: -len(x[1])):
            total_dollars = 0.0
            for item in items:
                amt = item.get("Line Item Charge") or item.get("AMOUNT") or 0
                try:
                    total_dollars += float(str(amt).replace('$', '').replace(',', ''))
                except (ValueError, TypeError):
                    pass
            submitters.append({
                "submitter": submitter,
                "count": len(items),
                "total_dollars": total_dollars,
                "items": items
            })

        return {
            "ok": True,
            "total_flagged": len(all_flagged),
            "submitters": submitters
        }

    except Exception as e:
        print(f"[FLAGGED LIST] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/flagged/unflag")
async def api_flagged_unflag(request: Request, user: str = Depends(require_user)):
    """Unflag items - moves items from Stage 9 back to Stage 7 (PostEntrata)."""
    try:
        from datetime import datetime

        if user not in ADMIN_USERS:
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        form = await request.form()
        line_hashes_str = form.get("line_hashes", "").strip()
        s3_key = form.get("s3_key", "").strip()

        if not line_hashes_str:
            return JSONResponse({"error": "Line hashes are required"}, status_code=400)

        if not s3_key:
            return JSONResponse({"error": "S3 key is required"}, status_code=400)

        if not _validate_s3_key(s3_key):
            return JSONResponse({"error": "Invalid S3 key"}, status_code=400)

        # Handle "ALL" to unflag entire invoice
        unflag_all = line_hashes_str.upper() == "ALL"
        line_hashes_to_unflag = set() if unflag_all else set(h.strip() for h in line_hashes_str.split(",") if h.strip())

        if not unflag_all and not line_hashes_to_unflag:
            return JSONResponse({"error": "No valid line hashes provided"}, status_code=400)

        now_utc = datetime.utcnow().isoformat() + "Z"

        print(f"[UNFLAG] Unflagging {'ALL lines' if unflag_all else f'{len(line_hashes_to_unflag)} line(s)'} from {s3_key}")

        # Read the flagged JSONL file
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as read_err:
            print(f"[UNFLAG] Error reading S3 file: {read_err}")
            return JSONResponse({"error": "Could not read flagged file. It may have been moved or deleted."}, status_code=500)

        # Parse all lines
        all_recs = []
        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                all_recs.append(rec)
            except (json.JSONDecodeError, ValueError, TypeError):
                pass

        # Match by hash (or all if unflag_all)
        unflagged_items = []
        remaining_flagged = []

        for rec in all_recs:
            line_hash = _compute_stable_line_hash(rec)
            if unflag_all or line_hash in line_hashes_to_unflag:
                # Remove flagging metadata before returning to billback
                rec.pop("flagged_date", None)
                rec.pop("flagged_by", None)
                rec.pop("flagged_reason", None)
                rec.pop("flagged_note", None)
                rec.pop("original_submitter", None)
                rec.pop("source_s3_key", None)
                rec["unflagged_date"] = now_utc
                rec["unflagged_by"] = user
                unflagged_items.append(rec)
            else:
                remaining_flagged.append(rec)

        if not unflagged_items:
            return JSONResponse({"error": "No matching items found"}, status_code=400)

        # Extract date parts
        y, m, d = _extract_ymd_from_key(s3_key)
        base = _basename_from_key(s3_key)

        # Write unflagged items back to Stage 7 (PostEntrata)
        unflag_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), unflagged_items)
        print(f"[UNFLAG] Wrote {len(unflagged_items)} items back to Stage 7: {unflag_key}")

        # Update Stage 9 file: rewrite with remaining items or delete if empty
        if remaining_flagged:
            _write_jsonl(FLAGGED_REVIEW_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_flagged)
        else:
            s3.delete_object(Bucket=BUCKET, Key=s3_key)
            print(f"[UNFLAG] Deleted empty flagged file {s3_key}")

        print(f"[UNFLAG] COMPLETED: Moved {len(unflagged_items)} items back to Stage 7")
        return {"ok": True, "unflagged": len(unflagged_items)}

    except Exception as e:
        print(f"[UNFLAG] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/flagged/generate-email")
async def api_flagged_generate_email(request: Request, user: str = Depends(require_user)):
    """Generate an email template for flagged items grouped by invoice."""
    try:
        if user not in ADMIN_USERS:
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        body = await request.json()
        submitter = body.get("submitter", "").strip()
        items = body.get("items", [])
        custom_message = body.get("message", "").strip()

        if not submitter or not items:
            return JSONResponse({"error": "Submitter and items are required"}, status_code=400)

        APP_BASE_URL = "https://billreview.jrkanalytics.com"

        # Group items by invoice (source_s3_key)
        import re
        invoices = {}
        for item in items:
            s3_key = item.get("source_s3_key") or item.get("_s3_key") or ""
            if s3_key not in invoices:
                # Get the original PDF key from source_input_key (e.g. Bill_Parser_2_Parsed_Inputs/filename.pdf)
                source_input_key = item.get("source_input_key") or ""
                # Get the original Stage 4 key for parse page link
                stage4_key = item.get("__s3_key__") or ""
                invoices[s3_key] = {
                    "vendor": item.get("EnrichedVendorName") or item.get("Vendor Name") or "Unknown Vendor",
                    "property": item.get("EnrichedPropertyName") or item.get("Property Name") or "Unknown Property",
                    "account": item.get("Account Number") or item.get("Line Item Account Number") or "N/A",
                    "note": item.get("flagged_note") or "",
                    "reason": item.get("flagged_reason") or "",
                    "s3_key": s3_key,
                    "source_input_key": source_input_key,
                    "stage4_key": stage4_key,
                    "lines": [],
                    "total": 0.0
                }
            inv = invoices[s3_key]
            desc = item.get("Line Item Description") or item.get("Charge Code Description") or "Line item"
            amt = item.get("Line Item Charge") or item.get("AMOUNT") or 0
            try:
                amt_val = float(str(amt).replace('$', '').replace(',', ''))
            except (ValueError, TypeError):
                amt_val = 0.0
            inv["total"] += amt_val
            inv["lines"].append({"desc": desc, "amount": amt_val})

        # Build email content grouped by invoice
        total_dollars = sum(inv["total"] for inv in invoices.values())
        invoice_count = len(invoices)

        invoice_sections = []
        for idx, (s3_key, inv) in enumerate(invoices.items(), 1):
            # Format invoice section
            section = f"""
==================================================
INVOICE {idx}: {inv['vendor']}
==================================================
Property:  {inv['property']}
Account:   {inv['account']}
Total:     ${inv['total']:,.2f}
"""
            if inv["note"]:
                section += f"""
[!] FLAGGED NOTE: {inv['note']}
"""
            elif inv["reason"] and inv["reason"] != "Flagged for review":
                section += f"""
[!] ISSUE: {inv['reason']}
"""

            section += f"""
Lines:
"""
            for line in inv["lines"]:
                section += f"  - {line['desc']}: ${line['amount']:,.2f}\n"

            # Build working PDF link using source_input_key with /pdf?k= endpoint
            from urllib.parse import quote
            if inv.get("source_input_key"):
                pdf_url = f"{APP_BASE_URL}/pdf?k={quote(inv['source_input_key'], safe='')}"
                section += f"""
[PDF] View PDF: {pdf_url}
"""

            # Build parse page link using Stage 4 key
            stage4_key = inv.get("stage4_key", "")
            if stage4_key:
                # Extract date from Stage 4 key: Bill_Parser_4_.../yyyy=2026/mm=01/dd=15/...
                date_match = re.search(r'yyyy=(\d{4})/mm=(\d{2})/dd=(\d{2})', stage4_key)
                if date_match:
                    y, m, d = date_match.groups()
                    parse_date = f"{y}-{m}-{d}"
                    # pdf_id is SHA1 of Stage 4 key
                    parse_pdf_id = pdf_id_from_key(stage4_key)
                    parse_url = f"{APP_BASE_URL}/review?date={parse_date}&pdf_id={parse_pdf_id}"
                    section += f"[PARSE] Parse Page: {parse_url}\n"

            invoice_sections.append(section)

        email_subject = f"Bills Flagged for Review - {invoice_count} invoice(s) - ${total_dollars:,.2f}"

        email_body = f"""Hi {submitter.split('@')[0].title()},

The following {invoice_count} invoice(s) totaling ${total_dollars:,.2f} have been flagged for review:
{''.join(invoice_sections)}

==================================================
NEXT STEPS
==================================================
{custom_message if custom_message else '''Please review these invoices and either:
  1. Confirm they are correct (reply to this email)
  2. Fix any errors in Entrata Core and resubmit'''}

Once resolved, these items will be moved back to the BILLBACK queue.

Thanks,
{user.split('@')[0].title()}
"""

        # Include AP supervisors in the To field
        ap_supervisors = "mpastrana@jrk.com;tdavies@jrk.com"
        email_to = f"{submitter};{ap_supervisors}"

        return {
            "ok": True,
            "email": {
                "to": email_to,
                "subject": email_subject,
                "body": email_body
            }
        }

    except Exception as e:
        print(f"[GENERATE EMAIL] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/flagged/confirm")
async def api_flagged_confirm(request: Request, user: str = Depends(require_user)):
    """Confirm or dismiss flagged items as mistakes."""
    try:
        from datetime import datetime

        if user not in ADMIN_USERS:
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        form = await request.form()
        line_hashes_str = form.get("line_hashes", "").strip()
        s3_key = form.get("s3_key", "").strip()
        is_mistake = form.get("is_mistake", "true").lower() == "true"

        if not line_hashes_str or not s3_key:
            return JSONResponse({"error": "Line hashes and S3 key are required"}, status_code=400)

        if not _validate_s3_key(s3_key):
            return JSONResponse({"error": "Invalid S3 key"}, status_code=400)

        # Handle "ALL" to confirm entire invoice
        confirm_all = line_hashes_str.upper() == "ALL"
        line_hashes_to_confirm = set() if confirm_all else set(h.strip() for h in line_hashes_str.split(",") if h.strip())
        if not confirm_all and not line_hashes_to_confirm:
            return JSONResponse({"error": "No valid line hashes provided"}, status_code=400)

        now_utc = datetime.utcnow().isoformat() + "Z"

        print(f"[CONFIRM FLAGGED] Confirming {'ALL items' if confirm_all else f'{len(line_hashes_to_confirm)} items'} as {'MISTAKE' if is_mistake else 'NOT MISTAKE'}")

        # Read the flagged JSONL file
        try:
            body = _read_s3_text(BUCKET, s3_key)
        except Exception as read_err:
            print(f"[CONFIRM FLAGGED] Error reading S3 file: {read_err}")
            return JSONResponse({"error": "Could not read flagged file. It may have been moved or deleted."}, status_code=500)

        # Parse all lines
        all_recs = []
        for line in body.splitlines():
            line = (line or '').strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                all_recs.append(rec)
            except (json.JSONDecodeError, ValueError, TypeError):
                pass

        # Update matching records (or all if confirm_all)
        updated_count = 0
        for rec in all_recs:
            line_hash = _compute_stable_line_hash(rec)
            if confirm_all or line_hash in line_hashes_to_confirm:
                rec["confirmed_date"] = now_utc
                rec["confirmed_by"] = user
                rec["confirmed_as_mistake"] = is_mistake
                updated_count += 1

        if updated_count == 0:
            return JSONResponse({"error": "No matching items found"}, status_code=400)

        y, m, d = _extract_ymd_from_key(s3_key)
        base = _basename_from_key(s3_key)

        if is_mistake:
            # Confirmed as MISTAKE - keep in flagged with confirmed status
            _write_jsonl(FLAGGED_REVIEW_PREFIX, y, m, d, base.replace('.jsonl', ''), all_recs)
            print(f"[CONFIRM FLAGGED] Marked {updated_count} items as confirmed mistakes")
        else:
            # Confirmed as NOT a mistake - move back to billback
            # Separate confirmed items from remaining
            confirmed_items = []
            remaining_items = []
            for rec in all_recs:
                if confirm_all or _compute_stable_line_hash(rec) in line_hashes_to_confirm:
                    # Clean up flagging metadata before moving back
                    rec.pop("flagged_date", None)
                    rec.pop("flagged_by", None)
                    rec.pop("flagged_reason", None)
                    rec.pop("flagged_note", None)
                    rec.pop("original_submitter", None)
                    rec.pop("source_s3_key", None)
                    # Remove confirmed_* fields too since we're returning to normal flow
                    rec.pop("confirmed_date", None)
                    rec.pop("confirmed_by", None)
                    rec.pop("confirmed_as_mistake", None)
                    rec["unflagged_date"] = now_utc
                    rec["unflagged_by"] = user
                    rec["unflagged_reason"] = "Confirmed not a mistake"
                    confirmed_items.append(rec)
                else:
                    remaining_items.append(rec)

            # Write confirmed items to Stage 7 (billback) first
            if confirmed_items:
                _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), confirmed_items)
                print(f"[CONFIRM FLAGGED] Moved {len(confirmed_items)} NOT-MISTAKE items back to Stage 7")

            # Then update Stage 9: rewrite with remaining or delete if empty
            if remaining_items:
                _write_jsonl(FLAGGED_REVIEW_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining_items)
            else:
                s3.delete_object(Bucket=BUCKET, Key=s3_key)
                print(f"[CONFIRM FLAGGED] Deleted empty flagged file {s3_key}")

        print(f"[CONFIRM FLAGGED] Updated {updated_count} items")
        return {"ok": True, "confirmed": updated_count, "is_mistake": is_mistake}

    except Exception as e:
        print(f"[CONFIRM FLAGGED] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/flagged/stats")
def api_flagged_stats(user: str = Depends(require_user), days_back: int = 90):
    """Get quality metrics - flagged items stats by submitter over time."""
    try:
        from datetime import datetime, timedelta
        from collections import defaultdict
        from concurrent.futures import ThreadPoolExecutor, as_completed

        if user not in ADMIN_USERS:
            return JSONResponse({"error": "Admin access required"}, status_code=403)

        today = datetime.utcnow()

        def fetch_day_stats(d):
            """Fetch stats for a single day, returns list of (submitter, week_key, amount) tuples."""
            results = []
            week_key = f"{d.year}-W{d.isocalendar()[1]:02d}"
            prefix = f"{FLAGGED_REVIEW_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if not key.endswith('.jsonl'):
                            continue
                        try:
                            body = _read_s3_text(BUCKET, key)
                            for line in body.splitlines():
                                line = (line or '').strip()
                                if not line:
                                    continue
                                rec = json.loads(line)
                                submitter = rec.get('original_submitter', rec.get('Submitter', 'Unknown'))
                                amt = rec.get("Line Item Charge") or rec.get("AMOUNT") or 0
                                try:
                                    amt_val = float(str(amt).replace('$', '').replace(',', ''))
                                except (ValueError, TypeError):
                                    amt_val = 0.0
                                results.append((submitter, week_key, amt_val))
                        except Exception:
                            pass
            except Exception:
                pass
            return results

        # Parallel fetch all days
        dates_to_scan = [today - timedelta(days=i) for i in range(days_back)]
        all_results = []

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(fetch_day_stats, d): d for d in dates_to_scan}
            for future in as_completed(futures):
                try:
                    results = future.result()
                    all_results.extend(results)
                except Exception as e:
                    print(f"[FLAGGED STATS] Worker error: {e}")

        # Aggregate stats
        stats_by_submitter = defaultdict(lambda: {"total_flagged": 0, "total_dollars": 0.0, "by_week": defaultdict(int)})
        for submitter, week_key, amt_val in all_results:
            stats_by_submitter[submitter]["total_flagged"] += 1
            stats_by_submitter[submitter]["by_week"][week_key] += 1
            stats_by_submitter[submitter]["total_dollars"] += amt_val

        # Format response
        submitter_stats = []
        for submitter, stats in sorted(stats_by_submitter.items(), key=lambda x: -x[1]["total_flagged"]):
            submitter_stats.append({
                "submitter": submitter,
                "total_flagged": stats["total_flagged"],
                "total_dollars": stats["total_dollars"],
                "by_week": dict(stats["by_week"])
            })

        return {
            "ok": True,
            "stats": submitter_stats
        }

    except Exception as e:
        print(f"[FLAGGED STATS] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/ubi/stats/by_property")
def api_ubi_stats_by_property(
    user: str = Depends(require_user),
    days_back: int = 90
):
    """Get count of unassigned invoices grouped by property (not line items, invoices).
    Returns property names sorted by invoice count descending.
    """
    try:
        from datetime import datetime, timedelta
        from concurrent.futures import ThreadPoolExecutor, as_completed
        from collections import defaultdict

        start_time = datetime.now()

        # Use cached exclusion hashes
        excluded_hashes = _get_cached_exclusion_hashes(days_back)
        print(f"[UBI STATS BY PROPERTY] Using {len(excluded_hashes)} cached exclusion hashes")

        # Build date-partitioned prefixes
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(days_back):
            d = today - timedelta(days=i)
            prefix = f"{POST_ENTRATA_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        # Collect all S3 keys
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                s3_pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)
                for s3_page in s3_pages:
                    for obj in s3_page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception:
                continue

        print(f"[UBI STATS BY PROPERTY] Found {len(all_keys)} JSONL files to process")

        # Process a single S3 file - return property name if it has unassigned lines
        def get_property_if_unassigned(key):
            try:
                obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]

                if not lines:
                    return None

                try:
                    first_rec = json.loads(lines[0])
                except json.JSONDecodeError:
                    return None

                property_name = (
                    first_rec.get("EnrichedPropertyName") or
                    first_rec.get("Property Name") or
                    "Unknown Property"
                ).strip()

                # Check if any line is unassigned
                for line in lines:
                    try:
                        rec = json.loads(line)
                        _lh = _compute_stable_line_hash(rec)
                        if _lh not in excluded_hashes:
                            # This invoice has at least one unassigned line
                            return property_name
                    except (json.JSONDecodeError, ValueError, TypeError):
                        continue

                return None
            except Exception as e:
                print(f"[UBI STATS BY PROPERTY] Error processing {key}: {e}")
                return None

        # Process files concurrently
        property_counts = defaultdict(int)
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(get_property_if_unassigned, key): key for key in all_keys}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    property_counts[result] += 1

        # Sort by count descending
        sorted_stats = sorted(
            [{"property": prop, "count": count} for prop, count in property_counts.items()],
            key=lambda x: x["count"],
            reverse=True
        )

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"[UBI STATS BY PROPERTY] Found {len(sorted_stats)} properties with unassigned invoices in {elapsed:.1f}s")

        return {
            "stats": sorted_stats,
            "total_properties": len(sorted_stats),
            "total_invoices": sum(s["count"] for s in sorted_stats),
            "processing_time_seconds": round(elapsed, 1)
        }

    except Exception as e:
        print(f"[UBI STATS BY PROPERTY] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/billback/charts", response_class=HTMLResponse)
def billback_charts_view(request: Request, user: str = Depends(require_user)):
    """Show charts for UBI billing analytics."""
    user_data = auth.get_user(user)
    user_role = user_data.get("role") if user_data else None
    return templates.TemplateResponse("billback_chart.html", {
        "request": request,
        "user": user,
        "user_role": user_role
    })


@app.get("/config", response_class=HTMLResponse)
def config_menu_view(request: Request, user: str = Depends(require_user)):
    """Config page - admin only."""
    if user not in ADMIN_USERS:
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error": "You don't have permission to access this page. Admin access required."
        }, status_code=403)
    user_data = auth.get_user(user)
    user_role = user_data.get("role") if user_data else None
    return templates.TemplateResponse("config_menu.html", {
        "request": request,
        "user": user,
        "user_role": user_role
    })


@app.get("/config/gl-code-mapping", response_class=HTMLResponse)
def config_gl_code_mapping_view(request: Request, user: str = Depends(require_user)):
    """GL Code to Charge Code Mapping (Property-Aware) - for automatic charge code lookup during billback."""
    return templates.TemplateResponse("gl_code_mapping.html", {"request": request, "user": user})


@app.get("/config/account-tracking", response_class=HTMLResponse)
def config_account_tracking_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("config.html", {"request": request, "user": user})


@app.get("/config/ap-team", response_class=HTMLResponse)
def config_ap_team_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("ap_team.html", {"request": request, "user": user})


@app.get("/config/ap-mapping", response_class=HTMLResponse)
def config_ap_mapping_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("ap_mapping.html", {"request": request, "user": user})


@app.get("/config/ubi-mapping", response_class=HTMLResponse)
def config_ubi_mapping_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("ubi_mapping.html", {"request": request, "user": user})


@app.get("/config/charge-codes", response_class=HTMLResponse)
def config_charge_codes_view(request: Request, user: str = Depends(require_user)):
    """Charge codes configuration page - manage charge codes and utility names."""
    return templates.TemplateResponse("charge_codes.html", {"request": request, "user": user})


@app.get("/config/post-validation", response_class=HTMLResponse)
def config_post_validation_view(request: Request, user: str = Depends(require_user)):
    """Post validation overrides - vendor-property and vendor-GL overrides."""
    return templates.TemplateResponse("post_validation_config.html", {"request": request, "user": user})


@app.get("/api/config/charge-codes")
def api_get_charge_codes(user: str = Depends(require_user)):
    """Get charge codes configuration."""
    arr = _ddb_get_config("charge-codes")
    if not isinstance(arr, list):
        arr = []
    # normalize to {chargeCode, utilityName}
    out = []
    for r in arr:
        if isinstance(r, dict):
            out.append({
                "chargeCode": str(r.get("chargeCode") or "").strip(),
                "utilityName": str(r.get("utilityName") or "").strip()
            })
    return {"items": out}


@app.post("/api/config/charge-codes")
async def api_save_charge_codes(request: Request, user: str = Depends(require_user)):
    """Save charge codes configuration."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    norm = []
    for r in items:
        if not isinstance(r, dict):
            continue
        charge_code = str(r.get("chargeCode") or "").strip()
        utility_name = str(r.get("utilityName") or "").strip()
        if charge_code:  # Only save if charge code is not empty
            norm.append({
                "chargeCode": charge_code,
                "utilityName": utility_name
            })
    ok = _ddb_put_config("charge-codes", norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True, "saved": len(norm)}


@app.get("/config/uom-mapping", response_class=HTMLResponse)
def config_uom_mapping_view(request: Request, user: str = Depends(require_user)):
    """UOM (Unit of Measure) conversion mapping configuration page."""
    return templates.TemplateResponse("uom_mapping.html", {"request": request, "user": user})


# -------- WORKFLOW Feature --------
# Helper functions for workflow S3 storage
def _s3_get_workflow_reasons() -> list:
    """Load workflow reason codes from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=WORKFLOW_REASONS_KEY)
        data = json.loads(obj["Body"].read().decode("utf-8"))
        return data if isinstance(data, list) else []
    except s3.exceptions.NoSuchKey:
        return []
    except Exception as e:
        print(f"[_s3_get_workflow_reasons] Error: {e}")
        return []


def _s3_put_workflow_reasons(reasons: list) -> bool:
    """Save workflow reason codes to S3."""
    try:
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=WORKFLOW_REASONS_KEY,
            Body=json.dumps(reasons, indent=2).encode("utf-8"),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[_s3_put_workflow_reasons] Error: {e}")
        return False


def _s3_get_workflow_notes() -> list:
    """Load workflow notes from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=WORKFLOW_NOTES_KEY)
        data = json.loads(obj["Body"].read().decode("utf-8"))
        return data.get("notes", []) if isinstance(data, dict) else data if isinstance(data, list) else []
    except s3.exceptions.NoSuchKey:
        return []
    except Exception as e:
        print(f"[_s3_get_workflow_notes] Error: {e}")
        return []


def _s3_put_workflow_notes(notes: list) -> bool:
    """Save workflow notes to S3."""
    try:
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=WORKFLOW_NOTES_KEY,
            Body=json.dumps({"notes": notes}, indent=2).encode("utf-8"),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[_s3_put_workflow_notes] Error: {e}")
        return False


# Workflow Reason Codes Config
@app.get("/config/workflow-reasons", response_class=HTMLResponse)
def config_workflow_reasons_view(request: Request, user: str = Depends(require_user)):
    """Workflow reason codes configuration page."""
    return templates.TemplateResponse("workflow_reasons.html", {"request": request, "user": user})


@app.get("/api/config/workflow-reasons")
def api_get_workflow_reasons(user: str = Depends(require_user)):
    """Get workflow reason codes."""
    reasons = _s3_get_workflow_reasons()
    return {"items": reasons}


@app.post("/api/config/workflow-reasons")
async def api_save_workflow_reasons(request: Request, user: str = Depends(require_user)):
    """Save workflow reason codes."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    # Normalize reason codes
    norm = []
    for r in items:
        if isinstance(r, dict):
            code = str(r.get("code") or "").strip()
            label = str(r.get("label") or "").strip()
            if code:
                norm.append({"code": code, "label": label or code})
        elif isinstance(r, str) and r.strip():
            norm.append({"code": r.strip(), "label": r.strip()})
    ok = _s3_put_workflow_reasons(norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True, "saved": len(norm)}


# Workflow Notes API
@app.get("/api/workflow/notes")
def api_get_workflow_notes(user: str = Depends(require_user)):
    """Get all workflow notes."""
    notes = _s3_get_workflow_notes()
    return {"items": notes}


@app.post("/api/workflow/notes")
async def api_save_workflow_note(request: Request, user: str = Depends(require_user)):
    """Save or update a workflow note for an account."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)

    account_key = str(payload.get("accountKey") or "").strip()
    reason_code = str(payload.get("reasonCode") or "").strip()
    custom_note = str(payload.get("customNote") or "").strip()

    if not account_key:
        return JSONResponse({"error": "accountKey required"}, status_code=400)

    notes = _s3_get_workflow_notes()
    now_utc = dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

    # Find existing note for this account
    existing_idx = None
    for i, n in enumerate(notes):
        if n.get("accountKey") == account_key:
            existing_idx = i
            break

    if reason_code or custom_note:
        # Add or update note
        note_obj = {
            "accountKey": account_key,
            "reasonCode": reason_code,
            "customNote": custom_note,
            "updatedBy": user,
            "updatedAt": now_utc
        }
        if existing_idx is not None:
            # Preserve original creator
            note_obj["createdBy"] = notes[existing_idx].get("createdBy", user)
            note_obj["createdAt"] = notes[existing_idx].get("createdAt", now_utc)
            notes[existing_idx] = note_obj
        else:
            note_obj["createdBy"] = user
            note_obj["createdAt"] = now_utc
            notes.append(note_obj)
    else:
        # Clear note (no reason code and no custom note)
        if existing_idx is not None:
            notes.pop(existing_idx)

    ok = _s3_put_workflow_notes(notes)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True}


@app.post("/api/workflow/notes/bulk")
async def api_bulk_save_workflow_notes(request: Request, user: str = Depends(require_user)):
    """Bulk-set reason code and note for multiple accounts."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)

    account_keys = payload.get("accountKeys", [])
    reason_code = str(payload.get("reasonCode") or "").strip()
    custom_note = str(payload.get("customNote") or "").strip()

    if not account_keys:
        return JSONResponse({"error": "accountKeys required"}, status_code=400)

    notes = _s3_get_workflow_notes()
    now_utc = dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

    # Build index for fast lookup
    notes_by_key = {n["accountKey"]: (i, n) for i, n in enumerate(notes)}
    updated_count = 0

    for akey in account_keys:
        akey = str(akey).strip()
        if not akey:
            continue

        if reason_code or custom_note:
            note_obj = {
                "accountKey": akey,
                "reasonCode": reason_code,
                "customNote": custom_note,
                "updatedBy": user,
                "updatedAt": now_utc,
            }
            if akey in notes_by_key:
                idx, existing = notes_by_key[akey]
                note_obj["createdBy"] = existing.get("createdBy", user)
                note_obj["createdAt"] = existing.get("createdAt", now_utc)
                notes[idx] = note_obj
            else:
                note_obj["createdBy"] = user
                note_obj["createdAt"] = now_utc
                notes.append(note_obj)
                notes_by_key[akey] = (len(notes) - 1, note_obj)
            updated_count += 1
        else:
            # Clear note
            if akey in notes_by_key:
                idx, _ = notes_by_key[akey]
                notes[idx] = None  # Mark for removal
                updated_count += 1

    # Remove None entries (cleared notes)
    notes = [n for n in notes if n is not None]

    ok = _s3_put_workflow_notes(notes)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    print(f"[BULK_NOTES] {user} bulk-set reason '{reason_code}' for {updated_count} accounts")
    return {"ok": True, "updatedCount": updated_count}


# Workflow Page Routes
@app.get("/workflow", response_class=HTMLResponse)
def workflow_view(request: Request, user: str = Depends(require_user)):
    """Workflow page - prioritized account status view."""
    is_admin = user in ADMIN_USERS
    return templates.TemplateResponse("workflow.html", {"request": request, "user": user, "is_admin": is_admin})


@app.get("/workflow/manager", response_class=HTMLResponse)
def workflow_manager_view(request: Request, user: str = Depends(require_user)):
    """Workflow manager view - aggregated notes by reason code."""
    return templates.TemplateResponse("workflow_manager.html", {"request": request, "user": user})


@app.get("/workflow/manage", response_class=HTMLResponse)
def workflow_manage_view(request: Request, user: str = Depends(require_user)):
    """Account management page - admin only."""
    if user not in ADMIN_USERS:
        return templates.TemplateResponse("error.html", {
            "request": request,
            "user": user,
            "error": "Admin access required"
        }, status_code=403)
    return templates.TemplateResponse("workflow_manage.html", {"request": request, "user": user})


@app.get("/directed", response_class=HTMLResponse)
def directed_view(request: Request, user: str = Depends(require_user)):
    """Directed Workflow - Daily work plan generator."""
    return templates.TemplateResponse("directed.html", {"request": request, "user": user})


@app.get("/api/workflow/vacant-accounts")
def api_vacant_accounts(
    min_vacant_pct: int = 50,
    user: str = Depends(require_user)
):
    """Get accounts with high percentage of VACANT line items."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        print(f"[VACANT DETECT] Scanning for accounts with {min_vacant_pct}%+ vacant lines...")

        # Load tracked accounts
        accounts = _get_accounts_to_track()
        account_keys = {}
        for a in accounts:
            key = f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}"
            if a.get('status') == 'archived':
                continue  # Skip already archived
            if not a.get('is_tracked', True):
                continue  # Skip removed accounts
            account_keys[key] = a

        # Scan recent bills to count vacant vs house
        today = dt.date.today()
        prefixes = []
        for i in range(90):  # Last 90 days
            d = today - dt.timedelta(days=i)
            y, m, day = d.strftime('%Y'), d.strftime('%m'), d.strftime('%d')
            prefixes.append(f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={day}/")
            prefixes.append(f"{HIST_ARCHIVE_PREFIX}yyyy={y}/mm={m}/dd={day}/")

        # Track vacant/house counts per account
        account_stats = {}  # key -> {vacant: int, house: int, total_bills: int, invoices: set}

        paginator = s3.get_paginator('list_objects_v2')
        for prefix in prefixes[:60]:  # Limit to ~30 days to keep it fast
            try:
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get("Contents", []) or []:
                        key = obj.get("Key", "")
                        if not key.endswith('.jsonl'):
                            continue

                        try:
                            file_obj = s3.get_object(Bucket=BUCKET, Key=key)
                            content = file_obj['Body'].read().decode('utf-8', errors='ignore')

                            first_line = content.split('\n')[0].strip()
                            if not first_line:
                                continue
                            rec = json.loads(first_line)

                            rec_acct = str(rec.get("Account Number", "") or rec.get("Line Item Account Number", "")).strip()
                            rec_prop = str(rec.get("EnrichedPropertyID", "")).strip()
                            rec_vendor = str(rec.get("EnrichedVendorID", "")).strip()
                            account_key = f"{rec_prop}|{rec_vendor}|{rec_acct}"

                            # Only check tracked accounts
                            if account_key not in account_keys:
                                continue

                            invoice_num = rec.get("Invoice Number", "")

                            if account_key not in account_stats:
                                account_stats[account_key] = {
                                    "vacant": 0,
                                    "house": 0,
                                    "total_bills": 0,
                                    "invoices": set(),
                                    "property_name": rec.get("EnrichedPropertyName", ""),
                                    "vendor_name": rec.get("EnrichedVendorName", ""),
                                    "account_number": rec_acct
                                }

                            if invoice_num not in account_stats[account_key]["invoices"]:
                                account_stats[account_key]["invoices"].add(invoice_num)
                                account_stats[account_key]["total_bills"] += 1

                            # Count line items
                            for line in content.strip().split('\n'):
                                if not line.strip():
                                    continue
                                try:
                                    line_rec = json.loads(line)
                                    hov = str(line_rec.get("House Or Vacant", "")).lower().strip()
                                    if hov == "vacant":
                                        account_stats[account_key]["vacant"] += 1
                                    elif hov == "house":
                                        account_stats[account_key]["house"] += 1
                                except json.JSONDecodeError:
                                    continue

                        except Exception as e:
                            continue
            except Exception as e:
                continue

        # Calculate percentages and filter
        vacant_accounts = []
        for key, stats in account_stats.items():
            total_lines = stats["vacant"] + stats["house"]
            if total_lines == 0:
                continue

            vacant_pct = round(stats["vacant"] / total_lines * 100, 1)
            if vacant_pct < min_vacant_pct:
                continue

            # Get additional info from accounts_to_track
            acct_info = account_keys.get(key, {})

            vacant_accounts.append({
                "account_key": key,
                "property_id": acct_info.get("propertyId", ""),
                "vendor_id": acct_info.get("vendorId", ""),
                "property_name": stats["property_name"] or acct_info.get("propertyName", ""),
                "vendor_name": stats["vendor_name"] or acct_info.get("vendorName", ""),
                "account_number": stats["account_number"],
                "vacant_lines": stats["vacant"],
                "house_lines": stats["house"],
                "total_bills": stats["total_bills"],
                "vacant_pct": vacant_pct,
                "is_ubi": acct_info.get("is_ubi", False)
            })

        # Sort by vacant percentage descending
        vacant_accounts.sort(key=lambda x: x["vacant_pct"], reverse=True)

        print(f"[VACANT DETECT] Found {len(vacant_accounts)} accounts with {min_vacant_pct}%+ vacant")

        return {
            "accounts": vacant_accounts,
            "total": len(vacant_accounts),
            "threshold": min_vacant_pct
        }
    except Exception as e:
        import traceback
        print(f"[VACANT DETECT] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/workflow/accounts/archive")
async def api_archive_accounts(request: Request, user: str = Depends(require_user)):
    """Archive (soft-delete) accounts from tracking. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        body = await request.json()
        account_keys = body.get("account_keys", [])
        reason = body.get("reason", "vacant_account")

        if not account_keys:
            return JSONResponse({"error": "No accounts specified"}, status_code=400)

        # Load accounts
        accounts = _get_accounts_to_track()

        # Build key index
        key_to_idx = {}
        for i, a in enumerate(accounts):
            key = f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}"
            key_to_idx[key] = i

        archived_count = 0
        for key in account_keys:
            if key in key_to_idx:
                idx = key_to_idx[key]
                accounts[idx]["status"] = "archived"
                accounts[idx]["archived_at"] = dt.datetime.utcnow().isoformat() + "Z"
                accounts[idx]["archived_by"] = user
                accounts[idx]["archive_reason"] = reason
                archived_count += 1

        # Save
        if archived_count > 0:
            _put_accounts_to_track(accounts)
            print(f"[ARCHIVE] {user} archived {archived_count} accounts (reason: {reason})")

        return {"archived": archived_count, "total_requested": len(account_keys)}
    except Exception as e:
        import traceback
        print(f"[ARCHIVE] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/workflow/accounts/restore")
async def api_restore_accounts(request: Request, user: str = Depends(require_user)):
    """Restore archived accounts. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        body = await request.json()
        account_keys = body.get("account_keys", [])

        if not account_keys:
            return JSONResponse({"error": "No accounts specified"}, status_code=400)

        accounts = _get_accounts_to_track()

        key_to_idx = {}
        for i, a in enumerate(accounts):
            key = f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}"
            key_to_idx[key] = i

        restored_count = 0
        for key in account_keys:
            if key in key_to_idx:
                idx = key_to_idx[key]
                if accounts[idx].get("status") == "archived":
                    accounts[idx]["status"] = "active"
                    accounts[idx]["restored_at"] = dt.datetime.utcnow().isoformat() + "Z"
                    accounts[idx]["restored_by"] = user
                    restored_count += 1

        if restored_count > 0:
            _put_accounts_to_track(accounts)
            print(f"[RESTORE] {user} restored {restored_count} accounts")

        return {"restored": restored_count, "total_requested": len(account_keys)}
    except Exception as e:
        import traceback
        print(f"[RESTORE] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/workflow/accounts/archived")
def api_archived_accounts(user: str = Depends(require_user)):
    """List archived accounts. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        accounts = _get_accounts_to_track()
        archived = [a for a in accounts if a.get("status") == "archived"]

        # Format for display
        result = []
        for a in archived:
            result.append({
                "account_key": f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}",
                "property_name": a.get("propertyName", ""),
                "vendor_name": a.get("vendorName", ""),
                "account_number": a.get("accountNumber", ""),
                "archived_at": a.get("archived_at", ""),
                "archived_by": a.get("archived_by", ""),
                "archive_reason": a.get("archive_reason", "")
            })

        return {"accounts": result, "total": len(result)}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/workflow/accounts/update")
async def api_update_account_mapping(request: Request, user: str = Depends(require_user)):
    """Update the property/vendor mapping for a tracked account."""
    try:
        body = await request.json()
        account_key = body.get("accountKey", "")
        new_property_id = body.get("newPropertyId", "")
        new_property_name = body.get("newPropertyName", "")
        new_vendor_id = body.get("newVendorId", "")
        new_vendor_name = body.get("newVendorName", "")

        if not account_key:
            return JSONResponse({"detail": "Missing accountKey"}, status_code=400)
        if not new_property_id or not new_vendor_id:
            return JSONResponse({"detail": "Missing property or vendor ID"}, status_code=400)

        # Parse account key (format: propertyId|vendorId|accountNumber)
        parts = account_key.split("|")
        if len(parts) != 3:
            return JSONResponse({"detail": "Invalid accountKey format"}, status_code=400)

        old_pid, old_vid, acct_num = parts

        accounts = _get_accounts_to_track()

        # Find and update the matching account
        updated = False
        for a in accounts:
            if (str(a.get("propertyId")) == old_pid and
                str(a.get("vendorId")) == old_vid and
                str(a.get("accountNumber")) == acct_num):

                # Update mapping
                a["propertyId"] = new_property_id
                a["propertyName"] = new_property_name
                a["vendorId"] = new_vendor_id
                a["vendorName"] = new_vendor_name
                a["updated_at"] = dt.datetime.utcnow().isoformat() + "Z"
                a["updated_by"] = user
                updated = True
                print(f"[UPDATE_ACCOUNT] {user} updated account {acct_num}: "
                      f"property {old_pid}->>{new_property_id}, vendor {old_vid}->>{new_vendor_id}")
                break

        if not updated:
            return JSONResponse({"detail": "Account not found"}, status_code=404)

        _put_accounts_to_track(accounts)

        return {"success": True, "message": "Account mapping updated"}
    except Exception as e:
        import traceback
        print(f"[UPDATE_ACCOUNT] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"detail": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/workflow/accounts/bulk-update")
async def api_bulk_update_account_mapping(request: Request, user: str = Depends(require_user)):
    """Bulk-update vendor (and optionally property) for multiple tracked accounts."""
    try:
        body = await request.json()
        account_keys = body.get("accountKeys", [])
        new_vendor_id = body.get("newVendorId", "")
        new_vendor_name = body.get("newVendorName", "")
        new_property_id = body.get("newPropertyId", "")
        new_property_name = body.get("newPropertyName", "")

        if not account_keys or not new_vendor_id:
            return JSONResponse({"detail": "Missing accountKeys or newVendorId"}, status_code=400)

        accounts = _get_accounts_to_track()
        now_utc = dt.datetime.utcnow().isoformat() + "Z"

        # Build lookup set for fast matching
        key_set = set(account_keys)
        updated_count = 0
        for a in accounts:
            akey = f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}"
            if akey in key_set:
                a["vendorId"] = new_vendor_id
                a["vendorName"] = new_vendor_name
                if new_property_id:
                    a["propertyId"] = new_property_id
                    a["propertyName"] = new_property_name
                a["updated_at"] = now_utc
                a["updated_by"] = user
                updated_count += 1

        if updated_count == 0:
            return JSONResponse({"detail": "No matching accounts found"}, status_code=404)

        _put_accounts_to_track(accounts)
        print(f"[BULK_UPDATE] {user} bulk-updated vendor to {new_vendor_id} for {updated_count} accounts")
        return {"success": True, "updatedCount": updated_count}
    except Exception as e:
        import traceback
        print(f"[BULK_UPDATE] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"detail": _sanitize_error(e, "request")}, status_code=500)


# -------- Vendor Correction Admin APIs --------

@app.get("/vendor-corrections", response_class=HTMLResponse)
def vendor_corrections_view(request: Request, user: str = Depends(require_user)):
    """Admin page for vendor corrections. Admin only."""
    if user not in ADMIN_USERS:
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error": "You don't have permission to access this page. Admin access required."
        }, status_code=403)
    return templates.TemplateResponse("vendor_corrections.html", {"request": request, "user": user})


def _safe_str(val) -> str:
    """Convert value to JSON-safe string, handling None and special chars."""
    if val is None:
        return ""
    s = str(val)
    # Remove any characters that could break JSON
    return s.replace('\x00', '').replace('\r', '').strip()


@app.get("/api/vendor-corrections/suspects")
def api_vendor_correction_suspects(user: str = Depends(require_user), days: int = 30):
    """Find property+account combos in accounts_to_track that exist under multiple vendors. Admin only.

    Args:
        days: Number of days to scan for submissions (default 30, max 90)
    """
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    # Clamp days to reasonable range
    scan_days = max(7, min(90, days))

    try:
        from collections import defaultdict
        from datetime import datetime, timedelta

        print(f"[VENDOR CORRECT] Starting suspects scan ({scan_days} days)...")

        accounts = _get_accounts_to_track()
        if not isinstance(accounts, list):
            print(f"[VENDOR CORRECT] ERROR: accounts is not a list: {type(accounts)}")
            return JSONResponse({"error": "Failed to load accounts data"}, status_code=500)

        active_accounts = [a for a in accounts if isinstance(a, dict) and a.get("status") != "archived"]
        print(f"[VENDOR CORRECT] Checking {len(active_accounts)} tracked accounts for duplicates")

        # Group by (property_id, account_number) -> list of entries with different vendors
        account_map = defaultdict(list)

        for acct in active_accounts:
            try:
                property_id = _safe_str(acct.get("propertyId", ""))
                property_name = _safe_str(acct.get("propertyName", ""))
                vendor_id = _safe_str(acct.get("vendorId", ""))
                vendor_name = _safe_str(acct.get("vendorName", ""))
                account_number = _safe_str(acct.get("accountNumber", ""))

                if not property_id or not account_number:
                    continue

                key = (property_id, account_number)
                account_map[key].append({
                    "property_id": property_id,
                    "property_name": property_name,
                    "vendor_id": vendor_id,
                    "vendor_name": vendor_name,
                    "account_number": account_number,
                    "is_ubi": bool(acct.get("isUBI", False)),
                    "account_key": f"{property_id}|{vendor_id}|{account_number}",
                    "added_by": _safe_str(acct.get("addedBy", "")),
                    "added_at": _safe_str(acct.get("addedAt", ""))
                })
            except Exception as e:
                print(f"[VENDOR CORRECT] Error processing account: {e}")
                continue

        # Build a lookup of recent submissions from Stage 7 AND Stage 8 (last N days)
        # Key: (vendor_id, account_number) -> {posted_by, posted_at, invoice_no, amount, service_dates, s3_key}
        print("[VENDOR CORRECT] Building recent submission lookup from Stage 7 + Stage 8...")
        submission_lookup = {}
        files_scanned = 0

        from concurrent.futures import ThreadPoolExecutor

        def process_file_for_submission(s3_key):
            """Read first line only to extract submission metadata"""
            try:
                # Read full file to ensure complete JSON line (4KB truncates PostedBy/PostedAt)
                resp = s3.get_object(Bucket=BUCKET, Key=s3_key)
                content = resp["Body"].read().decode("utf-8", errors="ignore")
                first_line = content.split("\n")[0].strip()
                if not first_line:
                    return None

                rec = json.loads(first_line)
                vid = _safe_str(rec.get("EnrichedVendorID", ""))
                acct_num = _safe_str(rec.get("Account Number", ""))
                if not vid or not acct_num:
                    return None

                posted_by = _safe_str(rec.get("PostedBy", rec.get("SubmittedBy", rec.get("AssignedBy", ""))))
                posted_at = _safe_str(rec.get("PostedAt", rec.get("SubmittedAt", rec.get("AssignedAt", ""))))
                invoice_no = _safe_str(rec.get("Invoice Number", ""))
                bill_date = _safe_str(rec.get("Bill Date", ""))
                service_start = _safe_str(rec.get("Bill Period Start", ""))
                service_end = _safe_str(rec.get("Bill Period End", ""))
                vendor_name = _safe_str(rec.get("EnrichedVendorName", rec.get("Vendor Name", "")))
                # Get amount from first line only (close enough approximation)
                charge_str = str(rec.get("Line Item Charge", "0")).replace("$", "").replace(",", "").strip()
                try:
                    amount = float(charge_str)
                except:
                    amount = 0.0

                return {
                    "key": (vid, acct_num),
                    "posted_by": posted_by,
                    "posted_at": posted_at,
                    "invoice_no": invoice_no,
                    "bill_date": bill_date,
                    "service_start": service_start,
                    "service_end": service_end,
                    "total_amount": round(amount, 2),
                    "vendor_name": vendor_name,
                    "s3_key": s3_key
                }
            except Exception:
                return None

        try:
            now = datetime.utcnow()
            all_keys = []

            # Collect keys from BOTH Stage 7 (PostEntrata) AND Stage 8 (UBI_Assigned)
            for stage_prefix in [POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX]:
                for days_ago in range(scan_days):
                    d = now - timedelta(days=days_ago)
                    prefix = f"{stage_prefix}yyyy={d.year}/mm={str(d.month).zfill(2)}/dd={str(d.day).zfill(2)}/"
                    try:
                        paginator = s3.get_paginator('list_objects_v2')
                        for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                            for obj in page.get("Contents", []):
                                if obj["Key"].endswith(".jsonl"):
                                    all_keys.append(obj["Key"])
                    except Exception:
                        continue

            print(f"[VENDOR CORRECT] Found {len(all_keys)} files to scan across Stage 7 + Stage 8")
            files_scanned = len(all_keys)

            # Process files in parallel (much faster)
            with ThreadPoolExecutor(max_workers=30) as executor:
                results = list(executor.map(process_file_for_submission, all_keys))

            # Build lookup from results
            for result in results:
                if result:
                    lookup_key = result["key"]
                    existing_posted_at = submission_lookup.get(lookup_key, {}).get("posted_at", "")
                    if lookup_key not in submission_lookup or result["posted_at"] > existing_posted_at:
                        submission_lookup[lookup_key] = {
                            "posted_by": result["posted_by"],
                            "posted_at": result["posted_at"],
                            "invoice_no": result["invoice_no"],
                            "bill_date": result["bill_date"],
                            "service_start": result["service_start"],
                            "service_end": result["service_end"],
                            "total_amount": result["total_amount"],
                            "vendor_name": result["vendor_name"],
                            "s3_key": result["s3_key"]
                        }

            print(f"[VENDOR CORRECT] Scanned {files_scanned} files, found {len(submission_lookup)} unique vendor+account submissions")
        except Exception as e:
            print(f"[VENDOR CORRECT] Error building submission lookup: {e}")
            import traceback
            traceback.print_exc()

        # Find property+account combos with multiple vendors
        suspects = []
        for (property_id, account_number), entries in account_map.items():
            try:
                # Check if there are different vendors
                vendor_ids = set(e["vendor_id"] for e in entries if e.get("vendor_id"))

                if len(vendor_ids) > 1:
                    # Same property+account tracked under different vendors
                    property_name = _safe_str(entries[0].get("property_name", "")) if entries else ""

                    # Add submission info to each entry
                    for entry in entries:
                        lookup_key = (entry.get("vendor_id", ""), account_number)
                        if lookup_key in submission_lookup:
                            sub = submission_lookup[lookup_key]
                            entry["last_posted_by"] = sub.get("posted_by", "")
                            entry["last_posted_at"] = sub.get("posted_at", "")
                            entry["last_invoice_no"] = sub.get("invoice_no", "")
                            entry["last_bill_date"] = sub.get("bill_date", "")
                            entry["last_service_start"] = sub.get("service_start", "")
                            entry["last_service_end"] = sub.get("service_end", "")
                            entry["last_total_amount"] = sub.get("total_amount", 0)
                            entry["last_s3_key"] = sub.get("s3_key", "")
                            entry["has_recent_post"] = True
                        else:
                            entry["last_posted_by"] = entry.get("added_by", "")
                            entry["last_posted_at"] = entry.get("added_at", "")
                            entry["last_invoice_no"] = ""
                            entry["last_bill_date"] = ""
                            entry["last_service_start"] = ""
                            entry["last_service_end"] = ""
                            entry["last_total_amount"] = 0
                            entry["last_s3_key"] = ""
                            entry["has_recent_post"] = False

                    suspects.append({
                        "property_id": property_id,
                        "property_name": property_name,
                        "account_number": account_number,
                        "entries": entries,
                        "vendor_count": len(vendor_ids),
                        "entry_count": len(entries)
                    })
            except Exception as e:
                print(f"[VENDOR CORRECT] Error processing suspect: {e}")
                continue

        # Sort by vendor count (most duplicates first)
        suspects.sort(key=lambda x: (-x.get("vendor_count", 0), -x.get("entry_count", 0)))

        print(f"[VENDOR CORRECT] Found {len(suspects)} property+account combos with multiple vendors")

        # Build response and ensure it's JSON serializable
        response_data = {
            "suspects": suspects,
            "total_accounts": len(active_accounts)
        }

        # Validate the response is JSON serializable before returning
        try:
            json.dumps(response_data, ensure_ascii=False)
        except (TypeError, ValueError) as json_err:
            print(f"[VENDOR CORRECT] JSON serialization error: {json_err}")
            # Return a safe fallback
            return JSONResponse({
                "suspects": [],
                "total_accounts": len(active_accounts),
                "error": "Data serialization issue, please try again"
            }, status_code=200)

        return JSONResponse(response_data, status_code=200)

    except Exception as e:
        print(f"[VENDOR CORRECT] Fatal error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": "Server error processing vendor corrections", "suspects": [], "total_accounts": 0}, status_code=500)


@app.post("/api/vendor-corrections/analyze")
async def api_vendor_correction_analyze(request: Request, user: str = Depends(require_user)):
    """Use Gemini AI to analyze vendor name similarities. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        form = await request.form()
        vendor_a = form.get("vendor_a", "").strip()
        vendor_b = form.get("vendor_b", "").strip()

        if not vendor_a or not vendor_b:
            return JSONResponse({"error": "Both vendor names required"}, status_code=400)

        result = await _compare_vendors_with_gemini(vendor_a, vendor_b)

        return {"ok": True, "comparison": result}

    except Exception as e:
        print(f"[VENDOR CORRECT] Analyze error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/vendor-corrections/apply")
async def api_vendor_correction_apply(request: Request, user: str = Depends(require_user)):
    """Apply a vendor correction to accounts_to_track. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        form = await request.form()
        account_key = form.get("account_key", "").strip()
        new_vendor_id = form.get("new_vendor_id", "").strip()
        new_vendor_name = form.get("new_vendor_name", "").strip()

        if not account_key or not new_vendor_id or not new_vendor_name:
            return JSONResponse({"error": "account_key, new_vendor_id, and new_vendor_name required"}, status_code=400)

        # Parse account key
        parts = account_key.split("|")
        if len(parts) != 3:
            return JSONResponse({"error": "Invalid account_key format"}, status_code=400)

        property_id, old_vendor_id, account_number = parts

        # Load accounts
        accounts = _get_accounts_to_track()

        # Find and update the account
        updated = False
        old_vendor_name = ""
        for acct in accounts:
            if (str(acct.get("propertyId", "")) == property_id and
                str(acct.get("vendorId", "")) == old_vendor_id and
                str(acct.get("accountNumber", "")).strip() == account_number):

                old_vendor_name = acct.get("vendorName", "")

                # Update vendor
                acct["vendorId"] = new_vendor_id
                acct["vendorName"] = new_vendor_name

                # Add correction metadata
                acct["corrected_at"] = dt.datetime.utcnow().isoformat() + "Z"
                acct["corrected_by"] = user
                acct["previous_vendor_id"] = old_vendor_id
                acct["previous_vendor_name"] = old_vendor_name

                updated = True
                break

        if not updated:
            return JSONResponse({"error": "Account not found"}, status_code=404)

        # Save accounts
        _put_accounts_to_track(accounts)

        # Log correction
        corrections_data = _s3_get_vendor_corrections()
        corrections_data["corrections"].append({
            "account_key": account_key,
            "property_id": property_id,
            "account_number": account_number,
            "old_vendor_id": old_vendor_id,
            "old_vendor_name": old_vendor_name,
            "new_vendor_id": new_vendor_id,
            "new_vendor_name": new_vendor_name,
            "corrected_by": user,
            "corrected_at": dt.datetime.utcnow().isoformat() + "Z"
        })
        _s3_put_vendor_corrections(corrections_data)

        print(f"[VENDOR CORRECT] Applied correction: {account_key} vendor changed from {old_vendor_id} to {new_vendor_id}")

        return {
            "ok": True,
            "account_key": account_key,
            "old_vendor": {"id": old_vendor_id, "name": old_vendor_name},
            "new_vendor": {"id": new_vendor_id, "name": new_vendor_name}
        }

    except Exception as e:
        print(f"[VENDOR CORRECT] Apply error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/vendor-corrections/history")
def api_vendor_correction_history(user: str = Depends(require_user)):
    """Get history of vendor corrections. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        corrections_data = _s3_get_vendor_corrections()
        corrections = corrections_data.get("corrections", [])

        # Sort by date descending
        corrections.sort(key=lambda x: x.get("corrected_at", ""), reverse=True)

        return {"corrections": corrections, "total": len(corrections)}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- Account Gap Analysis Admin APIs --------

GAP_ANALYSIS_PREFIX = os.getenv("GAP_ANALYSIS_PREFIX", CONFIG_PREFIX + "gap_analysis/")


@app.get("/account-gap-analysis", response_class=HTMLResponse)
def account_gap_analysis_view(request: Request, user: str = Depends(require_user)):
    """Admin page for account gap analysis. Admin only."""
    if user not in ADMIN_USERS:
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error": "You don't have permission to access this page. Admin access required."
        }, status_code=403)
    return templates.TemplateResponse("account_gap_analysis.html", {"request": request, "user": user})


@app.post("/api/account-gap-analysis/upload")
async def api_gap_analysis_upload(request: Request, user: str = Depends(require_user)):
    """Upload external account list for comparison. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        import csv
        import io
        import uuid

        form = await request.form()
        file = form.get("file")

        if not file:
            return JSONResponse({"error": "No file uploaded"}, status_code=400)

        content = await file.read()
        content_str = content.decode('utf-8', errors='ignore')

        # Try to parse as CSV first
        accounts = []
        try:
            reader = csv.DictReader(io.StringIO(content_str))
            for row in reader:
                # Normalize field names (case-insensitive)
                norm_row = {k.lower().strip(): v.strip() for k, v in row.items() if v}

                # Map common field names
                account = {
                    "property_id": norm_row.get("propertyid") or norm_row.get("property_id") or norm_row.get("property") or "",
                    "property_name": norm_row.get("propertyname") or norm_row.get("property_name") or norm_row.get("property") or "",
                    "vendor_id": norm_row.get("vendorid") or norm_row.get("vendor_id") or "",
                    "vendor_name": norm_row.get("vendorname") or norm_row.get("vendor_name") or norm_row.get("vendor") or "",
                    "account_number": norm_row.get("accountnumber") or norm_row.get("account_number") or norm_row.get("account") or norm_row.get("acct") or "",
                }

                if account["account_number"]:
                    accounts.append(account)

        except Exception as csv_err:
            # Try JSON
            try:
                json_data = json.loads(content_str)
                if isinstance(json_data, list):
                    for item in json_data:
                        account = {
                            "property_id": str(item.get("propertyId") or item.get("property_id") or ""),
                            "property_name": str(item.get("propertyName") or item.get("property_name") or item.get("property") or ""),
                            "vendor_id": str(item.get("vendorId") or item.get("vendor_id") or ""),
                            "vendor_name": str(item.get("vendorName") or item.get("vendor_name") or item.get("vendor") or ""),
                            "account_number": str(item.get("accountNumber") or item.get("account_number") or item.get("account") or ""),
                        }
                        if account["account_number"]:
                            accounts.append(account)
            except Exception:
                return JSONResponse({"error": "Could not parse file as CSV or JSON"}, status_code=400)

        if not accounts:
            return JSONResponse({"error": "No valid accounts found in file"}, status_code=400)

        # Generate upload ID and save to S3
        upload_id = str(uuid.uuid4())[:8]
        upload_key = f"{GAP_ANALYSIS_PREFIX}uploads/{upload_id}.json"

        upload_data = {
            "upload_id": upload_id,
            "uploaded_by": user,
            "uploaded_at": dt.datetime.utcnow().isoformat() + "Z",
            "filename": file.filename,
            "accounts": accounts
        }

        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=upload_key,
            Body=json.dumps(upload_data, indent=2),
            ContentType="application/json"
        )

        print(f"[GAP ANALYSIS] Uploaded {len(accounts)} accounts as {upload_id}")

        return {
            "ok": True,
            "upload_id": upload_id,
            "account_count": len(accounts),
            "sample": accounts[:5]
        }

    except Exception as e:
        print(f"[GAP ANALYSIS] Upload error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


def _normalize_account_number(acct_num: str) -> str:
    """Normalize account number for fuzzy matching - strip dashes, spaces, leading zeros."""
    if not acct_num:
        return ""
    # Remove common separators and whitespace
    normalized = re.sub(r'[-\s\.\(\)]', '', str(acct_num).strip())
    # Strip leading zeros but keep at least one digit
    normalized = normalized.lstrip('0') or '0'
    return normalized.upper()


def _account_similarity(a: str, b: str) -> float:
    """Calculate similarity between two account numbers (0.0 to 1.0)."""
    if not a or not b:
        return 0.0
    if a == b:
        return 1.0
    # Check if one contains the other
    if a in b or b in a:
        return 0.9
    # Simple character-based similarity
    from difflib import SequenceMatcher
    return SequenceMatcher(None, a, b).ratio()


@app.post("/api/account-gap-analysis/run")
async def api_gap_analysis_run(request: Request, user: str = Depends(require_user)):
    """Run gap analysis comparing uploaded accounts to accounts_to_track. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        form = await request.form()
        upload_id = form.get("upload_id", "").strip()

        if not upload_id:
            return JSONResponse({"error": "upload_id required"}, status_code=400)

        # Load uploaded accounts
        upload_key = f"{GAP_ANALYSIS_PREFIX}uploads/{upload_id}.json"
        try:
            obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=upload_key)
            upload_data = json.loads(obj['Body'].read().decode('utf-8'))
        except Exception:
            return JSONResponse({"error": "Upload not found"}, status_code=404)

        uploaded_accounts = upload_data.get("accounts", [])

        # Load existing tracked accounts
        tracked_accounts = _get_accounts_to_track()
        active_tracked = [a for a in tracked_accounts if a.get("status") != "archived" and a.get("is_tracked", True)]

        # Build lookup indexes - both exact and normalized
        tracked_by_exact = {}
        tracked_by_prop_acct = {}
        tracked_by_acct = {}
        tracked_by_normalized = {}  # normalized account -> list of accounts
        tracked_by_prop_normalized = {}  # (prop_id, normalized_acct) -> list of accounts

        for acct in active_tracked:
            prop_id = str(acct.get("propertyId", "")).strip()
            vendor_id = str(acct.get("vendorId", "")).strip()
            acct_num = str(acct.get("accountNumber", "")).strip()
            normalized_acct = _normalize_account_number(acct_num)

            exact_key = f"{prop_id}|{vendor_id}|{acct_num}"
            prop_acct_key = f"{prop_id}|{acct_num}"
            prop_norm_key = f"{prop_id}|{normalized_acct}"

            tracked_by_exact[exact_key] = acct

            if prop_acct_key not in tracked_by_prop_acct:
                tracked_by_prop_acct[prop_acct_key] = []
            tracked_by_prop_acct[prop_acct_key].append(acct)

            if acct_num not in tracked_by_acct:
                tracked_by_acct[acct_num] = []
            tracked_by_acct[acct_num].append(acct)

            if normalized_acct not in tracked_by_normalized:
                tracked_by_normalized[normalized_acct] = []
            tracked_by_normalized[normalized_acct].append(acct)

            if prop_norm_key not in tracked_by_prop_normalized:
                tracked_by_prop_normalized[prop_norm_key] = []
            tracked_by_prop_normalized[prop_norm_key].append(acct)

        matched_tracked_keys = set()

        results = {
            "matched_exact": [],
            "matched_normalized": [],  # NEW: matches after normalizing account number
            "matched_vendor_diff": [],
            "matched_fuzzy": [],
            "not_found": [],
            "orphaned": []
        }

        for uploaded in uploaded_accounts:
            up_prop_id = str(uploaded.get("property_id", "")).strip()
            up_vendor_id = str(uploaded.get("vendor_id", "")).strip()
            up_acct_num = str(uploaded.get("account_number", "")).strip()
            up_prop_name = uploaded.get("property_name", "")
            up_vendor_name = uploaded.get("vendor_name", "")
            up_normalized = _normalize_account_number(up_acct_num)

            exact_key = f"{up_prop_id}|{up_vendor_id}|{up_acct_num}"
            prop_acct_key = f"{up_prop_id}|{up_acct_num}"
            prop_norm_key = f"{up_prop_id}|{up_normalized}"

            record = {
                "uploaded": uploaded,
                "tracked": None,
                "status": "not_found",
                "notes": ""
            }

            # 1. Exact match (property + vendor + account)
            if exact_key in tracked_by_exact:
                record["tracked"] = tracked_by_exact[exact_key]
                record["status"] = "matched_exact"
                record["notes"] = "Exact match"
                matched_tracked_keys.add(exact_key)
                results["matched_exact"].append(record)
                continue

            # 2. Normalized match (property + normalized account)
            if prop_norm_key in tracked_by_prop_normalized:
                matches = tracked_by_prop_normalized[prop_norm_key]
                record["tracked"] = matches[0]
                record["possible_matches"] = matches
                tracked_acct_num = matches[0].get("accountNumber", "")
                record["status"] = "matched_normalized"
                record["notes"] = f"Normalized match: '{up_acct_num}' -> '{tracked_acct_num}'"
                for m in matches:
                    m_key = f"{m.get('propertyId')}|{m.get('vendorId')}|{m.get('accountNumber')}"
                    matched_tracked_keys.add(m_key)
                results["matched_normalized"].append(record)
                continue

            # 3. Property + exact account (vendor differs)
            if prop_acct_key in tracked_by_prop_acct:
                matches = tracked_by_prop_acct[prop_acct_key]
                record["tracked"] = matches[0]
                record["status"] = "matched_vendor_diff"
                record["notes"] = f"Vendor differs: '{up_vendor_name}' vs '{matches[0].get('vendorName')}'"
                for m in matches:
                    m_key = f"{m.get('propertyId')}|{m.get('vendorId')}|{m.get('accountNumber')}"
                    matched_tracked_keys.add(m_key)
                results["matched_vendor_diff"].append(record)
                continue

            # 4. Normalized account exists anywhere (fuzzy)
            if up_normalized in tracked_by_normalized:
                matches = tracked_by_normalized[up_normalized]
                record["possible_matches"] = matches
                record["status"] = "matched_fuzzy"
                record["notes"] = f"Normalized account '{up_normalized}' found in different property"
                results["matched_fuzzy"].append(record)
                continue

            # 5. Try similarity matching against all tracked accounts
            best_match = None
            best_score = 0.0
            for tracked in active_tracked:
                tracked_norm = _normalize_account_number(str(tracked.get("accountNumber", "")))
                score = _account_similarity(up_normalized, tracked_norm)
                if score > best_score and score >= 0.8:  # 80% similarity threshold
                    best_score = score
                    best_match = tracked

            if best_match:
                record["tracked"] = best_match
                record["status"] = "matched_fuzzy"
                record["similarity"] = round(best_score, 2)
                record["notes"] = f"Similar account ({int(best_score*100)}% match): '{up_acct_num}' ~ '{best_match.get('accountNumber')}'"
                results["matched_fuzzy"].append(record)
                continue

            record["status"] = "not_found"
            record["notes"] = "Account not tracked"
            results["not_found"].append(record)

        uploaded_acct_nums = set(str(u.get("account_number", "")).strip() for u in uploaded_accounts)

        for acct in active_tracked:
            prop_id = str(acct.get("propertyId", "")).strip()
            vendor_id = str(acct.get("vendorId", "")).strip()
            acct_num = str(acct.get("accountNumber", "")).strip()
            exact_key = f"{prop_id}|{vendor_id}|{acct_num}"

            if exact_key not in matched_tracked_keys and acct_num not in uploaded_acct_nums:
                results["orphaned"].append({
                    "tracked": acct,
                    "status": "orphaned",
                    "notes": "In tracker but not in uploaded list"
                })

        results_key = f"{GAP_ANALYSIS_PREFIX}results/{upload_id}.json"
        results_data = {
            "upload_id": upload_id,
            "analyzed_at": dt.datetime.utcnow().isoformat() + "Z",
            "analyzed_by": user,
            "summary": {
                "matched_exact": len(results["matched_exact"]),
                "matched_vendor_diff": len(results["matched_vendor_diff"]),
                "matched_fuzzy": len(results["matched_fuzzy"]),
                "not_found": len(results["not_found"]),
                "orphaned": len(results["orphaned"])
            },
            "results": results
        }

        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=results_key,
            Body=json.dumps(results_data, indent=2),
            ContentType="application/json"
        )

        return {
            "ok": True,
            "upload_id": upload_id,
            "summary": results_data["summary"],
            "results": {k: v[:100] for k, v in results.items()}
        }

    except Exception as e:
        print(f"[GAP ANALYSIS] Run error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/account-gap-analysis/add-missing")
async def api_gap_analysis_add_missing(request: Request, user: str = Depends(require_user)):
    """Add missing accounts to accounts_to_track. Admin only."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    try:
        data = await request.json()
        accounts_to_add = data.get("accounts", [])

        if not accounts_to_add:
            return JSONResponse({"error": "No accounts to add"}, status_code=400)

        existing = _get_accounts_to_track()
        existing_keys = set()
        for acct in existing:
            key = f"{acct.get('propertyId')}|{acct.get('vendorId')}|{acct.get('accountNumber')}"
            existing_keys.add(key)

        added = 0
        for acct in accounts_to_add:
            key = f"{acct.get('property_id')}|{acct.get('vendor_id')}|{acct.get('account_number')}"
            if key in existing_keys:
                continue

            new_acct = {
                "propertyId": acct.get("property_id", ""),
                "propertyName": acct.get("property_name", ""),
                "vendorId": acct.get("vendor_id", ""),
                "vendorName": acct.get("vendor_name", ""),
                "accountNumber": acct.get("account_number", ""),
                "daysBetweenBills": 30,
                "is_tracked": True,
                "is_ubi": False,
                "status": "active",
                "added_by": user,
                "added_at": dt.datetime.utcnow().isoformat() + "Z",
                "added_via": "gap_analysis"
            }
            existing.append(new_acct)
            existing_keys.add(key)
            added += 1

        if added > 0:
            _put_accounts_to_track(existing)

        return {"ok": True, "added": added}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# Workflow S3 cache functions
def _s3_get_workflow_cache() -> dict | None:
    """Load pre-computed workflow data from S3 cache."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=WORKFLOW_CACHE_KEY)
        data = json.loads(obj["Body"].read().decode("utf-8"))
        return data if isinstance(data, dict) else None
    except s3.exceptions.NoSuchKey:
        return None
    except Exception as e:
        print(f"[_s3_get_workflow_cache] Error: {e}")
        return None


def _s3_put_workflow_cache(data: dict) -> bool:
    """Save pre-computed workflow data to S3 cache."""
    try:
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=WORKFLOW_CACHE_KEY,
            Body=json.dumps(data, indent=2).encode("utf-8"),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[_s3_put_workflow_cache] Error: {e}")
        return False


def _compute_workflow_data() -> dict:
    """Heavy computation: scan all bill stages and compute workflow status for each account.
    This should be called in background, not during page load.
    """
    print("[_compute_workflow_data] Starting heavy computation...")
    start_time = dt.datetime.utcnow()
    today = dt.date.today()

    # Load configs
    all_accounts = _get_accounts_to_track()
    # Filter out archived accounts
    accounts = [a for a in all_accounts if a.get("status") != "archived"]
    print(f"[_compute_workflow_data] {len(accounts)} active accounts (filtered from {len(all_accounts)} total)")
    mapping = _ddb_get_config("ap-mapping") or []
    map_by_pid = {str(r.get("propertyId") or "").strip(): r for r in mapping if isinstance(r, dict)}

    # Build vendor ID -> vendor code lookup
    vendor_code_map: dict[str, str] = {}
    try:
        vend_cache_obj = s3.get_object(Bucket="api-vendor", Key="vendors/latest.json")
        vend_cache_data = json.loads(vend_cache_obj["Body"].read().decode("utf-8"))
        for v in vend_cache_data.get("vendors", []):
            vid = str(v.get("vendorId", "")).strip()
            vcode = str(v.get("vendorCode", "")).strip()
            if vid and vcode:
                vendor_code_map[vid] = vcode
    except Exception as e:
        print(f"[_compute_workflow_data] Failed to load vendor cache: {e}")

    # Scan last 6 months of bills to find latest bill per account
    months: list[dt.date] = []
    ref = dt.date(today.year, today.month, 1)
    for i in range(6, 0, -1):
        y = (ref.year * 12 + ref.month - i - 1) // 12
        m = (ref.month - i - 1) % 12 + 1
        months.append(dt.date(y, m, 1))
    months.append(ref)  # Current month

    # Read bill records from all stages
    print(f"[_compute_workflow_data] Scanning {len(months)} months of bills...")
    stage4_keys = list(_iter_stage_objects_by_month(STAGE4_PREFIX, months))
    stage6_keys = list(_iter_stage_objects_by_month(STAGE6_PREFIX, months))
    stage7_keys = list(_iter_stage_objects_by_month(POST_ENTRATA_PREFIX, months))
    archive_keys = list(_iter_stage_objects_by_month(HIST_ARCHIVE_PREFIX, months))

    print(f"[_compute_workflow_data] Found keys: S4={len(stage4_keys)}, S6={len(stage6_keys)}, S7={len(stage7_keys)}, Archive={len(archive_keys)}")

    # Only read FIRST record from each file (we just need header info: bill date, property, vendor, account)
    stage4 = _read_first_record_from_s3(stage4_keys)
    stage6 = _read_first_record_from_s3(stage6_keys)
    stage7 = _read_first_record_from_s3(stage7_keys)
    archive = _read_first_record_from_s3(archive_keys)
    print(f"[_compute_workflow_data] Read first records: S4={len(stage4)}, S6={len(stage6)}, S7={len(stage7)}, Archive={len(archive)}")

    # Index bills by (propertyId, vendorId, accountNumber)
    def norm_rec(rec: dict) -> dict:
        pid = rec.get("EnrichedPropertyID") or rec.get("propertyId") or rec.get("PropertyID")
        vid = rec.get("EnrichedVendorID") or rec.get("vendorId") or rec.get("VendorID")
        acct = rec.get("Account Number") or rec.get("accountNumber") or rec.get("AccountNumber")
        bill_date = rec.get("Bill Date") or rec.get("billDate")
        pstart = rec.get("Bill Period Start") or rec.get("billPeriodStart")
        pend = rec.get("Bill Period End") or rec.get("billPeriodEnd")
        bd = _parse_date_any(str(bill_date or ""))
        ps = _parse_date_any(str(pstart or ""))
        pe = _parse_date_any(str(pend or ""))
        # Get PDF link - the actual invoice PDF S3 path
        # PDF_LINK often contains expired Lambda short URLs, so prefer source_input_key
        pdf_link = rec.get("source_input_key") or rec.get("pdfKey") or ""
        if not pdf_link:
            # Only use PDF_LINK if it looks like an S3 path (not a Lambda URL)
            pl = rec.get("PDF_LINK") or ""
            if pl and ("Bill_Parser" in pl or pl.startswith("s3://") or ".pdf" in pl.lower()):
                pdf_link = pl
        return {
            "propertyId": str(pid or "").strip(),
            "vendorId": str(vid or "").strip(),
            "accountNumber": str(acct or "").strip(),
            "billDate": bd,
            "billPeriodStart": ps,
            "billPeriodEnd": pe,
            "s3Key": rec.get("__s3_key__", ""),
            "pdfLink": str(pdf_link or "").strip(),
        }

    # Build index of latest bill per account key
    latest_bills: dict[tuple, dict] = {}
    all_records = [
        (stage7, "POSTED"),
        (archive, "ARCHIVED"),
        (stage6, "PENDING"),
        (stage4, "ENRICHED"),
    ]

    for records, stage in all_records:
        for r in records:
            n = norm_rec(r)
            key = (n["propertyId"], n["vendorId"], n["accountNumber"])
            if not all(key):
                continue
            bd = n["billDate"]
            if not bd:
                continue
            existing = latest_bills.get(key)
            if not existing or (existing["billDate"] and bd > existing["billDate"]):
                latest_bills[key] = {**n, "stage": stage}

    # Load scraper mappings for badge display (same approach as completion tracker)
    import re as _re
    _load_scraper_mappings()
    _scraper_acct_ids_clean = set()
    _scraper_acct_providers = {}  # clean_acct -> provider
    for _sa in _scraper_account_map.values():
        _aid = _sa.get("account_id", "")
        _prov = _sa.get("provider", "")
        if _aid:
            _clean = _re.sub(r'[^A-Za-z0-9]', '', _aid).lower()
            _scraper_acct_ids_clean.add(_clean)
            if _prov:
                _scraper_acct_providers[_clean] = _prov

    # Build duplicate account index: accountNumber -> list of (propertyName, vendorName)
    _dup_index: dict[str, list[tuple[str, str]]] = {}
    for a in accounts:
        acct = str(a.get("accountNumber") or "").strip()
        if acct:
            _dup_index.setdefault(acct, []).append((
                str(a.get("propertyName") or "").strip(),
                str(a.get("vendorName") or "").strip(),
            ))
    dup_accounts = {acct for acct, entries in _dup_index.items() if len(entries) > 1}

    # Build workflow rows
    rows = []
    summary = {"total": 0, "veryLate": 0, "late": 0, "slightlyLate": 0, "dueSoon": 0, "onTrack": 0}

    for a in accounts:
        pid = str(a.get("propertyId") or "").strip()
        vid = str(a.get("vendorId") or "").strip()
        acct = str(a.get("accountNumber") or "").strip()
        days_config = int(a.get("daysBetweenBills") or 30)

        account_key = f"{pid}|{vid}|{acct}"
        mrow = map_by_pid.get(pid) or {}
        ap_name = str(mrow.get("name") or "").strip()

        # Find latest bill
        latest = latest_bills.get((pid, vid, acct))

        # Calculate next expected date
        service_period_days = None
        next_expected = None
        calculation_method = "fallback"
        last_bill_date = None
        last_bill_period_start = None
        last_bill_period_end = None
        last_bill_stage = None

        if latest:
            last_bill_date = latest["billDate"]
            last_bill_period_start = latest["billPeriodStart"]
            last_bill_period_end = latest["billPeriodEnd"]
            last_bill_stage = latest["stage"]

            # Try to calculate service period from bill dates
            if last_bill_period_start and last_bill_period_end:
                service_period_days = (last_bill_period_end - last_bill_period_start).days
                if service_period_days > 0 and last_bill_date:
                    next_expected = last_bill_date + dt.timedelta(days=service_period_days)
                    calculation_method = "actual"

            # Fallback to daysBetweenBills if service period calc failed
            if not next_expected and last_bill_date and days_config > 0:
                next_expected = last_bill_date + dt.timedelta(days=days_config)
                service_period_days = days_config
                calculation_method = "fallback"

        # If no bill found at all, assume overdue (next_expected = today - days_config)
        if not next_expected:
            next_expected = today - dt.timedelta(days=days_config) if days_config > 0 else today
            service_period_days = days_config
            calculation_method = "no_history"

        # Calculate days overdue and status
        days_overdue = (today - next_expected).days

        if days_overdue >= 15:
            status = "VERY_LATE"
            urgency_score = 4
            summary["veryLate"] += 1
        elif days_overdue >= 8:
            status = "LATE"
            urgency_score = 3
            summary["late"] += 1
        elif days_overdue >= 1:
            status = "SLIGHTLY_LATE"
            urgency_score = 2
            summary["slightlyLate"] += 1
        elif days_overdue >= -7:
            status = "DUE_SOON"
            urgency_score = 1
            summary["dueSoon"] += 1
        else:
            status = "ON_TRACK"
            urgency_score = 0
            summary["onTrack"] += 1

        summary["total"] += 1

        last_bill_key = latest.get("s3Key", "") if latest else ""
        last_bill_pdf_id = pdf_id_from_key(last_bill_key) if last_bill_key else ""
        last_bill_pdf_link = latest.get("pdfLink", "") if latest else ""

        # Extract processing date from S3 key (yyyy=YYYY/mm=MM/dd=DD) for View link
        last_bill_key_date = ""
        if last_bill_key:
            _m = _re.search(r'yyyy=(\d{4})/mm=(\d{2})/dd=(\d{2})', last_bill_key)
            if _m:
                last_bill_key_date = f"{_m.group(1)}-{_m.group(2)}-{_m.group(3)}"

        # Check scraper using cleaned account number (same approach as completion tracker)
        _clean_acct = _re.sub(r'[^A-Za-z0-9]', '', acct).lower() if acct else ""
        has_scraper = _clean_acct in _scraper_acct_ids_clean if _clean_acct else False
        scraper_provider = _scraper_acct_providers.get(_clean_acct, "")

        rows.append({
            "accountKey": account_key,
            "propertyId": pid,
            "propertyName": a.get("propertyName") or "",
            "vendorId": vid,
            "vendorName": a.get("vendorName") or "",
            "vendorCode": vendor_code_map.get(vid, ""),
            "accountNumber": acct,
            "apName": ap_name,
            "glAccountName": a.get("glAccountName") or "",
            "lastBillDate": last_bill_date.isoformat() if last_bill_date else "",
            "lastBillPeriodStart": last_bill_period_start.isoformat() if last_bill_period_start else "",
            "lastBillPeriodEnd": last_bill_period_end.isoformat() if last_bill_period_end else "",
            "servicePeriodDays": service_period_days,
            "nextExpectedDate": next_expected.isoformat() if next_expected else "",
            "daysBetweenBills": days_config,
            "calculationMethod": calculation_method,
            "status": status,
            "daysOverdue": days_overdue,
            "urgencyScore": urgency_score,
            "lastBillStage": last_bill_stage or "",
            "lastBillPdfId": last_bill_pdf_id,
            "lastBillKeyDate": last_bill_key_date,
            "lastBillPdfLink": last_bill_pdf_link,
            "hasScraper": has_scraper,
            "scraperProvider": scraper_provider,
            "isDuplicate": acct in dup_accounts,
            "duplicateOf": [f"{p} / {v}" for p, v in _dup_index.get(acct, [])
                            if not (p == (a.get("propertyName") or "") and v == (a.get("vendorName") or ""))]
                           if acct in dup_accounts else [],
        })

    # Sort by urgency (highest first), then by days overdue (most overdue first)
    rows.sort(key=lambda r: (-r["urgencyScore"], -r["daysOverdue"]))

    # Build filter options
    ap_names = sorted(set(r["apName"] for r in rows if r["apName"]))
    properties = sorted(set((r["propertyId"], r["propertyName"]) for r in rows), key=lambda x: x[1])
    vendors = sorted(set((r["vendorId"], r["vendorName"]) for r in rows), key=lambda x: x[1])

    elapsed = (dt.datetime.utcnow() - start_time).total_seconds()
    print(f"[_compute_workflow_data] Completed in {elapsed:.1f}s, {len(rows)} accounts processed")

    return {
        "rows": rows,
        "summary": summary,
        "filters": {
            "apReps": ap_names,
            "properties": [{"id": p[0], "name": p[1]} for p in properties],
            "vendors": [{"id": v[0], "name": v[1]} for v in vendors],
            "statuses": ["VERY_LATE", "LATE", "SLIGHTLY_LATE", "DUE_SOON", "ON_TRACK"],
        },
        "computedAt": dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
        "computedForDate": today.isoformat(),
    }


@app.post("/api/workflow/recalculate")
def api_workflow_recalculate(background_tasks: BackgroundTasks, user: str = Depends(require_user)):
    """Trigger background recalculation of workflow data."""
    def do_recalculate():
        try:
            data = _compute_workflow_data()
            _s3_put_workflow_cache(data)
            print("[api_workflow_recalculate] Background recalculation complete, saved to S3")
        except Exception as e:
            print(f"[api_workflow_recalculate] Background recalculation failed: {e}")

    background_tasks.add_task(do_recalculate)
    return {"ok": True, "message": "Recalculation started in background"}


@app.get("/api/workflow")
def api_workflow(request: Request, user: str = Depends(require_user)):
    """Return WORKFLOW data from S3 cache (fast).
    Use /api/workflow/recalculate to refresh the cache.
    """
    # Load notes and reason codes (these are small and change frequently)
    notes = _s3_get_workflow_notes()
    notes_by_key = {n.get("accountKey"): n for n in notes}
    reason_codes = _s3_get_workflow_reasons()

    # Try to load from S3 cache first (fast path)
    cached = _s3_get_workflow_cache()

    if cached and cached.get("rows"):
        # Inject fresh notes into cached rows
        for row in cached.get("rows", []):
            ak = row.get("accountKey")
            note = notes_by_key.get(ak)
            row["note"] = note
            row["hasNote"] = bool(note)
        # Also inject reason codes into filters
        if "filters" in cached:
            cached["filters"]["reasonCodes"] = reason_codes
        return cached

    # Cache miss - return empty response with flag to trigger recalculation
    print("[WORKFLOW] Cache miss - returning empty, UI should trigger recalculation")
    return {
        "rows": [],
        "summary": {"total": 0, "veryLate": 0, "late": 0, "slightlyLate": 0, "dueSoon": 0, "onTrack": 0},
        "filters": {
            "apReps": [],
            "properties": [],
            "vendors": [],
            "statuses": ["VERY_LATE", "LATE", "SLIGHTLY_LATE", "DUE_SOON", "ON_TRACK"],
            "reasonCodes": reason_codes,
        },
        "cacheStatus": "empty",
        "computedAt": None,
    }


# -------- WORKFLOW COMPLETION TRACKER --------
# In-memory cache for workflow completion tracker (same pattern as _VENDOR_PAIR_CACHE)
_WORKFLOW_TRACKER_LOCK = threading.Lock()


def _compute_workflow_tracker(months_back: int = 6) -> dict:
    """Compute completion tracker data: for every tracked account  every month,
    determine whether a bill exists in the pipeline (stages 4/6/7/archive).
    Returns structured data for the completion tracker UI.
    """
    import calendar
    start_time = time.time()
    today = dt.date.today()

    # Step 1: Load accounts + AP mapping
    all_accounts = _get_accounts_to_track()
    active = [a for a in all_accounts
              if a.get("status") != "archived" and a.get("is_tracked", True)]
    print(f"[WORKFLOW TRACKER] {len(active)} active tracked accounts")

    mapping = _ddb_get_config("ap-mapping") or []
    ap_by_pid = {str(r.get("propertyId") or "").strip(): str(r.get("name") or "").strip()
                 for r in mapping if isinstance(r, dict) and r.get("propertyId") and r.get("name")}

    # Step 2: Generate month list (MM/YYYY strings + date objects)
    months_list = []  # list of (MM/YYYY string, first_day_date, last_day_date)
    for i in range(months_back, 0, -1):
        # Calculate month offset from today
        total_months = today.year * 12 + today.month - 1 - i
        y = total_months // 12
        m = total_months % 12 + 1
        last_day = calendar.monthrange(y, m)[1]
        months_list.append((f"{m:02d}/{y}", dt.date(y, m, 1), dt.date(y, m, last_day)))
    # Add current month
    cur_last = calendar.monthrange(today.year, today.month)[1]
    months_list.append((f"{today.month:02d}/{today.year}", dt.date(today.year, today.month, 1),
                        dt.date(today.year, today.month, cur_last)))

    month_strings = [m[0] for m in months_list]

    # Step 3: Scan pipeline for bills (stages 4, 6, 7, archive)
    # Scan 3 extra months before the display window to catch bills whose
    # service period extends into our window (e.g., quarterly bill processed
    # in JUL with service period JUL-SEP should mark SEP as covered)
    scan_months = []
    first_window = months_list[0][1]  # first_day of earliest display month
    for extra in range(3, 0, -1):
        total_m = first_window.year * 12 + first_window.month - 1 - extra
        scan_months.append(dt.date(total_m // 12, total_m % 12 + 1, 1))
    scan_months.extend([dt.date(ml[1].year, ml[1].month, 1) for ml in months_list])

    stage4_keys = list(_iter_stage_objects_by_month(STAGE4_PREFIX, scan_months))
    stage6_keys = list(_iter_stage_objects_by_month(STAGE6_PREFIX, scan_months))
    stage7_keys = list(_iter_stage_objects_by_month(POST_ENTRATA_PREFIX, scan_months))
    archive_keys = list(_iter_stage_objects_by_month(HIST_ARCHIVE_PREFIX, scan_months))

    total_keys = len(stage4_keys) + len(stage6_keys) + len(stage7_keys) + len(archive_keys)
    print(f"[WORKFLOW TRACKER] Found {total_keys} S3 keys (S4={len(stage4_keys)}, S6={len(stage6_keys)}, S7={len(stage7_keys)}, Archive={len(archive_keys)})")

    # Read first record from each file
    stage4 = _read_first_record_from_s3(stage4_keys)
    stage6 = _read_first_record_from_s3(stage6_keys)
    stage7 = _read_first_record_from_s3(stage7_keys)
    archive = _read_first_record_from_s3(archive_keys)

    # Build lookup: (normalized_property_id, normalized_account) -> { month_str: stage_label }
    bills_found: dict[tuple, dict[str, str]] = {}  # key -> {month: stage}

    def _extract_bill_months(rec: dict) -> list[str]:
        """Extract ALL months covered by a bill using service period dates.
        A quarterly bill with service AUG-OCT marks AUG, SEP, OCT all covered.
        Falls back to bill date month if no service period available."""
        # Try service period first
        ps_str = rec.get("Bill Period Start") or rec.get("billPeriodStart") or ""
        pe_str = rec.get("Bill Period End") or rec.get("billPeriodEnd") or ""
        ps = _parse_date_any(str(ps_str))
        pe = _parse_date_any(str(pe_str))
        if ps and pe and ps <= pe:
            covered = []
            cur = dt.date(ps.year, ps.month, 1)
            end_m = dt.date(pe.year, pe.month, 1)
            while cur <= end_m:
                covered.append(f"{cur.month:02d}/{cur.year}")
                if cur.month == 12:
                    cur = dt.date(cur.year + 1, 1, 1)
                else:
                    cur = dt.date(cur.year, cur.month + 1, 1)
            if covered:
                return covered
        # Fallback: bill date
        bd_str = rec.get("Bill Date") or rec.get("billDate") or ""
        bd = _parse_date_any(str(bd_str))
        if bd:
            return [f"{bd.month:02d}/{bd.year}"]
        # Fallback: S3 key path
        s3k = rec.get("__s3_key__", "")
        _m = re.search(r'yyyy=(\d{4})/mm=(\d{2})', s3k)
        if _m:
            return [f"{int(_m.group(2)):02d}/{int(_m.group(1))}"]
        return []

    stage_labels = [
        (stage7, "S7"),
        (archive, "S99"),
        (stage6, "S6"),
        (stage4, "S4"),
    ]

    for records, stage_label in stage_labels:
        for rec in records:
            pid = str(rec.get("EnrichedPropertyID") or rec.get("propertyId")
                      or rec.get("PropertyID") or rec.get("Property ID") or "").strip()
            acct = str(rec.get("Account Number") or rec.get("accountNumber")
                       or rec.get("AccountNumber") or "").strip()
            if not pid or not acct:
                continue
            norm_acct = _normalize_account_number(acct)
            bill_months = _extract_bill_months(rec)
            key = (pid, norm_acct)
            if key not in bills_found:
                bills_found[key] = {}
            for bm in bill_months:
                # First stage seen wins (stage7 > archive > stage6 > stage4)
                if bm not in bills_found[key]:
                    bills_found[key][bm] = stage_label

    print(f"[WORKFLOW TRACKER] Indexed bills for {len(bills_found)} account keys")

    # Load scraper mappings for badge
    import re as _re
    _load_scraper_mappings()
    _scraper_acct_ids_clean = set()
    for _sa in _scraper_account_map.values():
        _aid = _sa.get("account_id", "")
        if _aid:
            _scraper_acct_ids_clean.add(_re.sub(r'[^A-Za-z0-9]', '', _aid).lower())

    # Step 4: For each account, generate entries at its billing cadence
    # Only start from the first month we have a bill (avoids retroactive
    # missing entries for properties acquired mid-window).
    # Group by property for rollup.
    property_data: dict[str, dict] = {}  # property_id -> { info, items[] }

    for a in active:
        pid = str(a.get("propertyId") or "").strip()
        acct = str(a.get("accountNumber") or "").strip()
        vendor_name = str(a.get("vendorName") or "").strip()
        days_between = int(a.get("daysBetweenBills") or 30)
        norm_acct = _normalize_account_number(acct)
        acct_key = (pid, norm_acct)
        ap_name = ap_by_pid.get(pid, "")

        # Scraper check
        _clean_acct = _re.sub(r'[^A-Za-z0-9]', '', acct).lower() if acct else ""
        in_scraper = _clean_acct in _scraper_acct_ids_clean if _clean_acct else False

        if pid not in property_data:
            property_data[pid] = {
                "property_id": pid,
                "property_name": str(a.get("propertyName") or "").strip(),
                "ap_name": ap_name,
                "items": [],
            }

        acct_bills = bills_found.get(acct_key, {})
        months_per_cycle = max(1, round(days_between / 30))

        # Find earliest covered month in our display window as baseline.
        # Only generate entries starting from the first month we have a bill.
        # This prevents showing 6 months of "missing" for a property
        # acquired last month or an account that just started tracking.
        earliest_idx = None
        for idx, (ms, _, _) in enumerate(months_list):
            if ms in acct_bills:
                earliest_idx = idx
                break

        if earliest_idx is None:
            # No bills found in window at all  only show current period
            # so user sees the account exists but we don't retroactively
            # flag months we have no history for
            start_idx = len(months_list) - 1
        else:
            # Start from the first covered month (it will show as COMPLETE)
            start_idx = earliest_idx

        # Generate entries at billing cadence (step by months_per_cycle)
        i = start_idx
        while i < len(months_list):
            cycle_end_idx = min(i + months_per_cycle - 1, len(months_list) - 1)
            cycle_months = months_list[i:cycle_end_idx + 1]
            if not cycle_months:
                break

            # Display label: single month or range for multi-month cycles
            if months_per_cycle == 1:
                display_month = cycle_months[-1][0]
            else:
                if len(cycle_months) == 1:
                    display_month = cycle_months[0][0]
                else:
                    display_month = cycle_months[0][0] + "-" + cycle_months[-1][0]

            cycle_end_date = cycle_months[-1][2]  # Last day of last month in cycle

            # Check if ANY month in this cycle has a bill
            has_bill = any(m[0] in acct_bills for m in cycle_months)
            pipeline_stage = None
            for m in cycle_months:
                if m[0] in acct_bills:
                    pipeline_stage = acct_bills[m[0]]
                    break

            # Status calculation
            if has_bill:
                status_label = "COMPLETE"
                days_overdue = 0
            elif today <= cycle_end_date:
                # Cycle hasn't ended yet
                days_until_end = (cycle_end_date - today).days
                if days_until_end <= 7:
                    status_label = "DUE_SOON"
                else:
                    status_label = "ON_TRACK"
                days_overdue = 0
            else:
                # Cycle ended  expect bill within ~15 days after period end
                # (time for utility company to generate + deliver the bill).
                # This gives a realistic window instead of using the full
                # daysBetweenBills as grace, which pushed everything to VERY_LATE.
                expected_arrival = cycle_end_date + dt.timedelta(days=15)
                days_overdue = (today - expected_arrival).days
                if days_overdue <= 0:
                    status_label = "DUE_SOON"
                    days_overdue = 0
                elif days_overdue <= 7:
                    status_label = "SLIGHTLY_LATE"
                elif days_overdue <= 14:
                    status_label = "LATE"
                else:
                    status_label = "VERY_LATE"

            property_data[pid]["items"].append({
                "vendor_name": vendor_name,
                "account_number": acct,
                "month": display_month,
                "status": "COMPLETE" if has_bill else "MISSING",
                "days_overdue": days_overdue,
                "status_label": status_label,
                "days_between_bills": days_between,
                "in_scraper": in_scraper,
                "has_bill_in_pipeline": has_bill,
                "pipeline_stage": pipeline_stage,
            })

            i += months_per_cycle

    # Step 5: Build rollups
    summary_by_status = {
        "VERY_LATE": 0, "LATE": 0, "SLIGHTLY_LATE": 0,
        "DUE_SOON": 0, "ON_TRACK": 0, "COMPLETE": 0,
    }
    total_account_months = 0
    total_complete = 0

    ap_rollup: dict[str, dict] = {}  # ap_name -> {total, complete, missing}

    properties_out = []
    for pid, pdata in sorted(property_data.items(), key=lambda x: x[1]["property_name"]):
        items = pdata["items"]
        prop_complete = sum(1 for it in items if it["status_label"] == "COMPLETE")
        prop_missing = len(items) - prop_complete
        prop_pct = round(100 * prop_complete / len(items), 1) if items else 0

        for it in items:
            summary_by_status[it["status_label"]] += 1
            total_account_months += 1
            if it["status_label"] == "COMPLETE":
                total_complete += 1

        ap_name = pdata["ap_name"] or "Unassigned"
        if ap_name not in ap_rollup:
            ap_rollup[ap_name] = {"total": 0, "complete": 0, "missing": 0}
        ap_rollup[ap_name]["total"] += len(items)
        ap_rollup[ap_name]["complete"] += prop_complete
        ap_rollup[ap_name]["missing"] += prop_missing

        # Sort items: missing first (oldest first), then complete
        items.sort(key=lambda it: (
            0 if it["status_label"] != "COMPLETE" else 1,
            -it["days_overdue"],
            it["month"],
        ))

        properties_out.append({
            "property_id": pid,
            "property_name": pdata["property_name"],
            "ap_name": pdata["ap_name"],
            "total_account_months": len(items),
            "complete": prop_complete,
            "missing": prop_missing,
            "percentage": prop_pct,
            "items": items,
        })

    # Sort properties: lowest completion first
    properties_out.sort(key=lambda p: (p["percentage"], p["property_name"]))

    overall_pct = round(100 * total_complete / total_account_months, 1) if total_account_months else 0

    ap_summary = []
    for ap_name in sorted(ap_rollup.keys()):
        r = ap_rollup[ap_name]
        ap_summary.append({
            "ap_name": ap_name,
            "total": r["total"],
            "complete": r["complete"],
            "missing": r["missing"],
            "percentage": round(100 * r["complete"] / r["total"], 1) if r["total"] else 0,
        })

    elapsed = time.time() - start_time
    print(f"[WORKFLOW TRACKER] Complete in {elapsed:.1f}s  {total_account_months} account-months, {total_complete} complete ({overall_pct}%)")

    return {
        "generated_at": dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
        "months": month_strings,
        "summary": {
            "total_account_months": total_account_months,
            "complete": total_complete,
            "missing": total_account_months - total_complete,
            "percentage": overall_pct,
            "by_status": summary_by_status,
        },
        "ap_summary": ap_summary,
        "properties": properties_out,
    }


def _workflow_tracker_refresh_loop():
    """Background loop that keeps workflow tracker cache warm.
    Refreshes every 4 minutes (cache TTL is 5 minutes)."""
    _REFRESH_INTERVAL = 240  # 4 minutes
    # Initial load
    try:
        data = _compute_workflow_tracker()
        _CACHE[("workflow_tracker",)] = {"ts": time.time(), "data": data}
        print("[WORKFLOW TRACKER BG] Initial cache load complete")
    except Exception as e:
        print(f"[WORKFLOW TRACKER BG] Initial load error: {e}")
    while True:
        time.sleep(_REFRESH_INTERVAL)
        try:
            data = _compute_workflow_tracker()
            _CACHE[("workflow_tracker",)] = {"ts": time.time(), "data": data}
            print("[WORKFLOW TRACKER BG] Proactive refresh complete")
        except Exception as e:
            print(f"[WORKFLOW TRACKER BG] Refresh error (will retry): {e}")


@app.get("/api/workflow/completion-tracker")
def api_workflow_completion_tracker(
    months_back: int = 6,
    refresh: int = 0,
    user: str = Depends(require_user),
):
    """Completion tracker for ALL tracked accounts across multiple months.
    Each account  each month = one row. Accounts behind appear multiple times.
    """
    cache_key = ("workflow_tracker",)

    if not refresh:
        cached = _CACHE.get(cache_key)
        if cached and (time.time() - cached.get("ts", 0) < 300):
            return cached["data"]

    # Cache miss or forced refresh  compute now
    try:
        data = _compute_workflow_tracker(months_back)
        _CACHE[cache_key] = {"ts": time.time(), "data": data}
        return data
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "completion tracker")}, status_code=500)


def _get_ubi_account_amounts_from_stage8() -> dict:
    """Scan Stage 8 to get UBI accounts with their average amounts.
    Returns dict of account_key -> {
        "last_ubi_period": "02/2026",
        "last_service_month": (year, month),
        "avg_amount": 1234.56,
        "total_amount": 12345.60,
        "bill_count": 10
    }
    """
    import time
    start = time.time()
    print("[UBI AMOUNTS] Scanning Stage 8 for account amounts...")

    account_data = {}  # account_key -> list of {ubi_period, amount}

    try:
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET, Prefix=UBI_ASSIGNED_PREFIX)

        all_keys = []
        for page in pages:
            for obj in page.get('Contents', []):
                if obj['Key'].endswith('.jsonl'):
                    all_keys.append(obj['Key'])

        print(f"[UBI AMOUNTS] Found {len(all_keys)} Stage 8 files to scan")

        from concurrent.futures import ThreadPoolExecutor
        import re

        def process_stage8_file_for_amounts(key):
            local_accounts = {}
            try:
                ts_match = re.search(r'_(\d{8}T\d{6}Z)_', key)
                file_timestamp = ts_match.group(1) if ts_match else ""

                obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                for line in txt.splitlines():
                    if not line.strip():
                        continue
                    try:
                        rec = json.loads(line)
                        ubi_period = rec.get("ubi_period")
                        if not ubi_period:
                            continue

                        # Get amount
                        amount = 0.0
                        try:
                            amt_str = str(rec.get("ubi_amount") or rec.get("Line Item Charge") or "0")
                            amount = float(amt_str.replace("$", "").replace(",", "").strip())
                        except Exception:
                            pass

                        service_start = rec.get("Bill Period Start", "")
                        service_month = _parse_service_period_to_month(service_start)

                        prop_id = rec.get("EnrichedPropertyID", "")
                        vendor_id = rec.get("EnrichedVendorID", "")
                        acct_num = str(rec.get("Account Number", "")).strip()
                        account_key = f"{prop_id}|{vendor_id}|{acct_num}"

                        if account_key not in local_accounts:
                            local_accounts[account_key] = []
                        local_accounts[account_key].append({
                            "ubi_period": ubi_period,
                            "amount": amount,
                            "service_month": service_month,
                            "file_timestamp": file_timestamp
                        })
                    except Exception:
                        continue
            except Exception:
                pass
            return local_accounts

        with ThreadPoolExecutor(max_workers=20) as executor:
            results = list(executor.map(process_stage8_file_for_amounts, all_keys))

        # Merge results
        for local_accounts in results:
            for account_key, entries in local_accounts.items():
                if account_key not in account_data:
                    account_data[account_key] = []
                account_data[account_key].extend(entries)

        # Aggregate per account
        result = {}
        for account_key, entries in account_data.items():
            if not entries:
                continue
            # Sort by UBI period to find latest
            sorted_entries = sorted(
                entries,
                key=lambda x: (x.get("service_month") or (0, 0), x.get("file_timestamp", "")),
                reverse=True
            )
            latest = sorted_entries[0]
            total_amount = sum(e["amount"] for e in entries)
            bill_count = len(set((e["ubi_period"], e.get("service_month")) for e in entries))
            avg_amount = total_amount / bill_count if bill_count > 0 else 0

            result[account_key] = {
                "last_ubi_period": latest["ubi_period"],
                "last_service_month": latest.get("service_month"),
                "avg_amount": round(avg_amount, 2),
                "total_amount": round(total_amount, 2),
                "bill_count": bill_count,
            }

        elapsed = time.time() - start
        print(f"[UBI AMOUNTS] Scanned Stage 8 in {elapsed:.1f}s, found {len(result)} accounts")
        return result

    except Exception as e:
        print(f"[UBI AMOUNTS] Error scanning Stage 8: {e}")
        return {}


@app.get("/api/workflow/ap-priority")
def api_workflow_ap_priority(user: str = Depends(require_user)):
    """Get AP Priority list - accounts missing bills for upcoming UBI period, sorted by magnitude.
    This helps AP team prioritize which bills to find for UBI."""
    try:
        import time
        start = time.time()
        today = dt.date.today()

        # Calculate upcoming UBI period (next 1st of month)
        if today.month == 12:
            upcoming_ubi_period = f"01/{today.year + 1}"
        else:
            upcoming_ubi_period = f"{today.month + 1:02d}/{today.year}"

        print(f"[AP PRIORITY] Today: {today}, Upcoming UBI period: {upcoming_ubi_period}")

        # Get UBI account history from Stage 8 (with amounts)
        stage8_data = _get_ubi_account_amounts_from_stage8()

        # Get accounts that are tracked (for property/vendor names)
        accounts_track = _get_accounts_to_track()
        tracked_info = {}
        for a in accounts_track:
            pid = str(a.get("propertyId") or "").strip()
            vid = str(a.get("vendorId") or "").strip()
            acct = str(a.get("accountNumber") or "").strip()
            key = f"{pid}|{vid}|{acct}"
            tracked_info[key] = {
                "propertyName": a.get("propertyName", ""),
                "vendorName": a.get("vendorName", ""),
                "apName": "",  # Could add if needed
            }

        # Load AP mapping for names
        mapping = _ddb_get_config("ap-mapping") or []
        map_by_pid = {str(r.get("propertyId") or "").strip(): r for r in mapping if isinstance(r, dict)}

        # Find accounts missing bills for upcoming period
        priority_rows = []
        for account_key, data in stage8_data.items():
            last_ubi = data.get("last_ubi_period", "")
            avg_amount = data.get("avg_amount", 0)

            # Skip if already has a bill assigned for the upcoming period
            if last_ubi == upcoming_ubi_period:
                continue

            # Calculate how many months behind
            months_behind = 0
            if last_ubi:
                try:
                    parts = last_ubi.split("/")
                    last_month = int(parts[0])
                    last_year = int(parts[1])
                    # upcoming period is (today.month + 1) or January next year
                    up_parts = upcoming_ubi_period.split("/")
                    up_month = int(up_parts[0])
                    up_year = int(up_parts[1])
                    months_behind = (up_year - last_year) * 12 + (up_month - last_month)
                except Exception:
                    months_behind = 1

            # Skip if they're not behind (shouldn't happen but safety check)
            if months_behind <= 0:
                continue

            parts = account_key.split("|")
            pid = parts[0] if len(parts) > 0 else ""
            vid = parts[1] if len(parts) > 1 else ""
            acct = parts[2] if len(parts) > 2 else ""

            # Get property/vendor names from tracked info or defaults
            info = tracked_info.get(account_key, {})
            prop_name = info.get("propertyName", "")
            vendor_name = info.get("vendorName", "")

            # Get AP name from mapping
            ap_info = map_by_pid.get(pid, {})
            ap_name = ap_info.get("name", "")

            priority_rows.append({
                "accountKey": account_key,
                "propertyId": pid,
                "propertyName": prop_name,
                "vendorId": vid,
                "vendorName": vendor_name,
                "accountNumber": acct,
                "apName": ap_name,
                "lastUbiPeriod": last_ubi,
                "upcomingUbiPeriod": upcoming_ubi_period,
                "monthsBehind": months_behind,
                "avgAmount": avg_amount,
                "totalHistoricalAmount": data.get("total_amount", 0),
                "historicalBillCount": data.get("bill_count", 0),
            })

        # Sort by magnitude (highest average amount first)
        priority_rows.sort(key=lambda r: -r["avgAmount"])

        # Calculate summary
        total_magnitude = sum(r["avgAmount"] for r in priority_rows)

        elapsed = time.time() - start
        print(f"[AP PRIORITY] Found {len(priority_rows)} accounts missing bills, computed in {elapsed:.1f}s")

        # Build filter options
        ap_names = sorted(set(r["apName"] for r in priority_rows if r["apName"]))
        properties = sorted(set((r["propertyId"], r["propertyName"]) for r in priority_rows), key=lambda x: x[1])
        vendors = sorted(set((r["vendorId"], r["vendorName"]) for r in priority_rows), key=lambda x: x[1])

        return {
            "rows": priority_rows,
            "upcomingUbiPeriod": upcoming_ubi_period,
            "totalCount": len(priority_rows),
            "totalMagnitude": round(total_magnitude, 2),
            "filters": {
                "apReps": ap_names,
                "properties": [{"id": p[0], "name": p[1]} for p in properties],
                "vendors": [{"id": v[0], "name": v[1]} for v in vendors],
            },
            "computedAt": dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
        }
    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# ========== DIRECTED WORKFLOW API ENDPOINTS ==========


def _get_directed_plan_lock(user: str) -> threading.Lock:
    """Get or create a per-user lock for directed plan read-modify-write operations."""
    with _DIRECTED_PLAN_LOCKS_GUARD:
        if user not in _DIRECTED_PLAN_LOCKS:
            _DIRECTED_PLAN_LOCKS[user] = threading.Lock()
        return _DIRECTED_PLAN_LOCKS[user]

@app.post("/api/directed/generate")
def api_directed_generate(user: str = Depends(require_user)):
    """Generate today's directed work plan for the current user."""
    try:
        plan = _generate_directed_plan(user)
        return {"ok": True, "stats": plan.get("stats", {})}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "generating plan")}, status_code=500)


@app.get("/api/directed/plan")
def api_directed_plan(user: str = Depends(require_user)):
    """Get the current user's directed work plan."""
    try:
        plan = _s3_get_directed_plan(user)
        if not plan:
            return {"ok": True, "plan": None, "message": "No plan generated yet. Click Generate to build today's plan."}
        return {"ok": True, "plan": plan}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "loading plan")}, status_code=500)


@app.post("/api/directed/complete")
def api_directed_complete(request: Request, user: str = Depends(require_user)):
    """Mark a task as completed."""
    try:
        import asyncio
        body = asyncio.get_event_loop().run_until_complete(request.json())
        task_id = body.get("taskId", "")
        action = body.get("action", "")
        duration_sec = int(body.get("durationSec", 0))

        with _get_directed_plan_lock(user):
            plan = _s3_get_directed_plan(user)
            if not plan:
                return JSONResponse({"error": "No plan found"}, status_code=404)

            found = False
            for task in plan.get("tasks", []):
                if task.get("taskId") == task_id:
                    task["status"] = "completed"
                    task["completedAt"] = dt.datetime.utcnow().isoformat() + "Z"
                    task["completedAction"] = action
                    found = True
                    break

            if not found:
                return JSONResponse({"error": f"Task {task_id} not found"}, status_code=404)

            # Update stats
            completed = sum(1 for t in plan["tasks"] if t["status"] == "completed")
            plan["stats"]["completedCount"] = completed
            _s3_put_directed_plan(user, plan)

            # Log completion (inside lock to prevent read-modify-write race on log)
            today = dt.date.today()
            log = _get_directed_log(user, today)
            log["completions"].append({
                "taskId": task_id,
                "action": action,
                "taskType": next((t["taskType"] for t in plan["tasks"] if t["taskId"] == task_id), ""),
                "durationSec": duration_sec,
                "ts": dt.datetime.utcnow().isoformat() + "Z",
            })
            log["totalCompleted"] = len(log["completions"])
            log["totalPlanned"] = plan["stats"]["totalTasks"]
            _put_directed_log(user, today, log)

        return {"ok": True, "completedCount": completed}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "completing task")}, status_code=500)


@app.post("/api/directed/incomplete")
def api_directed_incomplete(request: Request, user: str = Depends(require_user)):
    """Mark a task as incomplete with reason code and optional notes."""
    try:
        import asyncio
        body = asyncio.get_event_loop().run_until_complete(request.json())
        task_id = body.get("taskId", "")
        reason_code = body.get("reasonCode", "")
        notes = body.get("notes", "")

        with _get_directed_plan_lock(user):
            plan = _s3_get_directed_plan(user)
            if not plan:
                return JSONResponse({"error": "No plan found"}, status_code=404)

            found = False
            for task in plan.get("tasks", []):
                if task.get("taskId") == task_id:
                    task["status"] = "incomplete"
                    task["incompleteReason"] = reason_code
                    task["incompleteNotes"] = notes
                    found = True
                    break

            if not found:
                return JSONResponse({"error": f"Task {task_id} not found"}, status_code=404)

            incomplete = sum(1 for t in plan["tasks"] if t["status"] == "incomplete")
            plan["stats"]["incompleteCount"] = incomplete
            _s3_put_directed_plan(user, plan)

        return {"ok": True, "incompleteCount": incomplete}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "marking incomplete")}, status_code=500)


@app.post("/api/directed/end-of-day")
def api_directed_end_of_day(request: Request, user: str = Depends(require_user)):
    """Bulk submit incomplete reasons for all remaining pending tasks."""
    try:
        import asyncio
        body = asyncio.get_event_loop().run_until_complete(request.json())
        incompletes = body.get("incompletes", [])

        today = dt.date.today()
        with _get_directed_plan_lock(user):
            plan = _s3_get_directed_plan(user)
            if not plan:
                return JSONResponse({"error": "No plan found"}, status_code=404)

            task_map = {t["taskId"]: t for t in plan.get("tasks", [])}
            log = _get_directed_log(user, today)

            for item in incompletes:
                tid = item.get("taskId", "")
                reason = item.get("reasonCode", "")
                notes = item.get("notes", "")
                task = task_map.get(tid)
                if task and task["status"] == "pending":
                    task["status"] = "incomplete"
                    task["incompleteReason"] = reason
                    task["incompleteNotes"] = notes
                    log["incompletes"].append({
                        "taskId": tid,
                        "reasonCode": reason,
                        "notes": notes,
                        "taskType": task.get("taskType", ""),
                        "accountKey": task.get("accountKey", ""),
                        "ts": dt.datetime.utcnow().isoformat() + "Z",
                    })

            completed = sum(1 for t in plan["tasks"] if t["status"] == "completed")
            incomplete = sum(1 for t in plan["tasks"] if t["status"] == "incomplete")
            plan["stats"]["completedCount"] = completed
            plan["stats"]["incompleteCount"] = incomplete
            _s3_put_directed_plan(user, plan)

            # Write log inside lock to prevent race with concurrent complete calls
            log["totalPlanned"] = plan["stats"]["totalTasks"]
            log["totalCompleted"] = completed
            log["totalIncomplete"] = incomplete
            _put_directed_log(user, today, log)

        return {"ok": True, "completed": completed, "incomplete": incomplete}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "end of day")}, status_code=500)


@app.get("/api/directed/rate")
def api_directed_rate(user: str = Depends(require_user)):
    """Get current user's processing rate stats."""
    try:
        rate = _get_directed_user_rate(user)
        return {"ok": True, "rate": rate}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "loading rate")}, status_code=500)


@app.get("/api/directed/history")
def api_directed_history(days: int = 7, user: str = Depends(require_user)):
    """Get daily plan logs for the last N days."""
    try:
        today = dt.date.today()
        days = min(days, 30)  # cap to prevent abuse

        # Batch-fetch all days in a single DDB call instead of N serial get_items
        date_list = [today - dt.timedelta(days=i) for i in range(days)]
        keys = [{"PK": {"S": "DIRECTED_LOG"}, "SK": {"S": f"{d.isoformat()}#{user}"}} for d in date_list]

        fetched_logs = {}
        for batch_start in range(0, len(keys), 100):
            batch_keys = keys[batch_start:batch_start + 100]
            try:
                resp = ddb.batch_get_item(
                    RequestItems={CONFIG_TABLE: {"Keys": batch_keys}}
                )
                for item in resp.get("Responses", {}).get(CONFIG_TABLE, []):
                    sk = item.get("SK", {}).get("S", "")
                    if item.get("data"):
                        fetched_logs[sk] = json.loads(item["data"]["S"])
            except Exception as e:
                print(f"[DIRECTED] batch_get_item error in history: {e}")

        history = []
        for d in date_list:
            sk = f"{d.isoformat()}#{user}"
            log = fetched_logs.get(sk)
            if not log:
                log = {"completions": [], "incompletes": [], "planDate": d.isoformat(), "user": user,
                       "totalPlanned": 0, "totalCompleted": 0, "totalIncomplete": 0}
            if log.get("totalPlanned", 0) > 0 or log.get("completions"):
                history.append(log)
        return {"ok": True, "history": history}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "loading history")}, status_code=500)


@app.get("/api/directed/reason-codes")
def api_directed_reason_codes(user: str = Depends(require_user)):
    """Get list of incomplete reason codes."""
    return {"ok": True, "codes": _DIRECTED_REASON_CODES}


@app.post("/api/directed/relink-scraper")
def api_directed_relink_scraper(user: str = Depends(require_user)):
    """Re-compute scraper-to-account links (admin-only)."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    try:
        result = _compute_scraper_links()
        return {"ok": True, "linked": len(result.get("links", {})), "unmatched": len(result.get("unmatchedAccounts", []))}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "relinking scraper")}, status_code=500)


@app.get("/api/directed/scraper-links")
def api_directed_scraper_links(user: str = Depends(require_user)):
    """Get current scraper-to-account links."""
    try:
        links = _s3_get_scraper_links()
        return {"ok": True, "data": links}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "loading scraper links")}, status_code=500)


@app.get("/api/directed/bw-lookup")
def api_directed_bw_lookup(provider: str, account: str = "", user: str = Depends(require_user)):
    """Look up portal credentials via Bitwarden Lambda."""
    try:
        result = _bw_lookup(provider, account)
        return {"ok": True, "data": result}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "Bitwarden lookup")}, status_code=500)


@app.get("/api/directed/team-summary")
def api_directed_team_summary(user: str = Depends(require_user)):
    """Admin view: all users' daily plan statuses and completion rates."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    try:
        today = dt.date.today()
        # Scan for all directed plan files
        summaries = []
        try:
            resp = s3.list_objects_v2(Bucket=CONFIG_BUCKET, Prefix=DIRECTED_PLAN_PREFIX, MaxKeys=100)
            for obj in resp.get("Contents", []) or []:
                try:
                    plan_data = json.loads(s3.get_object(Bucket=CONFIG_BUCKET, Key=obj["Key"])["Body"].read().decode("utf-8"))
                    if plan_data.get("planDate") == today.isoformat():
                        summaries.append({
                            "user": plan_data.get("user", ""),
                            "planDate": plan_data.get("planDate", ""),
                            "stats": plan_data.get("stats", {}),
                            "generatedAt": plan_data.get("generatedAt", ""),
                        })
                except Exception:
                    pass
        except Exception:
            pass
        return {"ok": True, "summaries": summaries}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "team summary")}, status_code=500)


@app.get("/track", response_class=HTMLResponse)
def track_view(request: Request, user: str = Depends(require_user)):
    return templates.TemplateResponse("track.html", {"request": request, "user": user})


@app.get("/debug", response_class=HTMLResponse)
def debug_view(request: Request, user: str = Depends(require_user)):
    """DEBUG page - Triage bug reports and enhancement requests. Admin only."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("debug.html", {"request": request, "user": user})


@app.get("/failed", response_class=HTMLResponse)
def failed_jobs_view(request: Request, user: str = Depends(require_user)):
    """FAILED JOBS page - View and manage failed parsing jobs. Admin only."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("failed.html", {"request": request, "user": user})


@app.get("/metrics", response_class=HTMLResponse)
def metrics_view(request: Request, user: str = Depends(require_user)):
    """METRICS page - View processing time and productivity statistics."""
    return templates.TemplateResponse("metrics.html", {"request": request, "user": user})


@app.get("/api/metrics/user-timing")
def api_metrics_user_timing(date: str = "", user: str = Depends(require_user)):
    """Get all user timing data across all users for metrics."""
    try:
        # Scan for all timing records
        response = ddb.scan(
            TableName=DRAFTS_TABLE,
            FilterExpression="begins_with(pk, :prefix)",
            ExpressionAttributeValues={
                ":prefix": {"S": "timing#"},
            },
        )
        # Group by user
        by_user = {}
        for item in response.get("Items", []):
            u = item.get("user", {}).get("S", "unknown")
            invoice_id = item.get("invoice_id", {}).get("S", "")
            total_secs = int(item.get("total_seconds", {}).get("N", 0))
            updated = item.get("updated_utc", {}).get("S", "")
            # Filter by date if provided
            if date and updated and not updated.startswith(date):
                continue
            if u not in by_user:
                by_user[u] = {"user": u, "invoices": [], "total_seconds": 0}
            by_user[u]["invoices"].append({
                "invoice_id": invoice_id,
                "seconds": total_secs,
                "updated": updated,
            })
            by_user[u]["total_seconds"] += total_secs

        # Format results
        users = []
        for u, data in by_user.items():
            users.append({
                "user": u,
                "invoice_count": len(data["invoices"]),
                "total_seconds": data["total_seconds"],
                "total_minutes": round(data["total_seconds"] / 60, 1),
                "total_hours": round(data["total_seconds"] / 3600, 2),
                "avg_seconds_per_invoice": round(data["total_seconds"] / len(data["invoices"]), 1) if data["invoices"] else 0,
            })
        users.sort(key=lambda x: x["total_seconds"], reverse=True)

        total_all = sum(u["total_seconds"] for u in users)
        return {
            "date_filter": date,
            "users": users,
            "user_count": len(users),
            "total_seconds": total_all,
            "total_hours": round(total_all / 3600, 2),
        }
    except Exception as e:
        print(f"[METRICS] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/parsing-volume")
def api_metrics_parsing_volume(days: int = 7, user: str = Depends(require_user)):
    """Get parsing volume metrics by day."""
    try:
        results = []
        today = dt.datetime.utcnow().date()
        for i in range(days):
            target_date = today - dt.timedelta(days=i)
            y, m, d = target_date.strftime('%Y'), target_date.strftime('%m'), target_date.strftime('%d')
            prefix = f"{ENRICH_PREFIX}yyyy={y}/mm={m}/dd={d}/"

            # Count files in enriched outputs for this day
            file_count = 0
            total_lines = 0
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", "")
                        if k.endswith('.jsonl'):
                            file_count += 1
                            # Optionally count lines (can be slow for many files)
            except Exception:
                pass

            results.append({
                "date": str(target_date),
                "invoices_parsed": file_count,
            })

        results.reverse()  # Chronological order
        return {"days": results, "total_invoices": sum(r["invoices_parsed"] for r in results)}
    except Exception as e:
        print(f"[METRICS] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/pipeline-summary")
def api_metrics_pipeline_summary(user: str = Depends(require_user)):
    """Get pipeline summary - count of files in each processing stage."""
    try:
        stages = [
            {"id": "pending_parsing", "name": "Pending Parsing", "prefix": "Bill_Parser_1_Pending_Parsing/", "color": "#f59e0b"},
            {"id": "rework", "name": "Rework Queue", "prefix": "Bill_Parser_Rework_Input/", "color": "#8b5cf6"},
            {"id": "in_review", "name": "Ready for Review", "prefix": ENRICH_PREFIX, "color": "#3b82f6", "is_stage4": True},
            {"id": "posted", "name": "Posted (Stage 6)", "prefix": STAGE6_PREFIX, "color": "#10b981", "is_stage6": True},
            {"id": "billback", "name": "Billback (Stage 7)", "prefix": POST_ENTRATA_PREFIX, "color": "#06b6d4", "is_stage7": True},
            {"id": "failed", "name": "Failed Jobs", "prefix": FAILED_JOBS_PREFIX, "color": "#ef4444"},
        ]

        results = []
        today = dt.datetime.utcnow().date()

        for stage in stages:
            count = 0
            oldest_ts = None
            prefix = stage["prefix"]

            try:
                paginator = s3.get_paginator('list_objects_v2')

                # For Stage 4/6/7, count only last 30 days by date partition
                if stage.get("is_stage4") or stage.get("is_stage6") or stage.get("is_stage7"):
                    for i in range(30):
                        target_date = today - dt.timedelta(days=i)
                        y, m, d = target_date.strftime('%Y'), target_date.strftime('%m'), target_date.strftime('%d')
                        day_prefix = f"{prefix}yyyy={y}/mm={m}/dd={d}/"
                        for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                            for obj in page.get("Contents", []) or []:
                                k = obj.get("Key", "")
                                if k.endswith('.jsonl'):
                                    count += 1
                                    mod_time = obj.get("LastModified")
                                    if mod_time and (oldest_ts is None or mod_time < oldest_ts):
                                        oldest_ts = mod_time
                else:
                    # For other stages, just list files directly
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get("Contents", []) or []:
                            k = obj.get("Key", "")
                            # Skip metadata files and diagnostics
                            if k.endswith('.json') or '/diagnostics/' in k or k == prefix:
                                continue
                            if k.endswith('.pdf') or k.endswith('.jsonl'):
                                count += 1
                                mod_time = obj.get("LastModified")
                                if mod_time and (oldest_ts is None or mod_time < oldest_ts):
                                    oldest_ts = mod_time
            except Exception as e:
                print(f"[PIPELINE] Error counting {stage['id']}: {e}")

            results.append({
                "id": stage["id"],
                "name": stage["name"],
                "count": count,
                "color": stage["color"],
                "oldest_age_days": (dt.datetime.now(dt.timezone.utc) - oldest_ts).days if oldest_ts else None,
            })

        return {"stages": results}
    except Exception as e:
        print(f"[METRICS] Pipeline summary error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/submitter-stats")
def api_metrics_submitter_stats(date: str = "", start_date: str = "", end_date: str = "", user: str = Depends(require_user)):
    """Get submitter productivity stats - invoices, lines, and dollars per submitter.

    Uses DynamoDB as source of truth for Submitted stats (consistent with PARSE DATE view).
    - Submitted: Lines with status=Submitted in DynamoDB where submitted_at matches target date(s)
    - Posted: Files in Stage 7 where PostedAt (or S3 LastModified) matches target date(s)
    - Aggregate: UNIQUE invoices per submitter (no double counting)

    Supports both single date and date range (for weekly views):
    - date: Single date (YYYY-MM-DD)
    - start_date, end_date: Date range (inclusive)
    """
    try:
        from zoneinfo import ZoneInfo
        pacific = ZoneInfo("America/Los_Angeles")
        utc = ZoneInfo("UTC")

        # Parse date filter(s) or use today (in Pacific time)
        if start_date and end_date:
            # Date range mode (weekly view)
            try:
                start_dt = dt.datetime.strptime(start_date, "%Y-%m-%d").date()
                end_dt = dt.datetime.strptime(end_date, "%Y-%m-%d").date()
            except ValueError:
                start_dt = end_dt = dt.datetime.now(pacific).date()
        elif date:
            # Single date mode
            try:
                start_dt = end_dt = dt.datetime.strptime(date, "%Y-%m-%d").date()
            except ValueError:
                start_dt = end_dt = dt.datetime.now(pacific).date()
        else:
            start_dt = end_dt = dt.datetime.now(pacific).date()

        target_date_str = start_dt.strftime('%Y-%m-%d')  # For backwards compatibility

        # Check cache first to avoid timeout on expensive scans
        cache_key = f"{start_dt.isoformat()}|{end_dt.isoformat()}"
        now = dt.datetime.now()
        if cache_key in _SUBMITTER_STATS_CACHE:
            cached = _SUBMITTER_STATS_CACHE[cache_key]
            cache_age = (now - cached["ts"]).total_seconds()
            if cache_age < _SUBMITTER_STATS_TTL:
                print(f"[SUBMITTER_STATS] Returning cached result (age: {cache_age:.1f}s)")
                return cached["data"]

        print(f"[SUBMITTER_STATS] Computing stats for {start_dt} to {end_dt}")

        # Calculate UTC time range for the target Pacific date range
        pacific_start = dt.datetime.combine(start_dt, dt.time.min).replace(tzinfo=pacific)
        pacific_end = dt.datetime.combine(end_dt, dt.time.max).replace(tzinfo=pacific)
        utc_start = pacific_start.astimezone(utc)
        utc_end = pacific_end.astimezone(utc)

        # Track stats per submitter
        submitted_stats: dict = {}  # submitter -> {invoices, lines, dollars}
        posted_stats: dict = {}     # poster -> {invoices, lines, dollars}
        # Track unique invoices per submitter for aggregate (no double counting)
        aggregate_invoices: dict = {}  # submitter -> set of basenames (consistent identifier)
        aggregate_stats: dict = {}     # submitter -> {lines, dollars}
        seen_posted_keys: set = set()  # Avoid counting same file twice in posted
        # Global unique invoice tracking for aggregate totals
        all_unique_invoices: set = set()  # All unique invoice basenames (submitted OR posted)
        all_unique_stats: dict = {"lines": 0, "dollars": 0.0}  # Global totals for unique invoices

        def parse_utc_timestamp(ts_str: str) -> dt.datetime | None:
            """Parse a timestamp string as UTC datetime."""
            if not ts_str:
                return None
            try:
                # Handle both with and without microseconds
                if '.' in ts_str:
                    parsed = dt.datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                else:
                    parsed = dt.datetime.fromisoformat(ts_str)
                # If no timezone info, assume UTC
                if parsed.tzinfo is None:
                    parsed = parsed.replace(tzinfo=utc)
                return parsed
            except (ValueError, TypeError):
                return None

        def get_file_stats_from_s3(s3_key: str) -> tuple:
            """Read Stage 4 file and count lines/dollars. Returns (line_count, total_dollars)."""
            try:
                file_obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                content = file_obj['Body'].read().decode('utf-8', errors='ignore')
                lines = content.strip().split('\n')
                line_count = 0
                total_dollars = 0.0
                for line in lines:
                    if not line.strip():
                        continue
                    try:
                        rec = json.loads(line)
                        line_count += 1
                        amt = rec.get("Line Item Charge") or rec.get("AMOUNT") or rec.get("amount") or rec.get("Amount") or 0
                        try:
                            total_dollars += float(str(amt).replace('$', '').replace(',', ''))
                        except (ValueError, TypeError):
                            pass
                    except json.JSONDecodeError:
                        continue
                return line_count, total_dollars
            except Exception as e:
                print(f"[SUBMITTER_STATS] Error reading {s3_key}: {e}")
                return 0, 0.0

        # === PHASE 1: Collect all SUBMITTED invoices ===
        # basename -> {submitter, lines, dollars, s3_key}
        submitted_invoices_data: dict = {}

        scan_paginator = ddb.get_paginator('scan')
        for page in scan_paginator.paginate(
            TableName=REVIEW_TABLE,
            FilterExpression='#s = :status',
            ExpressionAttributeNames={'#s': 'status'},
            ExpressionAttributeValues={':status': {'S': 'Submitted'}}
        ):
            for item in page.get('Items', []):
                pk = item.get('pk', {}).get('S', '')
                submitted_at_str = item.get('submitted_at', {}).get('S', '')
                updated_by = item.get('updated_by', {}).get('S', '') or 'Unknown'

                if not pk or '#' not in pk:
                    continue
                parts = pk.rsplit('#', 1)
                if len(parts) != 2:
                    continue
                s3_key, row_idx = parts[0], parts[1]

                # Filter by submitted_at timestamp being within target Pacific date
                submitted_at_utc = parse_utc_timestamp(submitted_at_str)
                if not submitted_at_utc or not (utc_start <= submitted_at_utc <= utc_end):
                    continue

                # Track by basename (unique invoice identifier)
                basename = s3_key.split('/')[-1]
                if basename not in submitted_invoices_data:
                    submitted_invoices_data[basename] = {
                        'submitter': updated_by,
                        'lines': set(),
                        's3_key': s3_key
                    }
                submitted_invoices_data[basename]['lines'].add(row_idx)

        # === PHASE 2: Collect all POSTED invoices ===
        # basename -> {poster, lines, dollars}
        posted_invoices_data: dict = {}

        def get_file_stats(content: str) -> tuple:
            lines = content.strip().split('\n')
            line_count = 0
            total_dollars = 0.0
            for line in lines:
                if not line.strip():
                    continue
                try:
                    rec = json.loads(line)
                    line_count += 1
                    amt = rec.get("Line Item Charge") or rec.get("AMOUNT") or rec.get("amount") or rec.get("Amount") or 0
                    try:
                        total_dollars += float(str(amt).replace('$', '').replace(',', ''))
                    except (ValueError, TypeError):
                        pass
                except json.JSONDecodeError:
                    continue
            return line_count, total_dollars

        paginator = s3.get_paginator('list_objects_v2')
        # Generate all dates in range plus +/- 2 days buffer
        scan_dates_set = set()
        current_date = start_dt - dt.timedelta(days=2)
        while current_date <= end_dt + dt.timedelta(days=2):
            scan_dates_set.add(current_date)
            current_date += dt.timedelta(days=1)
        scan_dates = sorted(scan_dates_set)

        # Step 1: Collect all S3 keys first (fast - just listing)
        stage7_keys_info = []  # [(key, s3_last_modified), ...]
        for check_date in scan_dates:
            y_str = check_date.strftime('%Y')
            m_str = check_date.strftime('%m')
            d_str = check_date.strftime('%d')
            day_prefix = f"{POST_ENTRATA_PREFIX}yyyy={y_str}/mm={m_str}/dd={d_str}/"

            try:
                for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", "")
                        if not k.endswith('.jsonl'):
                            continue
                        file_basename = k.split('/')[-1]
                        if file_basename in seen_posted_keys:
                            continue
                        seen_posted_keys.add(file_basename)
                        s3_last_modified = obj.get("LastModified")
                        if s3_last_modified and s3_last_modified.tzinfo is None:
                            s3_last_modified = s3_last_modified.replace(tzinfo=utc)
                        stage7_keys_info.append((k, s3_last_modified))
            except Exception as e:
                print(f"[SUBMITTER_STATS] Error listing {day_prefix}: {e}")

        print(f"[SUBMITTER_STATS] Found {len(stage7_keys_info)} Stage 7 files to check, fetching in parallel...")

        # Step 2: Fetch all files in PARALLEL
        from concurrent.futures import ThreadPoolExecutor, as_completed as as_completed2

        def fetch_stage7_file(key_info):
            k, s3_last_modified = key_info
            try:
                file_obj = s3.get_object(Bucket=BUCKET, Key=k)
                content = file_obj['Body'].read().decode('utf-8', errors='ignore')
                return (k, s3_last_modified, content)
            except Exception:
                return (k, s3_last_modified, None)

        stage7_contents = {}  # key -> (s3_last_modified, content)
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(fetch_stage7_file, ki): ki for ki in stage7_keys_info}
            for future in as_completed2(futures):
                try:
                    k, s3_last_modified, content = future.result()
                    if content:
                        stage7_contents[k] = (s3_last_modified, content)
                except Exception:
                    pass
        print(f"[SUBMITTER_STATS] Fetched {len(stage7_contents)} Stage 7 files")

        # Step 3: Process the fetched content
        for k, (s3_last_modified, content) in stage7_contents.items():
            try:
                first_line = content.split('\n')[0].strip()
                if not first_line:
                    continue
                first_rec = json.loads(first_line)

                # Check PostedAt field first, then SubmittedAt, then S3 LastModified
                posted_at_str = str(first_rec.get("PostedAt", "") or "").strip()
                posted_at_utc = parse_utc_timestamp(posted_at_str)

                if posted_at_utc:
                    if not (utc_start <= posted_at_utc <= utc_end):
                        continue
                else:
                    # Fall back to SubmittedAt (when PostedAt not set but file is in Stage 7)
                    submitted_at_str = str(first_rec.get("SubmittedAt", "") or "").strip()
                    submitted_at_utc = parse_utc_timestamp(submitted_at_str)
                    if submitted_at_utc:
                        if not (utc_start <= submitted_at_utc <= utc_end):
                            continue
                    else:
                        # Last resort: S3 LastModified
                        if not s3_last_modified or not (utc_start <= s3_last_modified <= utc_end):
                            continue

                poster = str(first_rec.get("PostedBy", "") or "").strip()
                if not poster:
                    poster = str(first_rec.get("Submitter", "") or "").strip() or "Unknown"

                # Get invoice identifier from __s3_key__ (links back to Stage 4)
                source_key = str(first_rec.get("__s3_key__", "") or "").strip()
                file_basename = k.split('/')[-1]
                invoice_basename = source_key.split('/')[-1] if source_key else file_basename

                line_count, total_dollars = get_file_stats(content)

                posted_invoices_data[invoice_basename] = {
                    'poster': poster,
                    'lines': line_count,
                    'dollars': total_dollars
                }
            except Exception:
                pass

        # === PHASE 3: Categorize into VENN DIAGRAM sections ===
        submitted_basenames = set(submitted_invoices_data.keys())
        posted_basenames = set(posted_invoices_data.keys())

        # SUBMITTED ONLY = submitted today but NOT posted today (left circle only)
        submitted_only = submitted_basenames - posted_basenames
        # SUBMITTED AND POSTED = submitted today AND posted today (overlap)
        submitted_and_posted = submitted_basenames & posted_basenames
        # AGGREGATE = all invoices that were SUBMITTED today (ignore POSTED ONLY)
        all_unique = submitted_basenames  # Only count invoices where someone submitted today

        # PRE-FETCH all S3 file stats in PARALLEL to avoid sequential reads
        from concurrent.futures import ThreadPoolExecutor, as_completed
        s3_keys_to_fetch = set()
        for basename in submitted_only | all_unique:
            if basename in submitted_invoices_data:
                s3_keys_to_fetch.add(submitted_invoices_data[basename]['s3_key'])

        print(f"[SUBMITTER_STATS] Pre-fetching {len(s3_keys_to_fetch)} S3 files in parallel...")
        prefetched_stats: dict = {}  # s3_key -> (line_count, total_dollars)

        def fetch_one(key):
            return key, get_file_stats_from_s3(key)

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(fetch_one, k): k for k in s3_keys_to_fetch}
            for future in as_completed(futures):
                try:
                    key, stats = future.result()
                    prefetched_stats[key] = stats
                except Exception:
                    pass
        print(f"[SUBMITTER_STATS] Pre-fetch complete, got {len(prefetched_stats)} results")

        # Populate submitted_stats with SUBMITTED ONLY invoices
        for basename in submitted_only:
            info = submitted_invoices_data[basename]
            submitter = info['submitter']
            s3_key = info['s3_key']
            line_count = len(info['lines'])
            _, total_dollars = prefetched_stats.get(s3_key, (0, 0.0))

            if submitter not in submitted_stats:
                submitted_stats[submitter] = {"invoices": 0, "lines": 0, "dollars": 0.0}
            submitted_stats[submitter]["invoices"] += 1
            submitted_stats[submitter]["lines"] += line_count
            submitted_stats[submitter]["dollars"] += total_dollars

        # Populate posted_stats with SUBMITTED AND POSTED invoices (full workflow completed)
        for basename in submitted_and_posted:
            # Use the POSTER from posted data (who completed the workflow)
            posted_info = posted_invoices_data[basename]
            poster = posted_info['poster']
            line_count = posted_info['lines']
            total_dollars = posted_info['dollars']

            if poster not in posted_stats:
                posted_stats[poster] = {"invoices": 0, "lines": 0, "dollars": 0.0}
            posted_stats[poster]["invoices"] += 1
            posted_stats[poster]["lines"] += line_count
            posted_stats[poster]["dollars"] += total_dollars

        # Calculate aggregate stats (ALL unique invoices)
        for basename in all_unique:
            if basename in submitted_invoices_data:
                info = submitted_invoices_data[basename]
                submitter = info['submitter']
                s3_key = info['s3_key']
                line_count = len(info['lines'])
                _, total_dollars = prefetched_stats.get(s3_key, (0, 0.0))
            else:
                info = posted_invoices_data[basename]
                submitter = info['poster']
                line_count = info['lines']
                total_dollars = info['dollars']

            if submitter not in aggregate_invoices:
                aggregate_invoices[submitter] = set()
                aggregate_stats[submitter] = {"lines": 0, "dollars": 0.0}
            aggregate_invoices[submitter].add(basename)
            aggregate_stats[submitter]["lines"] += line_count
            aggregate_stats[submitter]["dollars"] += total_dollars

            all_unique_invoices.add(basename)
            all_unique_stats["lines"] += line_count
            all_unique_stats["dollars"] += total_dollars

        # Format results
        submitted_list = [
            {"submitter": k, "invoices": v["invoices"], "lines": v["lines"], "dollars": round(v["dollars"], 2)}
            for k, v in sorted(submitted_stats.items(), key=lambda x: x[1]["invoices"], reverse=True)
        ]
        posted_list = [
            {"submitter": k, "invoices": v["invoices"], "lines": v["lines"], "dollars": round(v["dollars"], 2)}
            for k, v in sorted(posted_stats.items(), key=lambda x: x[1]["invoices"], reverse=True)
        ]

        # Calculate totals
        submitted_totals = {
            "invoices": sum(s["invoices"] for s in submitted_list),
            "lines": sum(s["lines"] for s in submitted_list),
            "dollars": round(sum(s["dollars"] for s in submitted_list), 2)
        }
        posted_totals = {
            "invoices": sum(s["invoices"] for s in posted_list),
            "lines": sum(s["lines"] for s in posted_list),
            "dollars": round(sum(s["dollars"] for s in posted_list), 2)
        }

        # Calculate aggregate totals
        aggregate_totals = {
            "invoices": len(all_unique_invoices),
            "lines": all_unique_stats["lines"],
            "dollars": round(all_unique_stats["dollars"], 2)
        }

        # Build aggregate by submitter list
        aggregate_by_submitter = [
            {"submitter": k, "invoices": len(aggregate_invoices[k]), "lines": aggregate_stats[k]["lines"], "dollars": round(aggregate_stats[k]["dollars"], 2)}
            for k in sorted(aggregate_invoices.keys(), key=lambda x: len(aggregate_invoices[x]), reverse=True)
        ]

        result = {
            "date": target_date_str,
            "submitted": submitted_list,
            "submitted_totals": submitted_totals,
            "posted": posted_list,
            "posted_totals": posted_totals,
            "aggregate_totals": aggregate_totals,
            "aggregate_by_submitter": aggregate_by_submitter
        }

        # Cache the result
        _SUBMITTER_STATS_CACHE[cache_key] = {"data": result, "ts": dt.datetime.now()}
        print(f"[SUBMITTER_STATS] Cached result for {cache_key}")

        return result
    except Exception as e:
        import traceback
        print(f"[METRICS] Submitter stats error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/week-over-week")
def api_metrics_week_over_week(weeks: int = 6, submitter: str = "", user: str = Depends(require_user)):
    """Get week-over-week productivity stats for the team.

    Returns stats for the last N weeks (default 6):
    - Total invoices posted
    - Total lines processed
    - Total dollars processed
    - Total late fees

    Each week runs Monday-Sunday.
    Can filter by submitter to see individual AP stats.

    Uses DynamoDB POSTED_INVOICES metadata (same source as submitter-stats) for accurate
    posted_at timestamps rather than S3 path dates.
    """
    from zoneinfo import ZoneInfo

    try:
        pacific = ZoneInfo("America/Los_Angeles")
        utc = ZoneInfo("UTC")

        # Check cache first
        cache_key = f"wow|{weeks}|{submitter}"
        if cache_key in _WEEK_OVER_WEEK_CACHE:
            cached = _WEEK_OVER_WEEK_CACHE[cache_key]
            cache_age = (dt.datetime.now() - cached.get("ts", dt.datetime.min)).total_seconds()
            if cache_age < _WEEK_OVER_WEEK_TTL:
                print(f"[WEEK_OVER_WEEK] Returning cached result (age: {cache_age:.1f}s)")
                return cached["data"]

        print(f"[WEEK_OVER_WEEK] Computing stats for last {weeks} weeks, submitter filter: '{submitter}'")

        # Calculate week boundaries (Monday-Sunday)
        today = dt.datetime.now(pacific).date()
        day_of_week = today.weekday()  # Monday = 0
        current_week_start = today - dt.timedelta(days=day_of_week)

        week_ranges = []
        for i in range(weeks):
            week_start = current_week_start - dt.timedelta(weeks=i)
            week_end = week_start + dt.timedelta(days=6)
            week_ranges.append({
                "week_num": i,
                "start": week_start,
                "end": week_end,
                "label": f"{week_start.strftime('%b %d')} - {week_end.strftime('%b %d, %Y')}"
            })

        # Use SUBMITTED invoices from jrk-bill-review table (same source as submitter-stats)
        # This ensures week-over-week ties exactly with the productivity section
        earliest_week_start = week_ranges[-1]["start"]
        latest_week_end = week_ranges[0]["end"]

        print(f"[WEEK_OVER_WEEK] Loading SUBMITTED invoices from {earliest_week_start} to {latest_week_end}")

        def parse_timestamp(ts_str: str) -> dt.datetime | None:
            """Parse timestamp string as UTC datetime."""
            if not ts_str:
                return None
            try:
                if '.' in ts_str:
                    parsed = dt.datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                else:
                    parsed = dt.datetime.fromisoformat(ts_str)
                if parsed.tzinfo is None:
                    parsed = parsed.replace(tzinfo=utc)
                return parsed
            except (ValueError, TypeError):
                return None

        def get_week_for_date(d: dt.date) -> int | None:
            """Return week_num (0=current, 1=last week, etc.) for a date, or None if not in range."""
            for i, w in enumerate(week_ranges):
                if w["start"] <= d <= w["end"]:
                    return i
            return None

        # Initialize week stats
        weeks_data = {}
        for i, w in enumerate(week_ranges):
            weeks_data[i] = {
                "week_start": w["start"].isoformat(),
                "week_end": w["end"].isoformat(),
                "label": w["label"],
                "invoices": 0,
                "lines": 0,
                "dollars": 0.0,
                "late_fees": 0.0,
                "by_submitter": {},
            }

        # Calculate UTC bounds for entire date range
        range_start_utc = dt.datetime.combine(earliest_week_start, dt.time.min).replace(tzinfo=pacific).astimezone(utc)
        range_end_utc = dt.datetime.combine(latest_week_end, dt.time.max).replace(tzinfo=pacific).astimezone(utc)

        # Scan SUBMITTED invoices from jrk-bill-review (same as submitter-stats)
        # Group by basename to count unique invoices
        invoice_data = {}  # basename -> {submitter, submitted_at, lines, s3_key}

        scan_paginator = ddb.get_paginator('scan')
        for page in scan_paginator.paginate(
            TableName=REVIEW_TABLE,
            FilterExpression='#s = :status',
            ExpressionAttributeNames={'#s': 'status'},
            ExpressionAttributeValues={':status': {'S': 'Submitted'}}
        ):
            for item in page.get('Items', []):
                pk = item.get('pk', {}).get('S', '')
                submitted_at_str = item.get('submitted_at', {}).get('S', '')
                updated_by = item.get('updated_by', {}).get('S', '') or 'Unknown'

                if not pk or '#' not in pk:
                    continue
                parts = pk.rsplit('#', 1)
                if len(parts) != 2:
                    continue
                s3_key, row_idx = parts[0], parts[1]

                # Parse and filter by submitted_at
                submitted_at = parse_timestamp(submitted_at_str)
                if not submitted_at or not (range_start_utc <= submitted_at <= range_end_utc):
                    continue

                # Track by basename (unique invoice identifier)
                basename = s3_key.split('/')[-1]
                if basename not in invoice_data:
                    invoice_data[basename] = {
                        'submitter': updated_by,
                        'submitted_at': submitted_at,
                        'lines': set(),
                        's3_key': s3_key
                    }
                invoice_data[basename]['lines'].add(row_idx)

        print(f"[WEEK_OVER_WEEK] Found {len(invoice_data)} unique SUBMITTED invoices")

        # Pre-fetch S3 file stats in parallel (same as submitter-stats)
        from concurrent.futures import ThreadPoolExecutor, as_completed

        def get_file_stats(s3_key: str) -> tuple:
            """Read Stage 4 file and count lines/dollars/late fees."""
            try:
                file_obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                content = file_obj['Body'].read().decode('utf-8', errors='ignore')
                lines = content.strip().split('\n')
                line_count = 0
                total_dollars = 0.0
                late_fee_total = 0.0
                for line in lines:
                    if not line.strip():
                        continue
                    try:
                        rec = json.loads(line)
                        line_count += 1
                        amt = rec.get("Line Item Charge") or rec.get("AMOUNT") or 0
                        try:
                            amt_float = float(str(amt).replace('$', '').replace(',', ''))
                            total_dollars += amt_float
                            # Check for late fee
                            desc = str(rec.get("Line Item Description", "") or "").lower()
                            for pattern in LATE_FEE_PATTERNS:
                                if re.search(pattern, desc, re.IGNORECASE):
                                    late_fee_total += abs(amt_float)
                                    break
                        except (ValueError, TypeError):
                            pass
                    except json.JSONDecodeError:
                        continue
                return s3_key, line_count, total_dollars, late_fee_total
            except Exception:
                return s3_key, 0, 0.0, 0.0

        s3_keys_to_fetch = {inv['s3_key'] for inv in invoice_data.values()}
        file_stats = {}  # s3_key -> (lines, dollars, late_fees)

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(get_file_stats, k): k for k in s3_keys_to_fetch}
            for future in as_completed(futures):
                try:
                    s3_key, lines, dollars, late_fees = future.result()
                    file_stats[s3_key] = (lines, dollars, late_fees)
                except Exception:
                    pass

        print(f"[WEEK_OVER_WEEK] Fetched stats for {len(file_stats)} S3 files")

        # Group invoices by week
        for basename, inv in invoice_data.items():
            submitted_at = inv['submitted_at']
            pacific_date = submitted_at.astimezone(pacific).date()
            week_num = get_week_for_date(pacific_date)
            if week_num is None:
                continue

            submitter = inv['submitter']
            s3_key = inv['s3_key']
            line_count, total_dollars, late_fee = file_stats.get(s3_key, (len(inv['lines']), 0.0, 0.0))

            # Update week totals
            weeks_data[week_num]["invoices"] += 1
            weeks_data[week_num]["lines"] += line_count
            weeks_data[week_num]["dollars"] += total_dollars
            weeks_data[week_num]["late_fees"] += late_fee

            # Update per-submitter stats
            if submitter not in weeks_data[week_num]["by_submitter"]:
                weeks_data[week_num]["by_submitter"][submitter] = {
                    "invoices": 0, "lines": 0, "dollars": 0.0, "late_fees": 0.0
                }
            weeks_data[week_num]["by_submitter"][submitter]["invoices"] += 1
            weeks_data[week_num]["by_submitter"][submitter]["lines"] += line_count
            weeks_data[week_num]["by_submitter"][submitter]["dollars"] += total_dollars
            weeks_data[week_num]["by_submitter"][submitter]["late_fees"] += late_fee

        # Convert to list and round values
        weeks_list = []
        for i in range(weeks):
            stats = weeks_data[i]
            stats["dollars"] = round(stats["dollars"], 2)
            stats["late_fees"] = round(stats["late_fees"], 2)
            for sub_stats in stats["by_submitter"].values():
                sub_stats["dollars"] = round(sub_stats["dollars"], 2)
                sub_stats["late_fees"] = round(sub_stats["late_fees"], 2)
            weeks_list.append(stats)

        # Sort by week_start descending (most recent first)
        weeks_list.sort(key=lambda x: x["week_start"], reverse=True)

        # Apply submitter filter if specified - recalculate totals for just that submitter
        if submitter:
            submitter_lower = submitter.lower()
            for week in weeks_list:
                by_sub = week.get("by_submitter", {})
                # Find matching submitter (case-insensitive)
                matched_sub = None
                for s in by_sub.keys():
                    if s.lower() == submitter_lower:
                        matched_sub = s
                        break

                if matched_sub and matched_sub in by_sub:
                    sub_stats = by_sub[matched_sub]
                    week["invoices"] = sub_stats.get("invoices", 0)
                    week["lines"] = sub_stats.get("lines", 0)
                    week["dollars"] = sub_stats.get("dollars", 0.0)
                    week["late_fees"] = sub_stats.get("late_fees", 0.0)
                else:
                    # Submitter has no data for this week
                    week["invoices"] = 0
                    week["lines"] = 0
                    week["dollars"] = 0.0
                    week["late_fees"] = 0.0

        # Calculate week-over-week changes
        for i, week in enumerate(weeks_list):
            if i + 1 < len(weeks_list):
                prev_week = weeks_list[i + 1]
                week["change"] = {
                    "invoices": week["invoices"] - prev_week["invoices"],
                    "lines": week["lines"] - prev_week["lines"],
                    "dollars": round(week["dollars"] - prev_week["dollars"], 2),
                    "late_fees": round(week["late_fees"] - prev_week["late_fees"], 2),
                }
                if prev_week["invoices"] > 0:
                    week["change"]["invoices_pct"] = round((week["invoices"] - prev_week["invoices"]) / prev_week["invoices"] * 100, 1)
                else:
                    week["change"]["invoices_pct"] = 0
            else:
                week["change"] = None

        # Build submitter totals across all weeks
        all_submitters = set()
        for w in weeks_list:
            all_submitters.update(w["by_submitter"].keys())

        submitter_totals = {}
        for sub in all_submitters:
            submitter_totals[sub] = {"invoices": 0, "lines": 0, "dollars": 0.0, "late_fees": 0.0}
            for w in weeks_list:
                if sub in w["by_submitter"]:
                    submitter_totals[sub]["invoices"] += w["by_submitter"][sub]["invoices"]
                    submitter_totals[sub]["lines"] += w["by_submitter"][sub]["lines"]
                    submitter_totals[sub]["dollars"] += w["by_submitter"][sub]["dollars"]
                    submitter_totals[sub]["late_fees"] += w["by_submitter"][sub]["late_fees"]
            submitter_totals[sub]["dollars"] = round(submitter_totals[sub]["dollars"], 2)
            submitter_totals[sub]["late_fees"] = round(submitter_totals[sub]["late_fees"], 2)

        # Sort submitters by total invoices
        submitter_list = [
            {"submitter": k, **v}
            for k, v in sorted(submitter_totals.items(), key=lambda x: x[1]["invoices"], reverse=True)
        ]

        result = {
            "weeks": weeks_list,
            "submitter_totals": submitter_list,
            "submitter_filter": submitter if submitter else None,
            "all_submitters": sorted(all_submitters),
        }

        # Cache the result
        _WEEK_OVER_WEEK_CACHE[cache_key] = {"data": result, "ts": dt.datetime.now()}
        print(f"[WEEK_OVER_WEEK] Cached result for {cache_key}")

        return result

    except Exception as e:
        import traceback
        print(f"[WEEK_OVER_WEEK] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/late-fees")
def api_metrics_late_fees(
    weeks: int = 6,
    user: str = Depends(require_user)
):
    """Get late fee analysis with per-invoice, per-account, per-vendor detail.

    Returns:
    - All invoices with late fees (sorted by amount desc)
    - Accounts with repeat late fees (problem accounts needing attention)
    - Vendors with most late fees
    - Properties with most late fees
    - Week-over-week trend
    """
    try:
        from collections import defaultdict

        pacific = pytz.timezone("America/Los_Angeles")
        now_pacific = dt.datetime.now(pacific)
        today = now_pacific.date()

        # Calculate date range
        days_since_monday = today.weekday()
        current_week_start = today - dt.timedelta(days=days_since_monday)
        oldest_date = current_week_start - dt.timedelta(weeks=weeks)
        newest_date = today

        # Load all posted invoices for the date range
        invoices = _load_posted_invoices_from_ddb(oldest_date, newest_date)

        # Filter to only invoices with late fees
        late_fee_invoices = [inv for inv in invoices if inv.get("late_fee", 0) > 0]

        # Build all aggregations
        by_account = defaultdict(lambda: {"total": 0.0, "count": 0, "invoices": [], "vendor_name": "", "property_name": ""})
        by_vendor = defaultdict(lambda: {"total": 0.0, "count": 0, "vendor_name": ""})
        by_property = defaultdict(lambda: {"total": 0.0, "count": 0, "property_name": ""})
        by_week = defaultdict(lambda: {"total": 0.0, "count": 0})
        all_invoices = []

        for inv in late_fee_invoices:
            posted_at = inv.get("posted_at", "")
            try:
                posted_date = dt.datetime.fromisoformat(posted_at.replace("Z", "+00:00")).date()
            except:
                continue

            late_amt = inv.get("late_fee", 0)
            account = inv.get("account_number", "") or "Unknown"
            vendor_id = inv.get("vendor_id", "") or "Unknown"
            vendor_name = inv.get("vendor_name", "") or vendor_id
            property_id = inv.get("property_id", "") or "Unknown"
            property_name = inv.get("property_name", "") or property_id

            # Build invoice record
            inv_record = {
                "pdf_id": inv.get("pdf_id", ""),
                "vendor_id": vendor_id,
                "vendor_name": vendor_name,
                "property_id": property_id,
                "property_name": property_name,
                "account_number": account,
                "invoice_date": inv.get("invoice_date", ""),
                "invoice_total": inv.get("invoice_total", 0),
                "late_fee": late_amt,
                "posted_at": posted_at,
                "posted_by": inv.get("posted_by", ""),
            }
            all_invoices.append(inv_record)

            # Aggregate by account (key = vendor_id + account for uniqueness)
            account_key = f"{vendor_id}|{account}"
            by_account[account_key]["total"] += late_amt
            by_account[account_key]["count"] += 1
            by_account[account_key]["account_number"] = account
            by_account[account_key]["vendor_id"] = vendor_id
            by_account[account_key]["vendor_name"] = vendor_name
            by_account[account_key]["property_id"] = property_id
            by_account[account_key]["property_name"] = property_name
            by_account[account_key]["invoices"].append(inv_record)

            # Aggregate by vendor
            by_vendor[vendor_id]["total"] += late_amt
            by_vendor[vendor_id]["count"] += 1
            by_vendor[vendor_id]["vendor_name"] = vendor_name

            # Aggregate by property
            by_property[property_id]["total"] += late_amt
            by_property[property_id]["count"] += 1
            by_property[property_id]["property_name"] = property_name

            # Aggregate by week
            days_since_monday = posted_date.weekday()
            week_start = posted_date - dt.timedelta(days=days_since_monday)
            week_key = week_start.isoformat()
            by_week[week_key]["total"] += late_amt
            by_week[week_key]["count"] += 1

        # Sort all invoices by late fee amount (highest first)
        all_invoices.sort(key=lambda x: x["late_fee"], reverse=True)

        # Convert accounts to sorted list - REPEAT OFFENDERS first (most occurrences)
        accounts_list = [
            {
                "account_key": k,
                "account_number": v["account_number"],
                "vendor_id": v["vendor_id"],
                "vendor_name": v["vendor_name"],
                "property_id": v["property_id"],
                "property_name": v["property_name"],
                "total": round(v["total"], 2),
                "count": v["count"],
                "invoices": v["invoices"],
            }
            for k, v in by_account.items()
        ]
        # Sort by count (repeat offenders) then by total
        accounts_list.sort(key=lambda x: (x["count"], x["total"]), reverse=True)

        # Convert vendors to sorted list
        vendors_list = [
            {"vendor_id": k, "vendor_name": v["vendor_name"], "total": round(v["total"], 2), "count": v["count"]}
            for k, v in sorted(by_vendor.items(), key=lambda x: x[1]["total"], reverse=True)
        ]

        # Convert properties to sorted list
        properties_list = [
            {"property_id": k, "property_name": v["property_name"], "total": round(v["total"], 2), "count": v["count"]}
            for k, v in sorted(by_property.items(), key=lambda x: x[1]["total"], reverse=True)
        ]

        # Convert weeks to sorted list (most recent first)
        weeks_list = []
        for week_start in sorted(by_week.keys(), reverse=True):
            week_end = (dt.date.fromisoformat(week_start) + dt.timedelta(days=6)).isoformat()
            weeks_list.append({
                "week_start": week_start,
                "week_end": week_end,
                "total": round(by_week[week_start]["total"], 2),
                "count": by_week[week_start]["count"],
            })

        # Calculate week-over-week change
        for i, week in enumerate(weeks_list):
            if i + 1 < len(weeks_list):
                prev = weeks_list[i + 1]
                week["change"] = round(week["total"] - prev["total"], 2)
                week["change_pct"] = round((week["total"] - prev["total"]) / prev["total"] * 100, 1) if prev["total"] > 0 else 0
            else:
                week["change"] = 0
                week["change_pct"] = 0

        # Grand totals
        grand_total = sum(inv["late_fee"] for inv in all_invoices)
        grand_count = len(all_invoices)

        # Repeat offender accounts (2+ late fees in period)
        repeat_offenders = [a for a in accounts_list if a["count"] >= 2]

        return {
            # Summary
            "grand_total": round(grand_total, 2),
            "grand_count": grand_count,
            "repeat_offender_count": len(repeat_offenders),

            # Week-over-week trend (is it getting better?)
            "weeks": weeks_list,

            # All invoices with late fees (for full drill-down)
            "invoices": all_invoices[:100],  # Top 100 by amount
            "total_invoices": len(all_invoices),

            # Problem accounts - REPEAT OFFENDERS (need autopay, timing changes, etc)
            "repeat_offenders": repeat_offenders[:20],  # Top 20 repeat offenders
            "all_accounts": accounts_list[:50],  # Top 50 accounts by occurrence

            # By vendor (which utilities have late fee issues?)
            "by_vendor": vendors_list[:20],

            # By property (which properties have late fee issues?)
            "by_property": properties_list[:20],
        }

    except Exception as e:
        import traceback
        print(f"[LATE_FEES] Error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/activity-detail")
def api_metrics_activity_detail(date: str = "", start_date: str = "", end_date: str = "", user: str = Depends(require_user)):
    """Get detailed activity log showing each activity (submission OR posting) with timestamp.

    Returns separate events for:
    - Submitted: When invoice was submitted (SubmittedAt matches target date)
    - Posted: When invoice was posted to billback (PostedAt or S3 LastModified matches target date)

    Supports both single date and date range (for weekly views):
    - date: Single date (YYYY-MM-DD)
    - start_date, end_date: Date range (inclusive)
    """
    try:
        from zoneinfo import ZoneInfo

        pacific = ZoneInfo("America/Los_Angeles")
        utc = ZoneInfo("UTC")

        # Parse date filter(s) or use today (in Pacific time)
        if start_date and end_date:
            # Date range mode (weekly view)
            try:
                start_dt = dt.datetime.strptime(start_date, "%Y-%m-%d").date()
                end_dt = dt.datetime.strptime(end_date, "%Y-%m-%d").date()
            except ValueError:
                start_dt = end_dt = dt.datetime.now(pacific).date()
        elif date:
            # Single date mode
            try:
                start_dt = end_dt = dt.datetime.strptime(date, "%Y-%m-%d").date()
            except ValueError:
                start_dt = end_dt = dt.datetime.now(pacific).date()
        else:
            start_dt = end_dt = dt.datetime.now(pacific).date()

        target_date_str = start_dt.strftime('%Y-%m-%d')  # For backwards compatibility

        # Check cache first to avoid timeout on expensive scans
        cache_key = f"activity|{start_dt.isoformat()}|{end_dt.isoformat()}"
        now = dt.datetime.now()
        if cache_key in _ACTIVITY_DETAIL_CACHE:
            cached = _ACTIVITY_DETAIL_CACHE[cache_key]
            cache_age = (now - cached["ts"]).total_seconds()
            if cache_age < _ACTIVITY_DETAIL_TTL:
                print(f"[ACTIVITY_DETAIL] Returning cached result (age: {cache_age:.1f}s)")
                return cached["data"]

        print(f"[ACTIVITY_DETAIL] Computing activity for {start_dt} to {end_dt}")

        # Calculate UTC time range for the target Pacific date range
        pacific_start = dt.datetime.combine(start_dt, dt.time.min).replace(tzinfo=pacific)
        pacific_end = dt.datetime.combine(end_dt, dt.time.max).replace(tzinfo=pacific)
        utc_start = pacific_start.astimezone(utc)
        utc_end = pacific_end.astimezone(utc)

        activities = []
        seen_submitted_keys = set()  # Avoid duplicate submitted events
        seen_posted_keys = set()  # Avoid duplicate posted events

        def parse_utc_timestamp(ts_str: str) -> dt.datetime | None:
            """Parse a timestamp string as UTC datetime."""
            if not ts_str:
                return None
            try:
                if '.' in ts_str:
                    parsed = dt.datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
                else:
                    parsed = dt.datetime.fromisoformat(ts_str)
                if parsed.tzinfo is None:
                    parsed = parsed.replace(tzinfo=utc)
                return parsed
            except (ValueError, TypeError):
                return None

        def get_file_stats(content: str) -> tuple:
            """Count lines and sum dollars. Returns (line_count, total_dollars)."""
            lines = content.strip().split('\n')
            line_count = 0
            total_dollars = 0.0
            for line in lines:
                if not line.strip():
                    continue
                try:
                    rec = json.loads(line)
                    line_count += 1
                    amt = rec.get("Line Item Charge") or rec.get("AMOUNT") or rec.get("amount") or rec.get("Amount") or 0
                    try:
                        total_dollars += float(str(amt).replace('$', '').replace(',', ''))
                    except (ValueError, TypeError):
                        pass
                except json.JSONDecodeError:
                    continue
            return line_count, total_dollars

        def extract_submitted_activity(k: str, content: str, first_rec: dict) -> dict | None:
            """Extract submitted activity if SubmittedAt matches target date."""
            file_basename = k.split('/')[-1]
            if file_basename in seen_submitted_keys:
                return None

            submitted_at_str = str(first_rec.get("SubmittedAt", "") or "").strip()
            if not submitted_at_str:
                return None

            submitted_at_utc = parse_utc_timestamp(submitted_at_str)
            if not submitted_at_utc or not (utc_start <= submitted_at_utc <= utc_end):
                return None

            seen_submitted_keys.add(file_basename)
            submitter = str(first_rec.get("Submitter", "") or "").strip() or "Unknown"
            property_name = first_rec.get("EnrichedPropertyName") or first_rec.get("Property Name") or ""
            vendor_name = first_rec.get("EnrichedVendorName") or first_rec.get("Vendor Name") or ""
            line_count, total_dollars = get_file_stats(content)

            return {
                "submitter": submitter,
                "submitted_at": submitted_at_str,
                "property": property_name,
                "vendor": vendor_name,
                "lines": line_count,
                "dollars": round(total_dollars, 2),
                "type": "submitted",
                "s3_key": k
            }

        def extract_posted_activity(k: str, content: str, first_rec: dict, s3_last_modified: dt.datetime) -> dict | None:
            """Extract posted activity if PostedAt or S3 LastModified matches target date."""
            file_basename = k.split('/')[-1]
            if file_basename in seen_posted_keys:
                return None

            # Check PostedAt field first, then SubmittedAt, then S3 LastModified
            posted_at_str = str(first_rec.get("PostedAt", "") or "").strip()
            posted_at_utc = parse_utc_timestamp(posted_at_str)

            # Determine if this was posted on the target date
            if posted_at_utc:
                if not (utc_start <= posted_at_utc <= utc_end):
                    return None
                timestamp_str = posted_at_str
            else:
                # Fall back to SubmittedAt (when PostedAt not set but file is in Stage 7)
                submitted_at_str = str(first_rec.get("SubmittedAt", "") or "").strip()
                submitted_at_utc = parse_utc_timestamp(submitted_at_str)
                if submitted_at_utc:
                    if not (utc_start <= submitted_at_utc <= utc_end):
                        return None
                    timestamp_str = submitted_at_str
                else:
                    # Last resort: S3 LastModified
                    if not (utc_start <= s3_last_modified <= utc_end):
                        return None
                    timestamp_str = s3_last_modified.strftime('%Y-%m-%dT%H:%M:%S')

            seen_posted_keys.add(file_basename)

            # Use PostedBy if available, otherwise fall back to Submitter (legacy)
            poster = str(first_rec.get("PostedBy", "") or "").strip()
            if not poster:
                poster = str(first_rec.get("Submitter", "") or "").strip() or "Unknown"

            property_name = first_rec.get("EnrichedPropertyName") or first_rec.get("Property Name") or ""
            vendor_name = first_rec.get("EnrichedVendorName") or first_rec.get("Vendor Name") or ""
            line_count, total_dollars = get_file_stats(content)

            return {
                "submitter": poster,  # For posted events, show who posted
                "submitted_at": timestamp_str,  # Reuse field for timeline sorting
                "property": property_name,
                "vendor": vendor_name,
                "lines": line_count,
                "dollars": round(total_dollars, 2),
                "type": "posted",
                "s3_key": k
            }

        paginator = s3.get_paginator('list_objects_v2')

        # Cache file contents to avoid reading the same file twice
        file_cache: dict = {}  # key -> (content, first_rec)

        def get_file_data(k: str) -> tuple | None:
            """Get file content and first record, using cache."""
            if k in file_cache:
                return file_cache[k]
            try:
                file_obj = s3.get_object(Bucket=BUCKET, Key=k)
                content = file_obj['Body'].read().decode('utf-8', errors='ignore')
                first_line = content.split('\n')[0].strip()
                if not first_line:
                    return None
                first_rec = json.loads(first_line)
                file_cache[k] = (content, first_rec)
                return content, first_rec
            except Exception as e:
                print(f"[ACTIVITY_DETAIL] Error reading {k}: {e}")
                return None

        # OPTIMIZATION: Only scan dates within the range plus +/- 2 days buffer
        # This reduces S3 calls dramatically, fixing timeout issues
        # Generate all dates in range plus buffer
        scan_dates_set = set()
        current_date = start_dt - dt.timedelta(days=2)  # Start 2 days before
        while current_date <= end_dt + dt.timedelta(days=2):  # End 2 days after
            scan_dates_set.add(current_date)
            current_date += dt.timedelta(days=1)
        scan_dates = sorted(scan_dates_set)

        # Scan Stage 6 (POST) for submitted activities
        for check_date in scan_dates:
            y, m, d = check_date.strftime('%Y'), check_date.strftime('%m'), check_date.strftime('%d')
            day_prefix = f"{STAGE6_PREFIX}yyyy={y}/mm={m}/dd={d}/"

            try:
                for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", "")
                        if k.endswith('.jsonl'):
                            data = get_file_data(k)
                            if data:
                                content, first_rec = data
                                activity = extract_submitted_activity(k, content, first_rec)
                                if activity:
                                    activities.append(activity)
            except Exception as e:
                print(f"[ACTIVITY_DETAIL] Error scanning {day_prefix}: {e}")
                continue

        # Scan Stage 7 (Billback) for both submitted AND posted activities
        for check_date in scan_dates:
            y, m, d = check_date.strftime('%Y'), check_date.strftime('%m'), check_date.strftime('%d')
            day_prefix = f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"

            try:
                for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", "")
                        if not k.endswith('.jsonl'):
                            continue
                        last_modified = obj.get("LastModified")
                        data = get_file_data(k)
                        if data:
                            content, first_rec = data
                            # Check for submitted activity
                            submitted_activity = extract_submitted_activity(k, content, first_rec)
                            if submitted_activity:
                                activities.append(submitted_activity)
                            # Check for posted activity
                            if last_modified:
                                posted_activity = extract_posted_activity(k, content, first_rec, last_modified)
                                if posted_activity:
                                    activities.append(posted_activity)
            except Exception as e:
                print(f"[ACTIVITY_DETAIL] Error scanning {day_prefix}: {e}")
                continue

        # Sort by timestamp (earliest first for chronological view)
        activities.sort(key=lambda x: x.get("submitted_at", ""), reverse=False)

        result = {
            "date": target_date_str,
            "start_date": start_dt.strftime('%Y-%m-%d'),
            "end_date": end_dt.strftime('%Y-%m-%d'),
            "activities": activities,
            "count": len(activities)
        }

        # Cache the result
        _ACTIVITY_DETAIL_CACHE[cache_key] = {"data": result, "ts": dt.datetime.now()}
        print(f"[ACTIVITY_DETAIL] Cached result for {cache_key}")

        return result
    except Exception as e:
        import traceback
        print(f"[METRICS] Activity detail error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/overrides")
def api_metrics_overrides(user: str = Depends(require_user)):
    """Get override statistics - all line item changes from the last 3 weeks.

    Groups by property name. Cached for 10 minutes.
    """
    # Check cache first
    cache_key = ("metrics_overrides",)
    now = time.time()
    cached = _CACHE.get(cache_key)
    if cached and (now - cached.get("ts", 0) < 600):  # 10 minute cache
        return cached.get("data")

    try:
        from zoneinfo import ZoneInfo
        utc = ZoneInfo("UTC")

        # Calculate 3 weeks ago cutoff
        three_weeks_ago = dt.datetime.now(utc) - dt.timedelta(weeks=3)

        # Scan all draft records
        all_drafts = []
        paginator_token = None
        while True:
            scan_params = {
                "TableName": DRAFTS_TABLE,
                "FilterExpression": "begins_with(pk, :prefix)",
                "ExpressionAttributeValues": {
                    ":prefix": {"S": "draft#"},
                },
            }
            if paginator_token:
                scan_params["ExclusiveStartKey"] = paginator_token

            response = ddb.scan(**scan_params)
            all_drafts.extend(response.get("Items", []))

            paginator_token = response.get("LastEvaluatedKey")
            if not paginator_token:
                break

        # Process drafts to extract override info
        overrides = []
        for item in all_drafts:
            pk = item.get("pk", {}).get("S", "")
            if not pk.startswith("draft#"):
                continue

            # The draft has a 'user' field stored directly - use that instead of parsing pk
            draft_user = item.get("user", {}).get("S", "")

            # Skip system entries like __final__
            if not draft_user or draft_user in ("__final__",):
                continue

            # Skip if user looks like a number (not a real user)
            if draft_user.isdigit():
                continue

            # Must look like an email or username
            if "@" not in draft_user and len(draft_user) < 3:
                continue

            # Get updated timestamp and filter to last 3 weeks
            updated_utc = item.get("updated_utc", {}).get("S", "")
            if updated_utc:
                try:
                    parsed_utc = dt.datetime.fromisoformat(updated_utc.replace('Z', '+00:00'))
                    if parsed_utc.tzinfo is None:
                        parsed_utc = parsed_utc.replace(tzinfo=utc)
                    if parsed_utc < three_weeks_ago:
                        continue
                except (ValueError, TypeError):
                    continue
            else:
                continue  # Skip records without timestamp

            # Parse fields to see what was changed
            fields_raw = item.get("fields", {}).get("S", "{}")
            try:
                fields = json.loads(fields_raw) if fields_raw else {}
            except json.JSONDecodeError:
                fields = {}

            if not fields:
                continue

            # Get property name - first check if stored in draft, then check fields
            property_name = ""
            vendor_name = ""

            # Check if property is in the override fields
            if fields.get("EnrichedPropertyName"):
                property_name = fields.get("EnrichedPropertyName", "")

            if fields.get("EnrichedVendorName"):
                vendor_name = fields.get("EnrichedVendorName", "")

            # If no property in fields, use the invoice date as grouping
            if not property_name:
                date_str = item.get("date", {}).get("S", "")
                invoice_str = item.get("invoice", {}).get("S", "")
                if invoice_str:
                    property_name = invoice_str[:50]  # Use invoice name
                elif date_str:
                    property_name = f"Invoices from {date_str}"
                else:
                    property_name = "(Unknown)"

            # Determine what types of changes were made
            # ONLY count fields that represent actual user overrides, not stored values
            change_types = []

            # Property override - user explicitly assigned a property
            if fields.get("EnrichedPropertyID") or fields.get("EnrichedPropertyName"):
                change_types.append("Property")

            # Vendor override - user explicitly assigned a vendor
            if fields.get("EnrichedVendorID") or fields.get("EnrichedVendorName"):
                change_types.append("Vendor")

            # GL override - user explicitly assigned a GL account
            if fields.get("EnrichedGLAccountID") or fields.get("EnrichedGLAccountName") or fields.get("EnrichedGLAccountNumber"):
                change_types.append("GL Account")

            if not change_types:
                continue

            # Get invoice info
            invoice = item.get("invoice", {}).get("S", "")
            invoice_date = item.get("date", {}).get("S", "")
            pdf_id = item.get("pdf_id", {}).get("S", "")

            overrides.append({
                "user": draft_user,
                "pdf_id": pdf_id,
                "change_types": change_types,
                "property_name": property_name,
                "vendor_name": vendor_name,
                "invoice": invoice,
                "invoice_date": invoice_date,
                "updated_utc": updated_utc,
                "fields": fields
            })

        # Aggregate by property
        by_property = {}
        for o in overrides:
            prop = o.get("property_name") or "(Unknown Property)"
            if prop not in by_property:
                by_property[prop] = {
                    "property_name": prop,
                    "total_overrides": 0,
                    "by_user": {},
                    "change_type_counts": {},
                    "recent_changes": []
                }

            bp = by_property[prop]
            bp["total_overrides"] += 1

            # Count by user
            u = o["user"]
            if u not in bp["by_user"]:
                bp["by_user"][u] = {"count": 0, "change_types": set()}
            bp["by_user"][u]["count"] += 1
            bp["by_user"][u]["change_types"].update(o["change_types"])

            # Count change types
            for ct in o["change_types"]:
                bp["change_type_counts"][ct] = bp["change_type_counts"].get(ct, 0) + 1

            # Keep recent changes (limit to last 10 per property)
            bp["recent_changes"].append({
                "user": o["user"],
                "change_types": o["change_types"],
                "vendor_name": o["vendor_name"],
                "updated_utc": o["updated_utc"],
                "invoice": o["invoice"],
                "fields": o["fields"]
            })

        # Format aggregated data
        property_summary = []
        for prop_name, data in by_property.items():
            # Convert user stats
            user_stats = []
            for u, udata in data["by_user"].items():
                user_stats.append({
                    "user": u,
                    "count": udata["count"],
                    "change_types": list(udata["change_types"])
                })
            user_stats.sort(key=lambda x: x["count"], reverse=True)

            # Sort recent changes by time
            data["recent_changes"].sort(key=lambda x: x.get("updated_utc", ""), reverse=True)

            property_summary.append({
                "property_name": prop_name,
                "total_overrides": data["total_overrides"],
                "user_stats": user_stats,
                "change_type_counts": data["change_type_counts"],
                "recent_changes": data["recent_changes"][:10]  # Last 10
            })

        # Sort by total overrides
        property_summary.sort(key=lambda x: x["total_overrides"], reverse=True)

        # Overall stats
        total_overrides = sum(p["total_overrides"] for p in property_summary)
        unique_users = set()
        all_change_types = {}

        # Aggregate by user for bar chart
        by_user = {}
        for o in overrides:
            u = o["user"]
            unique_users.add(u)
            if u not in by_user:
                by_user[u] = {"user": u, "total": 0, "change_types": {}}
            by_user[u]["total"] += 1
            for ct in o["change_types"]:
                all_change_types[ct] = all_change_types.get(ct, 0) + 1
                by_user[u]["change_types"][ct] = by_user[u]["change_types"].get(ct, 0) + 1

        # Sort users by total overrides
        user_stats = sorted(by_user.values(), key=lambda x: x["total"], reverse=True)

        result = {
            "total_overrides": total_overrides,
            "unique_users": len(unique_users),
            "change_type_totals": all_change_types,
            "user_stats": user_stats,  # Full user breakdown for bar chart
            "by_property": property_summary,
            "raw_overrides": overrides[:100]  # Return first 100 raw records for detail view
        }

        # Cache the result
        _CACHE[cache_key] = {"ts": time.time(), "data": result}

        return result
    except Exception as e:
        import traceback
        print(f"[METRICS] Override stats error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- Outlier Detection --------

def _s3_get_account_statistics() -> dict:
    """Load account statistics from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=ACCOUNT_STATISTICS_KEY)
        return json.loads(obj['Body'].read().decode('utf-8'))
    except s3.exceptions.NoSuchKey:
        return {"accounts": {}, "last_updated": None}
    except Exception as e:
        print(f"[OUTLIER] Error loading account stats: {e}")
        return {"accounts": {}, "last_updated": None}


def _s3_put_account_statistics(data: dict) -> bool:
    """Save account statistics to S3."""
    try:
        data["last_updated"] = dt.datetime.utcnow().isoformat() + "Z"
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=ACCOUNT_STATISTICS_KEY,
            Body=json.dumps(data, indent=2),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[OUTLIER] Error saving account stats: {e}")
        return False


# -------- Directed Workflow: Scraper Auto-Linking --------

def _normalize_account_number(acct: str) -> str:
    """Normalize account number for fuzzy matching: strip non-alphanumeric, lowercase, strip leading zeros."""
    import re
    cleaned = re.sub(r'[^A-Za-z0-9]', '', str(acct or "")).lower()
    cleaned = cleaned.lstrip('0') or '0'
    return cleaned


def _s3_get_scraper_links() -> dict:
    """Load scraper-account links from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=SCRAPER_LINK_KEY)
        return json.loads(obj['Body'].read().decode('utf-8'))
    except s3.exceptions.NoSuchKey:
        return {"links": {}, "unmatchedAccounts": [], "computedAt": None}
    except Exception as e:
        print(f"[DIRECTED] Error loading scraper links: {e}")
        return {"links": {}, "unmatchedAccounts": [], "computedAt": None}


def _s3_put_scraper_links(data: dict) -> bool:
    """Save scraper-account links to S3."""
    try:
        data["computedAt"] = dt.datetime.utcnow().isoformat() + "Z"
        s3.put_object(
            Bucket=CONFIG_BUCKET, Key=SCRAPER_LINK_KEY,
            Body=json.dumps(data, indent=2), ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[DIRECTED] Error saving scraper links: {e}")
        return False


def _compute_scraper_links() -> dict:
    """Auto-match scraper accounts to accounts_to_track entries by normalized account number.
    Returns dict with 'links' keyed by accountKey (propertyId|vendorId|accountNumber).
    """
    _load_scraper_mappings()
    accounts = _get_accounts_to_track()
    if not accounts:
        return {"links": {}, "unmatchedAccounts": [], "computedAt": None}

    # Build reverse index: normalized_account_number -> list of account entries
    acct_index: dict[str, list[dict]] = {}
    for acc in accounts:
        if not isinstance(acc, dict):
            continue
        if acc.get("status") == "archived":
            continue
        acct_num = str(acc.get("accountNumber") or "").strip()
        if not acct_num:
            continue
        norm = _normalize_account_number(acct_num)
        acct_index.setdefault(norm, []).append(acc)

    links: dict[str, dict] = {}
    matched_account_keys: set[str] = set()

    # For each scraper account, try to match
    for uuid, info in _scraper_account_map.items():
        scraper_acct_id = str(info.get("account_id") or "").strip()
        provider = str(info.get("provider") or "").strip()
        if not scraper_acct_id:
            continue
        norm_scraper = _normalize_account_number(scraper_acct_id)

        # Exact normalized match
        if norm_scraper in acct_index:
            for acc in acct_index[norm_scraper]:
                pid = str(acc.get("propertyId") or "").strip()
                vid = str(acc.get("vendorId") or "").strip()
                acct = str(acc.get("accountNumber") or "").strip()
                key = f"{pid}|{vid}|{acct}"
                if key not in links:
                    links[key] = {
                        "scraperProvider": provider,
                        "scraperAccountId": scraper_acct_id,
                        "scraperAccountUuid": uuid,
                        "matchMethod": "exact",
                        "linkedAt": dt.datetime.utcnow().isoformat() + "Z"
                    }
                    matched_account_keys.add(key)
            continue

        # Substring match: scraper ID contained in tracked account or vice versa
        for norm_tracked, accs in acct_index.items():
            if norm_scraper in norm_tracked or norm_tracked in norm_scraper:
                for acc in accs:
                    pid = str(acc.get("propertyId") or "").strip()
                    vid = str(acc.get("vendorId") or "").strip()
                    acct = str(acc.get("accountNumber") or "").strip()
                    key = f"{pid}|{vid}|{acct}"
                    if key not in links:
                        links[key] = {
                            "scraperProvider": provider,
                            "scraperAccountId": scraper_acct_id,
                            "scraperAccountUuid": uuid,
                            "matchMethod": "substring",
                            "linkedAt": dt.datetime.utcnow().isoformat() + "Z"
                        }
                        matched_account_keys.add(key)

    # Find unmatched accounts (tracked but no scraper link)
    unmatched = []
    for acc in accounts:
        if not isinstance(acc, dict) or acc.get("status") == "archived":
            continue
        pid = str(acc.get("propertyId") or "").strip()
        vid = str(acc.get("vendorId") or "").strip()
        acct = str(acc.get("accountNumber") or "").strip()
        key = f"{pid}|{vid}|{acct}"
        if key not in matched_account_keys:
            unmatched.append(key)

    result = {"links": links, "unmatchedAccounts": unmatched}
    _s3_put_scraper_links(result)
    print(f"[DIRECTED] Scraper auto-link: {len(links)} linked, {len(unmatched)} unmatched")
    return result


def _s3_get_directed_plan(user: str) -> dict | None:
    """Load a user's directed plan from S3."""
    user_hash = hashlib.sha1(user.encode()).hexdigest()[:8]
    key = f"{DIRECTED_PLAN_PREFIX}{user_hash}.json"
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=key)
        return json.loads(obj['Body'].read().decode('utf-8'))
    except s3.exceptions.NoSuchKey:
        return None
    except Exception as e:
        print(f"[DIRECTED] Error loading plan for {user}: {e}")
        return None


def _s3_put_directed_plan(user: str, data: dict) -> bool:
    """Save a user's directed plan to S3."""
    user_hash = hashlib.sha1(user.encode()).hexdigest()[:8]
    key = f"{DIRECTED_PLAN_PREFIX}{user_hash}.json"
    try:
        s3.put_object(
            Bucket=CONFIG_BUCKET, Key=key,
            Body=json.dumps(data, indent=2, default=str), ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[DIRECTED] Error saving plan for {user}: {e}")
        return False


# -------- Directed Workflow: Rate Tracking --------

_DIRECTED_DEFAULT_RATE = {
    "avgCollectionSec": 180,
    "avgProcessingSec": 60,
    "dailyCapacityMinutes": 360,
}


def _get_directed_user_rate(user: str) -> dict:
    """Get user's processing rate from DDB, or defaults."""
    try:
        resp = ddb.get_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": "DIRECTED_RATE"}, "SK": {"S": user}}
        )
        item = resp.get("Item")
        if item and item.get("data"):
            return json.loads(item["data"]["S"])
    except Exception as e:
        print(f"[DIRECTED] Error loading rate for {user}: {e}")
    return dict(_DIRECTED_DEFAULT_RATE)


def _put_directed_user_rate(user: str, rate: dict):
    """Save user's processing rate to DDB."""
    try:
        ddb.put_item(
            TableName=CONFIG_TABLE,
            Item={
                "PK": {"S": "DIRECTED_RATE"},
                "SK": {"S": user},
                "data": {"S": json.dumps(rate, default=str)},
                "updated_at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
            }
        )
    except Exception as e:
        print(f"[DIRECTED] Error saving rate for {user}: {e}")


def _compute_user_rate(user: str) -> dict:
    """Compute user's average processing rate from last 7 days of completion logs."""
    try:
        today = dt.date.today()
        collection_durations = []
        processing_durations = []
        days_with_completions = 0

        for day_offset in range(7):
            d = today - dt.timedelta(days=day_offset)
            sk = f"{d.isoformat()}#{user}"
            try:
                resp = ddb.get_item(
                    TableName=CONFIG_TABLE,
                    Key={"PK": {"S": "DIRECTED_LOG"}, "SK": {"S": sk}}
                )
                item = resp.get("Item")
                if not item or not item.get("data"):
                    continue
                log = json.loads(item["data"]["S"])
                day_had_data = False
                for c in log.get("completions", []):
                    dur = c.get("durationSec", 0)
                    if dur <= 0 or dur > 3600:
                        continue  # skip unreasonable durations
                    day_had_data = True
                    if c.get("taskType") == "collection":
                        collection_durations.append(dur)
                    else:
                        processing_durations.append(dur)
                if day_had_data:
                    days_with_completions += 1
            except Exception:
                pass

        rate = dict(_DIRECTED_DEFAULT_RATE)
        if collection_durations:
            rate["avgCollectionSec"] = round(sum(collection_durations) / len(collection_durations))
        if processing_durations:
            rate["avgProcessingSec"] = round(sum(processing_durations) / len(processing_durations))

        # Estimate daily capacity from actual completions
        total_tasks = len(collection_durations) + len(processing_durations)
        if total_tasks >= 5 and days_with_completions > 0:
            avg_tasks_per_day = total_tasks / days_with_completions
            avg_sec_per_task = (
                sum(collection_durations) + sum(processing_durations)
            ) / total_tasks
            capacity_min = min((avg_tasks_per_day * avg_sec_per_task * 1.2) / 60, 480)
            rate["dailyCapacityMinutes"] = round(max(capacity_min, 120))  # at least 2 hours

        _put_directed_user_rate(user, rate)
        return rate
    except Exception as e:
        print(f"[DIRECTED] Error computing rate for {user}: {e}")
        return dict(_DIRECTED_DEFAULT_RATE)


def _get_directed_log(user: str, date: dt.date) -> dict:
    """Get a user's directed plan log for a specific date."""
    sk = f"{date.isoformat()}#{user}"
    try:
        resp = ddb.get_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": "DIRECTED_LOG"}, "SK": {"S": sk}}
        )
        item = resp.get("Item")
        if item and item.get("data"):
            return json.loads(item["data"]["S"])
    except Exception:
        pass
    return {"completions": [], "incompletes": [], "planDate": date.isoformat(), "user": user,
            "totalPlanned": 0, "totalCompleted": 0, "totalIncomplete": 0}


def _put_directed_log(user: str, date: dt.date, log: dict):
    """Save a user's directed plan log for a specific date."""
    sk = f"{date.isoformat()}#{user}"
    try:
        ddb.put_item(
            TableName=CONFIG_TABLE,
            Item={
                "PK": {"S": "DIRECTED_LOG"},
                "SK": {"S": sk},
                "data": {"S": json.dumps(log, default=str)},
                "user": {"S": user},
                "plan_date": {"S": date.isoformat()},
                "updated_at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
            }
        )
    except Exception as e:
        print(f"[DIRECTED] Error saving log for {user}/{date}: {e}")


# -------- Directed Workflow: Queue Generation --------

_DIRECTED_REASON_CODES = [
    {"code": "portal_down", "label": "Portal/website was down"},
    {"code": "bill_not_available", "label": "Bill not yet available from provider"},
    {"code": "vendor_issue", "label": "Vendor issue (wrong account, closed, etc.)"},
    {"code": "ran_out_of_time", "label": "Ran out of time"},
    {"code": "system_error", "label": "System/app error prevented completion"},
    {"code": "reassigned", "label": "Task reassigned to someone else"},
    {"code": "other", "label": "Other (see notes)"},
]


def _compute_task_priority(urgency_score: int, is_house: bool, avg_amount: float, scraper_available: bool) -> int:
    """Compute priority score for a directed workflow task."""
    score = urgency_score * 100  # 0-400
    if is_house:
        score += 200  # house bonus
    score += min(avg_amount / 100, 50)  # amount bonus, capped at 50
    if scraper_available:
        score += 50  # easy win bonus
    return round(score)


def _bw_lookup(provider: str, account: str = "") -> dict:
    """Look up portal credentials via Bitwarden Lambda. Returns {exists, portal_url, username}."""
    try:
        payload = {"queryStringParameters": {"provider": provider, "account": account}}
        resp = _lambda_client.invoke(
            FunctionName=BW_LAMBDA_NAME,
            InvocationType='RequestResponse',
            Payload=json.dumps(payload)
        )
        result = json.loads(resp['Payload'].read())
        body = json.loads(result.get('body', '{}'))
        return body
    except Exception as e:
        print(f"[BW LOOKUP] Error for {provider}: {e}")
        return {"exists": False, "portal_url": "", "username": ""}


def _generate_directed_plan(user: str) -> dict:
    """Generate a daily work plan for the given user.
    Combines collection tasks (missing bills) and processing tasks (bills in pipeline).
    Returns plan dict saved to S3.
    """
    import uuid as _uuid
    today = dt.date.today()
    now_iso = dt.datetime.utcnow().isoformat() + "Z"

    # Step 1: Load inputs (parallel where possible)
    workflow_cache = _s3_get_workflow_cache()
    scraper_links = _s3_get_scraper_links()
    accounts = _get_accounts_to_track()
    user_rate = _compute_user_rate(user)

    # Build account lookup
    acct_by_key: dict[str, dict] = {}
    for acc in (accounts or []):
        if not isinstance(acc, dict) or acc.get("status") == "archived":
            continue
        pid = str(acc.get("propertyId") or "").strip()
        vid = str(acc.get("vendorId") or "").strip()
        acct = str(acc.get("accountNumber") or "").strip()
        key = f"{pid}|{vid}|{acct}"
        acct_by_key[key] = acc

    # Get workflow rows (urgency data)
    wf_rows = (workflow_cache or {}).get("rows", []) if workflow_cache else []
    wf_by_key: dict[str, dict] = {}
    for row in wf_rows:
        ak = row.get("accountKey", "")
        if ak:
            wf_by_key[ak] = row

    # Determine which accounts already have bills in the pipeline using workflow cache
    # (avoids scanning 30 days  2 stages  2 prefixes = 120+ S3 list calls)
    pipeline_account_keys: set[str] = set()
    for ak, wf_row in wf_by_key.items():
        stage = wf_row.get("lastBillStage", "")
        if stage in ("ENRICHED", "PENDING"):
            pipeline_account_keys.add(ak)

    # Scan recent Stage 4/6 for processing task details (S3 keys for deep links)
    # Limited to 7 days since older bills are stale for daily processing
    start_7 = today - dt.timedelta(days=7)
    stage_bills: list[dict] = []

    def _read_stage_bill(s3_key, sub_type):
        """Read first record from a stage JSONL file for processing task metadata."""
        try:
            resp = s3.get_object(Bucket=BUCKET, Key=s3_key)
            first_line = resp["Body"].read(524288).decode("utf-8", errors="ignore").split("\n")[0].strip()
            if not first_line:
                return None
            rec = json.loads(first_line)
            pid = str(rec.get("EnrichedPropertyID") or rec.get("Property Id") or "").strip()
            vid = str(rec.get("EnrichedVendorID") or rec.get("Vendor ID") or "").strip()
            acct = str(rec.get("Account Number") or "").strip()
            return {
                "s3Key": s3_key,
                "pdfId": hashlib.sha1(s3_key.encode()).hexdigest(),
                "accountKey": f"{pid}|{vid}|{acct}",
                "propertyName": str(rec.get("EnrichedPropertyName") or rec.get("Property Name") or ""),
                "vendorName": str(rec.get("EnrichedVendorName") or rec.get("Vendor Name") or ""),
                "subType": sub_type,
            }
        except Exception:
            return None

    stage_keys_and_types = []
    for prefix, sub_type in [(STAGE4_PREFIX, "review"), (STAGE6_PREFIX, "post")]:
        try:
            for s3_key in _iter_stage_objects(prefix, start_7, today):
                if s3_key.endswith('.jsonl'):
                    stage_keys_and_types.append((s3_key, sub_type))
        except Exception as e:
            print(f"[DIRECTED] Error listing {prefix}: {e}")

    if stage_keys_and_types:
        futures = [_GLOBAL_EXECUTOR.submit(_read_stage_bill, k, st) for k, st in stage_keys_and_types]
        try:
            for f in as_completed(futures, timeout=30):
                result = f.result()
                if result:
                    stage_bills.append(result)
                    pipeline_account_keys.add(result["accountKey"])
        except TimeoutError:
            print("[DIRECTED] Timeout reading stage bills  proceeding with partial results")

    # Step 2: Build collection tasks (accounts with urgency but no bill in pipeline)
    links_map = scraper_links.get("links", {})
    all_tasks: list[dict] = []

    for ak, wf_row in wf_by_key.items():
        urgency_score = wf_row.get("urgencyScore", 0)
        if urgency_score < 2:  # skip ON_TRACK and DUE_SOON
            continue
        if ak in pipeline_account_keys:
            continue  # bill already in pipeline, skip collection

        acc = acct_by_key.get(ak, {})
        is_house = bool(acc.get("is_house", True))
        avg_amount = 0.0
        # Try to get avg amount from workflow row
        try:
            avg_amount = float(wf_row.get("avgAmount") or wf_row.get("lastBillAmount") or 0)
        except (ValueError, TypeError):
            pass

        scraper_link = links_map.get(ak)
        scraper_available = bool(scraper_link)

        priority = _compute_task_priority(urgency_score, is_house, avg_amount, scraper_available)

        task = {
            "taskId": f"t_{_uuid.uuid4().hex[:8]}",
            "taskType": "collection",
            "accountKey": ak,
            "propertyId": str(acc.get("propertyId") or wf_row.get("propertyId") or ""),
            "propertyName": str(acc.get("propertyName") or wf_row.get("propertyName") or ""),
            "vendorId": str(acc.get("vendorId") or wf_row.get("vendorId") or ""),
            "vendorName": str(acc.get("vendorName") or wf_row.get("vendorName") or ""),
            "vendorCode": str(wf_row.get("vendorCode") or ""),
            "accountNumber": str(acc.get("accountNumber") or wf_row.get("accountNumber") or ""),
            "providerGroup": str(wf_row.get("vendorCode") or wf_row.get("vendorName") or "").lower(),
            "urgencyStatus": wf_row.get("status", ""),
            "urgencyScore": urgency_score,
            "daysOverdue": wf_row.get("daysOverdue", 0),
            "isHouse": is_house,
            "avgAmount": round(avg_amount, 2),
            "priorityScore": priority,
            "estimatedMinutes": 0,  # filled in step 6
            "source": {
                "scraperAvailable": scraper_available,
                "scraperPdfCount": 0,
                "bitwardenAvailable": None,
                "resolution": "scraper" if scraper_available else "unknown",
            },
            "status": "pending",
            "completedAt": None,
            "completedAction": None,
            "incompleteReason": None,
            "incompleteNotes": None,
        }
        all_tasks.append(task)

    # Step 3: Check scraper PDF availability (parallel)
    scraper_tasks = [t for t in all_tasks if t["source"]["scraperAvailable"]]
    if scraper_tasks:
        def _check_scraper_pdfs(task):
            ak = task["accountKey"]
            link = links_map.get(ak, {})
            uuid = link.get("scraperAccountUuid", "")
            provider = link.get("scraperProvider", "")
            acct_id = link.get("scraperAccountId", "")
            if uuid:
                prefix = f"{uuid}/bills/{provider}/{acct_id}/"
                try:
                    resp = s3.list_objects_v2(Bucket=SCRAPER_BUCKET, Prefix=prefix, MaxKeys=10)
                    pdfs = [o for o in resp.get("Contents", []) if o["Key"].lower().endswith('.pdf')]
                    task["source"]["scraperPdfCount"] = len(pdfs)
                except Exception:
                    pass

        with ThreadPoolExecutor(max_workers=10) as scraper_executor:
            futures = [scraper_executor.submit(_check_scraper_pdfs, t) for t in scraper_tasks]
            try:
                for f in as_completed(futures, timeout=30):
                    f.result()
            except TimeoutError:
                print("[DIRECTED] Timeout checking scraper PDFs  proceeding with partial results")

    # Step 4: Bitwarden resolution for non-scraper collection tasks (cached per provider)
    bw_cache: dict[str, dict] = {}
    for task in all_tasks:
        if task["taskType"] != "collection":
            continue
        if task["source"]["scraperAvailable"]:
            continue
        provider = task["providerGroup"]
        if provider not in bw_cache:
            bw_cache[provider] = _bw_lookup(provider)
        bw_result = bw_cache[provider]
        task["source"]["bitwardenAvailable"] = bw_result.get("exists", False)
        if bw_result.get("exists"):
            task["source"]["resolution"] = "portal"
            task["source"]["portalUrl"] = bw_result.get("portal_url", "")
            task["source"]["portalUsername"] = bw_result.get("username", "")
        else:
            task["source"]["resolution"] = "mail"

    # Step 5: Build processing tasks from pipeline bills
    for bill in stage_bills:
        ak = bill["accountKey"]
        wf_row = wf_by_key.get(ak, {})
        acc = acct_by_key.get(ak, {})
        is_house = bool(acc.get("is_house", True))
        urgency_score = wf_row.get("urgencyScore", 1)

        priority = _compute_task_priority(urgency_score, is_house, 0, False)

        task = {
            "taskId": f"t_{_uuid.uuid4().hex[:8]}",
            "taskType": "processing",
            "subType": bill["subType"],
            "s3Key": bill["s3Key"],
            "pdfId": bill["pdfId"],
            "accountKey": ak,
            "propertyId": str(acc.get("propertyId") or ""),
            "propertyName": bill["propertyName"],
            "vendorId": str(acc.get("vendorId") or ""),
            "vendorName": bill["vendorName"],
            "vendorCode": str(wf_row.get("vendorCode") or ""),
            "accountNumber": str(acc.get("accountNumber") or ""),
            "providerGroup": str(wf_row.get("vendorCode") or bill["vendorName"] or "").lower(),
            "urgencyStatus": wf_row.get("status", ""),
            "urgencyScore": urgency_score,
            "daysOverdue": wf_row.get("daysOverdue", 0),
            "isHouse": is_house,
            "avgAmount": 0,
            "priorityScore": priority,
            "estimatedMinutes": 0,
            "source": {"scraperAvailable": False, "scraperPdfCount": 0, "bitwardenAvailable": None, "resolution": "pipeline"},
            "status": "pending",
            "completedAt": None,
            "completedAction": None,
            "incompleteReason": None,
            "incompleteNotes": None,
        }
        all_tasks.append(task)

    # Step 6: Two-tier sort  house first (by provider), then vacant (by property batch)
    house_tasks = [t for t in all_tasks if t["isHouse"]]
    vacant_tasks = [t for t in all_tasks if not t["isHouse"]]

    house_tasks.sort(key=lambda t: (-t["priorityScore"] // 100, t["providerGroup"], -t["priorityScore"]))
    vacant_tasks.sort(key=lambda t: (t["propertyName"], t["providerGroup"], -t["priorityScore"]))

    sorted_tasks = house_tasks + vacant_tasks

    # Step 7: Estimate time and truncate to daily capacity
    daily_minutes = user_rate.get("dailyCapacityMinutes", 360)
    avg_coll = user_rate.get("avgCollectionSec", 180) / 60
    avg_proc = user_rate.get("avgProcessingSec", 60) / 60
    cumulative = 0.0
    plan_tasks: list[dict] = []

    for task in sorted_tasks:
        est = avg_coll if task["taskType"] == "collection" else avg_proc
        task["estimatedMinutes"] = round(est, 1)
        cumulative += est
        plan_tasks.append(task)
        if cumulative > daily_minutes * 1.2:
            break

    collection_count = sum(1 for t in plan_tasks if t["taskType"] == "collection")
    processing_count = len(plan_tasks) - collection_count

    plan = {
        "user": user,
        "planDate": today.isoformat(),
        "generatedAt": now_iso,
        "userRate": user_rate,
        "tasks": plan_tasks,
        "stats": {
            "totalTasks": len(plan_tasks),
            "collectionTasks": collection_count,
            "processingTasks": processing_count,
            "estimatedMinutes": round(cumulative),
            "completedCount": 0,
            "incompleteCount": 0,
        }
    }

    _s3_put_directed_plan(user, plan)
    print(f"[DIRECTED] Generated plan for {user}: {len(plan_tasks)} tasks ({collection_count} collection, {processing_count} processing), ~{round(cumulative)} min")
    return plan


def _s3_get_outlier_records() -> list:
    """Load outlier records from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=OUTLIER_RECORDS_KEY)
        data = json.loads(obj['Body'].read().decode('utf-8'))
        return data.get("outliers", [])
    except s3.exceptions.NoSuchKey:
        return []
    except Exception as e:
        print(f"[OUTLIER] Error loading outlier records: {e}")
        return []


def _s3_put_outlier_records(outliers: list) -> bool:
    """Save outlier records to S3."""
    try:
        data = {
            "outliers": outliers,
            "last_updated": dt.datetime.utcnow().isoformat() + "Z"
        }
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=OUTLIER_RECORDS_KEY,
            Body=json.dumps(data, indent=2),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[OUTLIER] Error saving outlier records: {e}")
        return False


# -------- Smart UBI Allocation Functions --------

def _s3_get_ubi_account_history() -> dict:
    """Load UBI account history from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=UBI_ACCOUNT_HISTORY_KEY)
        return json.loads(obj['Body'].read().decode('utf-8'))
    except s3.exceptions.NoSuchKey:
        return {"accounts": {}, "last_updated": None}
    except Exception as e:
        print(f"[UBI SUGGEST] Error loading account history: {e}")
        return {"accounts": {}, "last_updated": None}


def _s3_put_ubi_account_history(data: dict) -> bool:
    """Save UBI account history to S3."""
    try:
        data["last_updated"] = dt.datetime.utcnow().isoformat() + "Z"
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=UBI_ACCOUNT_HISTORY_KEY,
            Body=json.dumps(data, indent=2),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[UBI SUGGEST] Error saving account history: {e}")
        return False


def _calculate_ubi_period_from_date(date_val: dt.date) -> str:
    """Convert a date to UBI period format (MM/YYYY)."""
    return f"{date_val.month:02d}/{date_val.year}"


# Cache for last UBI periods from Stage 8 (5 min TTL)
_last_ubi_periods_cache = {"data": {}, "expires": 0}

def _parse_service_period_to_month(date_str: str) -> tuple:
    """Parse a date string like '11/01/2025' or '2025-11-01' to (year, month) tuple."""
    if not date_str:
        return None
    try:
        # Try MM/DD/YYYY format
        if "/" in date_str:
            parts = date_str.split("/")
            if len(parts) == 3:
                return (int(parts[2]), int(parts[0]))  # (year, month)
        # Try YYYY-MM-DD format
        elif "-" in date_str:
            parts = date_str.split("-")
            if len(parts) == 3:
                return (int(parts[0]), int(parts[1]))  # (year, month)
    except Exception:
        pass
    return None

def _get_last_ubi_periods_from_stage8() -> dict:
    """Scan Stage 8 to find the last assigned service period and UBI period per account.
    Returns dict of account_key -> {
        "last_service_month": (year, month),  # e.g. (2025, 11)
        "last_ubi_period": "02/2026"
    }
    """
    global _last_ubi_periods_cache
    import time
    now = time.time()

    # Return cached if not expired
    if _last_ubi_periods_cache["expires"] > now and _last_ubi_periods_cache["data"]:
        return _last_ubi_periods_cache["data"]

    print("[UBI SUGGEST] Scanning Stage 8 for last assigned periods...")
    start = time.time()

    account_data = {}  # account_key -> list of {service_month, ubi_period}

    try:
        # Scan all Stage 8 files
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=BUCKET, Prefix=UBI_ASSIGNED_PREFIX)

        all_keys = []
        for page in pages:
            for obj in page.get('Contents', []):
                if obj['Key'].endswith('.jsonl'):
                    all_keys.append(obj['Key'])

        print(f"[UBI SUGGEST] Found {len(all_keys)} Stage 8 files to scan")

        # Process files in parallel
        from concurrent.futures import ThreadPoolExecutor
        import re

        def process_stage8_file(key):
            local_accounts = {}
            try:
                # Extract timestamp from key for tiebreaker when same service month
                # Key format: ...filename_20251217T221100Z_20251217T221218Z.jsonl
                # The first timestamp is the assignment time
                file_timestamp = ""
                ts_match = re.search(r'_(\d{8}T\d{6}Z)_', key)
                if ts_match:
                    file_timestamp = ts_match.group(1)  # e.g., "20251217T221100Z"

                obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                for line in txt.splitlines():
                    if not line.strip():
                        continue
                    try:
                        rec = json.loads(line)
                        ubi_period = rec.get("ubi_period")
                        if not ubi_period:
                            continue

                        # Get service period start to determine the service month
                        service_start = rec.get("Bill Period Start", "")
                        service_end = rec.get("Bill Period End", "")
                        service_month = _parse_service_period_to_month(service_start)
                        if not service_month:
                            continue

                        prop_id = rec.get("EnrichedPropertyID", "")
                        vendor_id = rec.get("EnrichedVendorID", "")
                        acct_num = str(rec.get("Account Number", "")).strip()
                        account_key = f"{prop_id}|{vendor_id}|{acct_num}"

                        if account_key not in local_accounts:
                            local_accounts[account_key] = []
                        local_accounts[account_key].append({
                            "service_month": service_month,
                            "service_start": service_start,
                            "service_end": service_end,
                            "ubi_period": ubi_period,
                            "file_timestamp": file_timestamp  # For tiebreaker
                        })
                    except Exception:
                        continue
            except Exception as e:
                pass
            return local_accounts

        with ThreadPoolExecutor(max_workers=20) as executor:
            results = list(executor.map(process_stage8_file, all_keys))

        # Merge results
        for local_accounts in results:
            for account_key, entries in local_accounts.items():
                if account_key not in account_data:
                    account_data[account_key] = []
                account_data[account_key].extend(entries)

        # Find the latest service month per account and its corresponding UBI period
        # CRITICAL: When multiple files have the same service month (corrections/updates),
        # use file_timestamp as tiebreaker to get the most recent assignment
        result = {}
        for account_key, entries in account_data.items():
            if not entries:
                continue
            # Sort by service month (year, month) descending, then file_timestamp descending
            # This ensures most recent assignment wins when same service month
            sorted_entries = sorted(
                entries,
                key=lambda x: (x["service_month"], x.get("file_timestamp", "")),
                reverse=True
            )
            latest = sorted_entries[0]
            # Deduplicate assignments by (service_month, ubi_period) for duplicate detection
            seen_combos = set()
            all_assignments = []
            for e in sorted_entries:
                combo = (e["service_month"], e["ubi_period"])
                if combo not in seen_combos:
                    seen_combos.add(combo)
                    all_assignments.append({
                        "service_start": e.get("service_start", ""),
                        "service_end": e.get("service_end", ""),
                        "service_month": e["service_month"],
                        "ubi_period": e["ubi_period"],
                    })
            result[account_key] = {
                "last_service_month": latest["service_month"],
                "last_service_start": latest.get("service_start", ""),
                "last_service_end": latest.get("service_end", ""),
                "last_ubi_period": latest["ubi_period"],
                "all_assignments": all_assignments,
            }

        elapsed = time.time() - start
        print(f"[UBI SUGGEST] Scanned Stage 8 in {elapsed:.1f}s, found {len(result)} accounts with UBI history")

        # Cache for 5 minutes
        _last_ubi_periods_cache = {"data": result, "expires": now + 300}
        return result

    except Exception as e:
        print(f"[UBI SUGGEST] Error scanning Stage 8: {e}")
        return {}


def _get_next_ubi_period(current_period: str) -> str:
    """Given a UBI period like '01/2026', return the next month '02/2026'."""
    try:
        parts = current_period.split("/")
        month = int(parts[0])
        year = int(parts[1])

        if month == 12:
            return f"01/{year + 1}"
        else:
            return f"{month + 1:02d}/{year}"
    except Exception:
        return None


def _get_prev_ubi_period(current_period: str) -> str:
    """Given a UBI period like '02/2026', return the previous month '01/2026'."""
    try:
        parts = current_period.split("/")
        month = int(parts[0])
        year = int(parts[1])

        if month == 1:
            return f"12/{year - 1}"
        else:
            return f"{month - 1:02d}/{year}"
    except Exception:
        return None


def _calculate_ubi_suggestion(
    service_start: dt.date,
    service_end: dt.date,
    amount: float,
    account_history: dict = None
) -> dict:
    """Calculate UBI period suggestion based on SEQUENTIAL billing history.

    LOGIC: If the last bill for this account was assigned to 01/2026,
    suggest 02/2026 for the next bill. Service dates are unreliable,
    so we use the sequential approach based on assignment history.

    Returns:
        {
            "suggested_periods": [{"period": "02/2026", "amount": 100.00, "pct": 100}],
            "confidence": "high|medium|low",
            "reason": "...",
            "last_period": "01/2026",
            "spans_months": False
        }
    """
    # PRIMARY: Use sequential approach based on last assigned UBI period
    if account_history and account_history.get("lastUbiPeriods"):
        last_periods = account_history["lastUbiPeriods"]
        if last_periods:
            # Get the most recent (highest) period assigned
            # Periods are in MM/YYYY format, so sort by YYYY then MM
            sorted_periods = sorted(last_periods, key=lambda p: (int(p.split("/")[1]), int(p.split("/")[0])), reverse=True)
            last_period = sorted_periods[0]
            next_period = _get_next_ubi_period(last_period)

            if next_period:
                return {
                    "suggested_periods": [{
                        "period": next_period,
                        "days": 30,  # Assume standard month
                        "amount": round(amount, 2),
                        "pct": 100.0
                    }],
                    "confidence": "high",
                    "reason": f"Next sequential period after {last_period}",
                    "last_period": last_period,
                    "spans_months": False
                }

    # FALLBACK: No history - try to use current date to suggest
    # If we don't know what was last assigned, we can't reliably suggest
    # But we can suggest based on current month as a starting point
    today = dt.date.today()
    current_period = _calculate_ubi_period_from_date(today)

    # For new accounts, suggest current month's period
    return {
        "suggested_periods": [{
            "period": current_period,
            "days": 30,
            "amount": round(amount, 2),
            "pct": 100.0
        }],
        "confidence": "low",
        "reason": "No assignment history - defaulting to current month. Check previous bills.",
        "last_period": None,
        "spans_months": False
    }


def _update_ubi_account_history(
    account_key: str,
    bill_date: str,
    service_start: str,
    service_end: str,
    ubi_periods: list,
    amount: float
) -> bool:
    """Update UBI account history after an assignment."""
    try:
        history_data = _s3_get_ubi_account_history()
        accounts = history_data.get("accounts", {})

        # Get or create account entry
        if account_key not in accounts:
            accounts[account_key] = {
                "lastBillDate": None,
                "lastServiceEnd": None,
                "lastUbiPeriods": [],
                "avgServiceDays": None,
                "billHistory": []
            }

        acct = accounts[account_key]

        # Parse service dates for days calculation
        service_days = None
        if service_start and service_end:
            try:
                start_dt = _parse_date_any(service_start)
                end_dt = _parse_date_any(service_end)
                if start_dt and end_dt:
                    service_days = (end_dt - start_dt).days + 1
            except Exception:
                pass

        # Add to bill history (keep last 12)
        acct["billHistory"].append({
            "billDate": bill_date,
            "serviceStart": service_start,
            "serviceEnd": service_end,
            "ubiPeriods": ubi_periods,
            "amount": amount,
            "serviceDays": service_days
        })

        # Trim to last 12 entries
        acct["billHistory"] = acct["billHistory"][-12:]

        # Update rolling averages
        acct["lastBillDate"] = bill_date
        acct["lastServiceEnd"] = service_end
        acct["lastUbiPeriods"] = ubi_periods

        # Calculate average service days from history
        valid_days = [h["serviceDays"] for h in acct["billHistory"] if h.get("serviceDays")]
        if valid_days:
            acct["avgServiceDays"] = round(sum(valid_days) / len(valid_days), 1)

        history_data["accounts"] = accounts
        return _s3_put_ubi_account_history(history_data)

    except Exception as e:
        print(f"[UBI SUGGEST] Error updating history: {e}")
        return False


# -------- Vendor Correction Utility Functions --------
VENDOR_CORRECTIONS_KEY = os.getenv("VENDOR_CORRECTIONS_KEY", CONFIG_PREFIX + "vendor_corrections.json")


def _s3_get_vendor_corrections() -> dict:
    """Load vendor corrections data from S3."""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=VENDOR_CORRECTIONS_KEY)
        return json.loads(obj['Body'].read().decode('utf-8'))
    except Exception as e:
        error_str = str(e)
        if "NoSuchKey" in error_str or "does not exist" in error_str.lower() or "404" in error_str:
            return {"corrections": [], "last_updated": None}
        print(f"[VENDOR CORRECT] Error loading corrections: {e}")
        return {"corrections": [], "last_updated": None}


def _s3_put_vendor_corrections(data: dict) -> bool:
    """Save vendor corrections data to S3."""
    try:
        data["last_updated"] = dt.datetime.utcnow().isoformat() + "Z"
        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=VENDOR_CORRECTIONS_KEY,
            Body=json.dumps(data, indent=2),
            ContentType="application/json"
        )
        return True
    except Exception as e:
        print(f"[VENDOR CORRECT] Error saving corrections: {e}")
        return False


def _find_bills_for_account(property_id: str, account_number: str, days_back: int = 180) -> list:
    """Find all bills matching a property and account number across all stages."""
    from datetime import datetime, timedelta
    from concurrent.futures import ThreadPoolExecutor, as_completed

    account_number = str(account_number).strip()
    matching_bills = []

    # Stages to search
    stages = [
        ("Stage4", STAGE4_PREFIX),
        ("Stage6", STAGE6_PREFIX),
        ("Stage7", POST_ENTRATA_PREFIX),
        ("Stage8", UBI_ASSIGNED_PREFIX),
        ("Archive", HIST_ARCHIVE_PREFIX)
    ]

    # Build date prefixes
    prefixes_to_scan = []
    today = datetime.now()
    for i in range(days_back):
        d = today - timedelta(days=i)
        for stage_name, stage_prefix in stages:
            prefix = f"{stage_prefix}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append((stage_name, prefix))

    def search_prefix(args):
        stage_name, prefix = args
        results = []
        try:
            paginator = s3.get_paginator('list_objects_v2')
            s3_pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)
            for s3_page in s3_pages:
                for obj in s3_page.get('Contents', []):
                    key = obj['Key']
                    if not key.endswith('.jsonl'):
                        continue

                    try:
                        obj_data = s3.get_object(Bucket=BUCKET, Key=key)
                        txt = obj_data['Body'].read().decode('utf-8', errors='ignore')
                        lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]

                        for line in lines:
                            try:
                                rec = json.loads(line)
                                rec_acct = str(rec.get("Account Number", "")).strip()
                                rec_prop = str(rec.get("EnrichedPropertyID", "")).strip()

                                if rec_acct == account_number:
                                    # Match on account number
                                    if not property_id or rec_prop == property_id:
                                        results.append({
                                            "stage": stage_name,
                                            "s3_key": key,
                                            "account_number": rec_acct,
                                            "property_id": rec_prop,
                                            "property_name": rec.get("EnrichedPropertyName", ""),
                                            "vendor_id": rec.get("EnrichedVendorID", ""),
                                            "vendor_name": rec.get("EnrichedVendorName", ""),
                                            "raw_vendor_name": rec.get("Vendor Name", ""),
                                            "bill_date": rec.get("Bill Date", ""),
                                            "amount": rec.get("Line Item Charge", ""),
                                        })
                                        break  # One bill per file is enough
                            except json.JSONDecodeError:
                                continue
                    except Exception:
                        continue
        except Exception:
            pass
        return results

    # Search in parallel
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = [executor.submit(search_prefix, args) for args in prefixes_to_scan]
        for future in as_completed(futures):
            results = future.result()
            if results:
                matching_bills.extend(results)

    return matching_bills


async def _compare_vendors_with_gemini(vendor_a: str, vendor_b: str) -> dict:
    """Use Gemini AI to compare two vendor names."""
    try:
        import google.generativeai as genai
        from utils import get_secret

        secret = get_secret("gemini/matcher-keys")
        if not secret:
            return {"same": None, "confidence": 0, "reason": "Gemini API key not configured"}

        api_key = secret.get("api_key") or secret.get("GEMINI_API_KEY")
        if not api_key:
            return {"same": None, "confidence": 0, "reason": "Gemini API key not found"}

        genai.configure(api_key=api_key)
        model = genai.GenerativeModel("gemini-1.5-flash")

        prompt = f'''You are comparing utility vendor names to determine if they refer to the same company.

Vendor A: "{vendor_a}"
Vendor B: "{vendor_b}"

Consider:
- Abbreviations (SCE = Southern California Edison, WM = Waste Management)
- Common variations (SoCalGas = Southern California Gas Company)
- Parent/subsidiary relationships
- Spelling variations and typos

Return ONLY valid JSON with no markdown formatting:
{{"same": true or false, "confidence": 0.0 to 1.0, "reason": "brief explanation"}}'''

        response = model.generate_content(prompt)
        text = response.text.strip()

        # Clean up markdown if present
        if text.startswith("```"):
            text = text.split("\n", 1)[1]
        if text.endswith("```"):
            text = text.rsplit("```", 1)[0]
        text = text.strip()

        result = json.loads(text)
        return result

    except Exception as e:
        print(f"[VENDOR CORRECT] Gemini error: {e}")
        return {"same": None, "confidence": 0, "reason": f"Gemini error: {str(e)}"}


def _calculate_outlier_stats(amounts: list[float]) -> dict:
    """Calculate statistical measures for outlier detection."""
    import statistics

    n = len(amounts)
    if n == 0:
        return {"n": 0, "mean": 0, "std": 0, "median": 0, "method": "none"}

    mean = statistics.mean(amounts)
    median = statistics.median(amounts)

    if n >= 6:
        # Use standard deviation method
        std = statistics.stdev(amounts) if n > 1 else 0
        return {"n": n, "mean": mean, "std": std, "median": median, "method": "std_dev"}
    elif n >= 3:
        # Use percentage method (fallback)
        std = statistics.stdev(amounts) if n > 1 else 0
        return {"n": n, "mean": mean, "std": std, "median": median, "method": "percentage"}
    else:
        # Not enough data
        return {"n": n, "mean": mean, "std": 0, "median": median, "method": "insufficient"}


def _detect_outlier(amount: float, stats: dict, threshold_z: float = 3.0, threshold_pct: float = 50.0) -> dict | None:
    """Detect if an amount is an outlier based on statistics."""
    if stats["n"] < 3 or stats["method"] == "none":
        return None  # Not enough history to detect outliers

    if stats["method"] == "std_dev" and stats["std"] > 0:
        z_score = (amount - stats["mean"]) / stats["std"]
        if abs(z_score) > threshold_z:
            outlier_type = "SPIKE" if z_score > 0 else "DROP"
            return {
                "outlier_type": outlier_type,
                "z_score": round(z_score, 2),
                "method": "std_dev",
                "threshold": threshold_z,
                "reason": f"Amount ${amount:.2f} is {abs(z_score):.1f} std devs {'above' if z_score > 0 else 'below'} mean ${stats['mean']:.2f}"
            }
    elif stats["method"] == "percentage":
        pct_change = abs(amount - stats["median"]) / stats["median"] * 100 if stats["median"] > 0 else 0
        if pct_change > threshold_pct:
            outlier_type = "SPIKE" if amount > stats["median"] else "DROP"
            return {
                "outlier_type": outlier_type,
                "pct_change": round(pct_change, 1),
                "method": "percentage",
                "threshold": threshold_pct,
                "reason": f"Amount ${amount:.2f} is {pct_change:.0f}% {'above' if amount > stats['median'] else 'below'} median ${stats['median']:.2f}"
            }

    return None


def _get_account_bill_history(account_key: str, max_bills: int = 12) -> list[dict]:
    """Get bill history for an account from Stage 7 and Archive."""
    property_id, vendor_id, account_number = account_key.split("|")
    bills = []

    # Scan last 6 months of Stage 7 and Archive
    today = dt.date.today()
    prefixes_to_scan = []

    for i in range(180):  # Last 6 months
        d = today - dt.timedelta(days=i)
        y, m, day = d.strftime('%Y'), d.strftime('%m'), d.strftime('%d')
        prefixes_to_scan.append(f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={day}/")
        prefixes_to_scan.append(f"{HIST_ARCHIVE_PREFIX}yyyy={y}/mm={m}/dd={day}/")

    paginator = s3.get_paginator('list_objects_v2')
    seen_invoices = set()

    for prefix in prefixes_to_scan:
        if len(bills) >= max_bills:
            break
        try:
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get("Contents", []) or []:
                    if len(bills) >= max_bills:
                        break
                    key = obj.get("Key", "")
                    if not key.endswith('.jsonl'):
                        continue

                    try:
                        file_obj = s3.get_object(Bucket=BUCKET, Key=key)
                        content = file_obj['Body'].read().decode('utf-8', errors='ignore')

                        # Check first line for account match
                        first_line = content.split('\n')[0].strip()
                        if not first_line:
                            continue
                        rec = json.loads(first_line)

                        rec_acct = str(rec.get("Account Number", "") or rec.get("Line Item Account Number", "")).strip()
                        rec_prop = str(rec.get("EnrichedPropertyID", "")).strip()
                        rec_vendor = str(rec.get("EnrichedVendorID", "")).strip()

                        if rec_acct != account_number:
                            continue
                        if rec_prop != property_id:
                            continue
                        # Vendor can sometimes differ due to corrections, so be lenient

                        # Get invoice total
                        invoice_num = rec.get("Invoice Number", "")
                        if invoice_num in seen_invoices:
                            continue
                        seen_invoices.add(invoice_num)

                        # Sum all line items
                        total = 0.0
                        for line in content.strip().split('\n'):
                            if not line.strip():
                                continue
                            try:
                                line_rec = json.loads(line)
                                amt = line_rec.get("Line Item Charge") or line_rec.get("AMOUNT") or 0
                                try:
                                    total += float(str(amt).replace('$', '').replace(',', ''))
                                except (ValueError, TypeError):
                                    pass
                            except json.JSONDecodeError:
                                continue

                        bill_date = rec.get("Bill Date", "") or rec.get("Invoice Date", "")

                        bills.append({
                            "invoice": invoice_num,
                            "date": bill_date,
                            "amount": round(total, 2),
                            "s3_key": key
                        })

                    except Exception as e:
                        continue
        except Exception as e:
            continue

    # Sort by date descending
    bills.sort(key=lambda x: x.get("date", ""), reverse=True)
    return bills[:max_bills]


@app.get("/api/metrics/outliers")
def api_metrics_outliers(
    status: str = "",
    outlier_type: str = "",
    min_z: float = 0,
    user: str = Depends(require_user)
):
    """Get detected outliers with optional filtering."""
    try:
        outliers = _s3_get_outlier_records()

        # Filter by status
        if status:
            outliers = [o for o in outliers if o.get("status") == status]

        # Filter by type
        if outlier_type:
            outliers = [o for o in outliers if o.get("outlier_type") == outlier_type]

        # Filter by z-score
        if min_z > 0:
            outliers = [o for o in outliers if abs(o.get("z_score", 0)) >= min_z]

        # Sort by detection date (newest first)
        outliers.sort(key=lambda x: x.get("detected_at", ""), reverse=True)

        # Calculate summary
        summary = {
            "total": len(outliers),
            "spikes": len([o for o in outliers if o.get("outlier_type") == "SPIKE"]),
            "drops": len([o for o in outliers if o.get("outlier_type") == "DROP"]),
            "pending": len([o for o in outliers if o.get("status") == "pending"]),
            "reviewed": len([o for o in outliers if o.get("status") in ("resolved", "false_positive")])
        }

        return {"outliers": outliers[:100], "summary": summary}
    except Exception as e:
        print(f"[OUTLIER] Error getting outliers: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/metrics/outliers/{pdf_id}/review")
async def api_review_outlier(pdf_id: str, request: Request, user: str = Depends(require_user)):
    """Mark an outlier as reviewed."""
    try:
        body = await request.json()
        new_status = body.get("status", "resolved")
        notes = body.get("notes", "")

        if new_status not in ("resolved", "false_positive", "pending"):
            return JSONResponse({"error": "Invalid status"}, status_code=400)

        outliers = _s3_get_outlier_records()
        updated = False

        for o in outliers:
            if o.get("pdf_id") == pdf_id:
                o["status"] = new_status
                o["reviewed_by"] = user
                o["reviewed_at"] = dt.datetime.utcnow().isoformat() + "Z"
                o["review_notes"] = notes
                updated = True
                break

        if not updated:
            return JSONResponse({"error": "Outlier not found"}, status_code=404)

        _s3_put_outlier_records(outliers)
        return {"success": True}
    except Exception as e:
        print(f"[OUTLIER] Error reviewing outlier: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/account-stats/{account_key:path}")
def api_account_stats(account_key: str, user: str = Depends(require_user)):
    """Get historical stats for a specific account."""
    try:
        # Load bill history
        bills = _get_account_bill_history(account_key, max_bills=12)
        amounts = [b["amount"] for b in bills if b["amount"] > 0]

        stats = _calculate_outlier_stats(amounts)

        # Get outliers for this account
        all_outliers = _s3_get_outlier_records()
        account_outliers = [o for o in all_outliers if o.get("account_key") == account_key]

        return {
            "account_key": account_key,
            "history": bills,
            "stats": stats,
            "outliers": account_outliers
        }
    except Exception as e:
        print(f"[OUTLIER] Error getting account stats: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/metrics/outliers/scan")
async def api_scan_outliers(request: Request, background_tasks: BackgroundTasks, user: str = Depends(require_user)):
    """Scan recent bills for outliers. Runs in background."""
    try:
        body = await request.json() if request.headers.get("content-type") == "application/json" else {}
        days_back = body.get("days_back", 30)

        def scan_for_outliers(days: int):
            """Background task to scan for outliers."""
            print(f"[OUTLIER SCAN] Starting scan for last {days} days...")

            # Load accounts to track
            accounts = _get_accounts_to_track()
            account_keys = set()
            for a in accounts:
                key = f"{a.get('propertyId')}|{a.get('vendorId')}|{a.get('accountNumber')}"
                account_keys.add(key)

            print(f"[OUTLIER SCAN] Checking {len(account_keys)} tracked accounts...")

            # Load existing stats
            stats_data = _s3_get_account_statistics()
            existing_outliers = _s3_get_outlier_records()
            existing_pdf_ids = {o.get("pdf_id") for o in existing_outliers}

            new_outliers = []
            accounts_checked = 0

            # Scan recent Stage 7 bills
            today = dt.date.today()
            prefixes_to_scan = []
            for i in range(days):
                d = today - dt.timedelta(days=i)
                y, m, day = d.strftime('%Y'), d.strftime('%m'), d.strftime('%d')
                prefixes_to_scan.append(f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={day}/")

            paginator = s3.get_paginator('list_objects_v2')
            processed_invoices = set()

            for prefix in prefixes_to_scan:
                try:
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get("Contents", []) or []:
                            key = obj.get("Key", "")
                            if not key.endswith('.jsonl'):
                                continue

                            try:
                                file_obj = s3.get_object(Bucket=BUCKET, Key=key)
                                content = file_obj['Body'].read().decode('utf-8', errors='ignore')

                                first_line = content.split('\n')[0].strip()
                                if not first_line:
                                    continue
                                rec = json.loads(first_line)

                                # Build account key
                                rec_acct = str(rec.get("Account Number", "") or rec.get("Line Item Account Number", "")).strip()
                                rec_prop = str(rec.get("EnrichedPropertyID", "")).strip()
                                rec_vendor = str(rec.get("EnrichedVendorID", "")).strip()
                                account_key = f"{rec_prop}|{rec_vendor}|{rec_acct}"

                                # Skip if not tracked
                                if account_key not in account_keys:
                                    continue

                                # Skip duplicates
                                invoice_num = rec.get("Invoice Number", "")
                                if invoice_num in processed_invoices:
                                    continue
                                processed_invoices.add(invoice_num)

                                # Calculate pdf_id
                                pdf_id = hashlib.sha1(key.encode('utf-8')).hexdigest()

                                # Skip if already checked
                                if pdf_id in existing_pdf_ids:
                                    continue

                                # Sum line items
                                total = 0.0
                                for line in content.strip().split('\n'):
                                    if not line.strip():
                                        continue
                                    try:
                                        line_rec = json.loads(line)
                                        amt = line_rec.get("Line Item Charge") or line_rec.get("AMOUNT") or 0
                                        try:
                                            total += float(str(amt).replace('$', '').replace(',', ''))
                                        except (ValueError, TypeError):
                                            pass
                                    except json.JSONDecodeError:
                                        continue

                                if total <= 0:
                                    continue

                                # Get historical stats for this account
                                if account_key not in stats_data.get("accounts", {}):
                                    # Calculate stats from bill history
                                    bills = _get_account_bill_history(account_key, max_bills=12)
                                    # Exclude current bill from history
                                    hist_amounts = [b["amount"] for b in bills if b["invoice"] != invoice_num and b["amount"] > 0]
                                    stats = _calculate_outlier_stats(hist_amounts)
                                    stats_data.setdefault("accounts", {})[account_key] = {
                                        "stats": stats,
                                        "last_amount": total,
                                        "last_date": rec.get("Bill Date", "")
                                    }
                                else:
                                    stats = stats_data["accounts"][account_key].get("stats", {})

                                # Check for outlier
                                outlier_info = _detect_outlier(total, stats)
                                if outlier_info:
                                    accounts_checked += 1
                                    new_outliers.append({
                                        "pdf_id": pdf_id,
                                        "account_key": account_key,
                                        "property_name": rec.get("EnrichedPropertyName", ""),
                                        "vendor_name": rec.get("EnrichedVendorName", ""),
                                        "account_number": rec_acct,
                                        "invoice_number": invoice_num,
                                        "bill_date": rec.get("Bill Date", ""),
                                        "amount": round(total, 2),
                                        "historical_mean": round(stats.get("mean", 0), 2),
                                        "historical_std": round(stats.get("std", 0), 2),
                                        "historical_n": stats.get("n", 0),
                                        **outlier_info,
                                        "detected_at": dt.datetime.utcnow().isoformat() + "Z",
                                        "status": "pending",
                                        "s3_key": key
                                    })

                            except Exception as e:
                                continue
                except Exception as e:
                    continue

            # Save results
            if new_outliers:
                all_outliers = existing_outliers + new_outliers
                _s3_put_outlier_records(all_outliers)
                print(f"[OUTLIER SCAN] Found {len(new_outliers)} new outliers")

            _s3_put_account_statistics(stats_data)
            print(f"[OUTLIER SCAN] Scan complete. Checked {len(processed_invoices)} invoices, found {len(new_outliers)} outliers.")

        # Run in background
        background_tasks.add_task(scan_for_outliers, days_back)

        return {"status": "scanning", "message": f"Scanning last {days_back} days in background"}
    except Exception as e:
        print(f"[OUTLIER] Error starting scan: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/logins")
def api_metrics_logins(user: str = Depends(require_user)):
    """Get login statistics showing who logged in and when.

    Groups by user and shows login frequency. Cached for 10 minutes.
    """
    # Check cache first
    cache_key = ("metrics_logins",)
    now = time.time()
    cached = _CACHE.get(cache_key)
    if cached and (now - cached.get("ts", 0) < 600):  # 10 minute cache
        return cached.get("data")

    try:
        from zoneinfo import ZoneInfo
        utc = ZoneInfo("UTC")

        # Scan all login records
        all_logins = []
        paginator_token = None
        while True:
            scan_params = {
                "TableName": DRAFTS_TABLE,
                "FilterExpression": "begins_with(pk, :prefix)",
                "ExpressionAttributeValues": {
                    ":prefix": {"S": "login#"},
                },
            }
            if paginator_token:
                scan_params["ExclusiveStartKey"] = paginator_token

            response = ddb.scan(**scan_params)
            all_logins.extend(response.get("Items", []))

            paginator_token = response.get("LastEvaluatedKey")
            if not paginator_token:
                break

        # Process login records
        logins = []
        for item in all_logins:
            pk = item.get("pk", {}).get("S", "")
            if not pk.startswith("login#"):
                continue

            login_user = item.get("user", {}).get("S", "")
            login_utc = item.get("login_utc", {}).get("S", "")
            ip = item.get("ip", {}).get("S", "")

            if not login_user or not login_utc:
                continue

            logins.append({
                "user": login_user,
                "login_utc": login_utc,
                "ip": ip
            })

        # Group by user
        by_user = {}
        for login in logins:
            u = login["user"]
            if u not in by_user:
                by_user[u] = {
                    "user": u,
                    "logins": [],
                    "total_logins": 0
                }
            by_user[u]["logins"].append(login)
            by_user[u]["total_logins"] += 1

        # Calculate stats per user
        user_stats = []
        for u, data in by_user.items():
            # Sort logins by time
            data["logins"].sort(key=lambda x: x.get("login_utc", ""))

            # Group by date to show daily login pattern
            logins_by_date = {}
            for login in data["logins"]:
                login_time = login.get("login_utc", "")
                if login_time:
                    date_part = login_time[:10]  # YYYY-MM-DD
                    if date_part not in logins_by_date:
                        logins_by_date[date_part] = []
                    logins_by_date[date_part].append(login_time)

            # Calculate days active and average logins per day
            days_active = len(logins_by_date)
            avg_logins_per_day = round(data["total_logins"] / max(days_active, 1), 1)

            # Get first and last login
            first_login = data["logins"][0]["login_utc"] if data["logins"] else ""
            last_login = data["logins"][-1]["login_utc"] if data["logins"] else ""

            # Get recent logins (last 7 days)
            seven_days_ago = (dt.datetime.now(utc) - dt.timedelta(days=7)).isoformat()
            recent_logins = [l for l in data["logins"] if l.get("login_utc", "") >= seven_days_ago]

            # December activity (to check dgonzales claim)
            december_logins = [l for l in data["logins"] if l.get("login_utc", "").startswith("2024-12")]
            january_logins = [l for l in data["logins"] if l.get("login_utc", "").startswith("2025-01")]

            user_stats.append({
                "user": u,
                "total_logins": data["total_logins"],
                "days_active": days_active,
                "avg_logins_per_day": avg_logins_per_day,
                "first_login": first_login,
                "last_login": last_login,
                "recent_logins": len(recent_logins),
                "december_logins": len(december_logins),
                "january_logins": len(january_logins),
                "logins_by_date": logins_by_date
            })

        # Sort by total logins descending
        user_stats.sort(key=lambda x: x["total_logins"], reverse=True)

        # Calculate overall stats
        total_logins = sum(u["total_logins"] for u in user_stats)
        unique_users = len(user_stats)

        # Get date range
        all_dates = set()
        for u in user_stats:
            all_dates.update(u.get("logins_by_date", {}).keys())

        result = {
            "total_logins": total_logins,
            "unique_users": unique_users,
            "date_range": {
                "first": min(all_dates) if all_dates else "",
                "last": max(all_dates) if all_dates else ""
            },
            "user_stats": user_stats
        }

        # Cache the result
        _CACHE[cache_key] = {"ts": time.time(), "data": result}

        return result
    except Exception as e:
        import traceback
        print(f"[METRICS] Login stats error: {e}\n{traceback.format_exc()}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/metrics/job-log")
def api_metrics_job_log(limit: int = 100, stage: str = "", user: str = Depends(require_user)):
    """Get individual job log showing recent invoices with their status."""
    try:
        jobs = []
        today = dt.datetime.utcnow().date()

        # Define stages to check for each job's status
        stage_prefixes = {
            "in_review": (ENRICH_PREFIX, True),  # (prefix, is_date_partitioned)
            "posted": (STAGE6_PREFIX, True),
            "billback": (POST_ENTRATA_PREFIX, True),
            "pending_parsing": ("Bill_Parser_1_Pending_Parsing/", False),
            "rework": ("Bill_Parser_Rework_Input/", False),
            "failed": (FAILED_JOBS_PREFIX, False),
        }

        # If specific stage requested, only check that stage
        if stage and stage in stage_prefixes:
            check_stages = {stage: stage_prefixes[stage]}
        else:
            # Default: check review stage (Stage 4) for recent jobs
            check_stages = {"in_review": stage_prefixes["in_review"]}

        for stage_id, (prefix, is_partitioned) in check_stages.items():
            if is_partitioned:
                # Check last 7 days for partitioned stages
                for i in range(7):
                    if len(jobs) >= limit:
                        break
                    target_date = today - dt.timedelta(days=i)
                    y, m, d = target_date.strftime('%Y'), target_date.strftime('%m'), target_date.strftime('%d')
                    day_prefix = f"{prefix}yyyy={y}/mm={m}/dd={d}/"

                    try:
                        paginator = s3.get_paginator('list_objects_v2')
                        for page in paginator.paginate(Bucket=BUCKET, Prefix=day_prefix):
                            for obj in page.get("Contents", []) or []:
                                if len(jobs) >= limit:
                                    break
                                k = obj.get("Key", "")
                                if not k.endswith('.jsonl'):
                                    continue

                                filename = k.split("/")[-1] if "/" in k else k
                                mod_time = obj.get("LastModified")
                                age_hours = round((dt.datetime.now(dt.timezone.utc) - mod_time).total_seconds() / 3600, 1) if mod_time else None

                                # Extract job ID from filename (first part before first underscore or whole name)
                                job_id = filename.replace('.jsonl', '')

                                jobs.append({
                                    "job_id": job_id,
                                    "filename": filename,
                                    "stage": stage_id,
                                    "stage_name": {"in_review": "In Review", "posted": "Posted", "billback": "Billback", "pending_parsing": "Pending", "rework": "Rework", "failed": "Failed"}.get(stage_id, stage_id),
                                    "size": obj.get("Size", 0),
                                    "last_modified": mod_time.isoformat() if mod_time else "",
                                    "age_hours": age_hours,
                                    "date": str(target_date),
                                })
                    except Exception as e:
                        print(f"[JOB LOG] Error listing {day_prefix}: {e}")
            else:
                # Non-partitioned stages
                try:
                    paginator = s3.get_paginator('list_objects_v2')
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get("Contents", []) or []:
                            if len(jobs) >= limit:
                                break
                            k = obj.get("Key", "")
                            if k == prefix or '/diagnostics/' in k:
                                continue
                            if k.endswith('.json'):  # Skip metadata
                                continue
                            if not (k.endswith('.pdf') or k.endswith('.jsonl')):
                                continue

                            filename = k.split("/")[-1] if "/" in k else k
                            mod_time = obj.get("LastModified")
                            age_hours = round((dt.datetime.now(dt.timezone.utc) - mod_time).total_seconds() / 3600, 1) if mod_time else None

                            job_id = filename.rsplit('.', 1)[0] if '.' in filename else filename

                            jobs.append({
                                "job_id": job_id,
                                "filename": filename,
                                "stage": stage_id,
                                "stage_name": {"in_review": "In Review", "posted": "Posted", "billback": "Billback", "pending_parsing": "Pending", "rework": "Rework", "failed": "Failed"}.get(stage_id, stage_id),
                                "size": obj.get("Size", 0),
                                "last_modified": mod_time.isoformat() if mod_time else "",
                                "age_hours": age_hours,
                                "date": mod_time.strftime("%Y-%m-%d") if mod_time else "",
                            })
                except Exception as e:
                    print(f"[JOB LOG] Error listing {prefix}: {e}")

        # Sort by last_modified descending
        jobs.sort(key=lambda x: x.get("last_modified", ""), reverse=True)
        return {"jobs": jobs[:limit], "count": len(jobs[:limit])}
    except Exception as e:
        print(f"[METRICS] Job log error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- Performance Monitoring Endpoints --------

@app.get("/perf", response_class=HTMLResponse)
def perf_dashboard(request: Request, user: str = Depends(require_user)):
    """Performance monitoring dashboard (admin-only)."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/")
    return templates.TemplateResponse("perf.html", {"request": request, "user": user})


@app.get("/admin", response_class=HTMLResponse)
def page_admin(request: Request, user: str = Depends(require_user)):
    """Admin tools page for backfills and maintenance operations."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/")
    return templates.TemplateResponse("admin.html", {"request": request, "user": user})


@app.get("/api/perf/live")
def api_perf_live(minutes: int = 60, user: str = Depends(require_user)):
    """Get live performance data  raw recent requests + summary stats."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    cutoff = time.time() - (minutes * 60)
    with _PERF_LOG_LOCK:
        recent = [r for r in _PERF_LOG if r["ts"] >= cutoff]

    # Summary stats
    total = len(recent)
    if total == 0:
        return {"requests": [], "summary": {"total_requests": 0, "avg_ms": 0,
                "p50_ms": 0, "p95_ms": 0, "error_rate": 0, "slowest": [], "active_users": 0}}

    all_ms = sorted(r["ms"] for r in recent)
    errors = sum(1 for r in recent if r.get("status", 200) >= 500)
    users = set(r.get("user", "") for r in recent if r.get("user"))

    # Top 10 slowest endpoints by p95
    rollup = _perf_compute_rollup(recent)
    slowest = sorted(rollup.items(), key=lambda x: x[1].get("p95_ms", 0), reverse=True)[:10]

    return {
        "requests": list(recent)[-500:],  # Last 500 raw records
        "summary": {
            "total_requests": total,
            "avg_ms": round(sum(all_ms) / total, 1),
            "p50_ms": _perf_percentile(all_ms, 0.50),
            "p95_ms": _perf_percentile(all_ms, 0.95),
            "p99_ms": _perf_percentile(all_ms, 0.99),
            "error_rate": round(errors / total, 4),
            "active_users": len(users),
            "slowest": [{"path": ep, **stats} for ep, stats in slowest],
        }
    }


@app.get("/api/perf/rollups")
def api_perf_rollups(days: int = 7, user: str = Depends(require_user)):
    """Get hourly rollup data for performance charting."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    # Update current hour before returning
    _perf_update_current_hour()
    cutoff = (dt.datetime.utcnow() - dt.timedelta(days=days)).strftime("%Y-%m-%dT%H")
    hours = []
    with _PERF_ROLLUPS_LOCK:
        for hour_key in sorted(_PERF_ROLLUPS.keys()):
            if hour_key >= cutoff:
                hours.append({"hour": hour_key, "endpoints": _PERF_ROLLUPS[hour_key]})
    return {"hours": hours, "count": len(hours)}


@app.get("/api/perf/slow")
def api_perf_slow(threshold_ms: int = 3000, minutes: int = 60, user: str = Depends(require_user)):
    """Get requests slower than threshold."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    cutoff = time.time() - (minutes * 60)
    with _PERF_LOG_LOCK:
        slow = [r for r in _PERF_LOG if r["ts"] >= cutoff and r["ms"] >= threshold_ms]
    slow.sort(key=lambda r: r["ms"], reverse=True)
    return {"requests": slow[:200], "count": len(slow), "threshold_ms": threshold_ms}


@app.post("/api/admin/backfill-posted-metadata")
def api_admin_backfill_posted_metadata(user: str = Depends(require_user)):
    """Backfill posted invoice metadata into DynamoDB for existing Stage 7 + Archive invoices.
    This enables the fast DynamoDB path in CHECK REVIEW. Run once after deploying the metadata feature.
    """
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    import time as _time
    _start = _time.time()
    prefixes = [POST_ENTRATA_PREFIX, HIST_ARCHIVE_PREFIX]
    total_written = 0
    total_errors = 0
    total_skipped = 0

    for prefix in prefixes:
        try:
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    key = obj['Key']
                    if not key.endswith('.jsonl'):
                        continue
                    try:
                        pdf_id = hashlib.sha1(key.encode()).hexdigest()
                        resp = s3.get_object(Bucket=BUCKET, Key=key)
                        body = resp["Body"].read().decode("utf-8", errors="ignore")
                        rows = []
                        for line in body.strip().split("\n"):
                            line = line.strip()
                            if line:
                                try:
                                    rows.append(json.loads(line))
                                except Exception:
                                    pass
                        if rows:
                            _write_posted_invoice_metadata(pdf_id, key, rows)
                            total_written += 1
                        else:
                            total_skipped += 1
                    except Exception as e:
                        total_errors += 1
                        print(f"[BACKFILL] Error processing {key}: {e}")
        except Exception as e:
            print(f"[BACKFILL] Error listing {prefix}: {e}")

    elapsed = _time.time() - _start
    print(f"[BACKFILL] Complete: {total_written} written, {total_skipped} skipped, {total_errors} errors in {elapsed:.1f}s")
    return {
        "ok": True,
        "written": total_written,
        "skipped": total_skipped,
        "errors": total_errors,
        "elapsed_seconds": round(elapsed, 1)
    }


@app.post("/api/admin/backfill-late-fees")
def api_admin_backfill_late_fees(weeks: int = 6, user: str = Depends(require_user)):
    """Backfill late fee calculations for existing posted invoice records in DynamoDB.
    Only updates records that are missing the late_fee field or have it set to 0.
    Re-reads S3 files to calculate late fees from line item descriptions.

    Args:
        weeks: Number of weeks to backfill (default 6). Use 0 for all history.
    """
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)
    import time as _time
    _start = _time.time()

    updated = 0
    skipped = 0
    errors = 0

    # Calculate date range
    today = dt.date.today()
    if weeks > 0:
        oldest_date = today - dt.timedelta(weeks=weeks)
        oldest_str = oldest_date.isoformat()
    else:
        oldest_str = None

    # Query POSTED_INVOICES records with date filter
    kwargs = {
        "TableName": CONFIG_TABLE,
        "KeyConditionExpression": "PK = :pk",
        "ExpressionAttributeValues": {":pk": {"S": "POSTED_INVOICES"}},
    }
    # Add SK range filter if limiting to recent weeks
    if oldest_str:
        kwargs["KeyConditionExpression"] += " AND SK >= :start"
        kwargs["ExpressionAttributeValues"][":start"] = {"S": oldest_str}

    while True:
        resp = ddb.query(**kwargs)
        for item in resp.get("Items", []):
            try:
                # Check if late_fee already calculated
                existing_late_fee = float(item.get("late_fee", {}).get("N", "0") or "0")
                if existing_late_fee > 0:
                    skipped += 1
                    continue

                # Get S3 key and re-read file
                s3_key = item.get("s3_key", {}).get("S", "")
                if not s3_key:
                    skipped += 1
                    continue

                # Read S3 file
                try:
                    s3_resp = s3.get_object(Bucket=BUCKET, Key=s3_key)
                    body = s3_resp["Body"].read().decode("utf-8", errors="ignore")
                except s3.exceptions.NoSuchKey:
                    skipped += 1
                    continue
                except Exception:
                    skipped += 1
                    continue

                # Parse lines and calculate late fees
                late_fee = 0.0
                for line in body.strip().split("\n"):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        row = json.loads(line)
                        desc = str(row.get("Line Item Description") or row.get("line_item_description") or "").lower()
                        charge = row.get("Line Item Charge") or row.get("line_item_charge") or 0
                        try:
                            amt = float(str(charge).replace("$", "").replace(",", ""))
                        except (ValueError, TypeError):
                            continue

                        for pattern in LATE_FEE_PATTERNS:
                            if re.search(pattern, desc, re.IGNORECASE):
                                late_fee += abs(amt)
                                break
                    except json.JSONDecodeError:
                        continue

                # Update record with late_fee
                if late_fee > 0:
                    ddb.update_item(
                        TableName=CONFIG_TABLE,
                        Key={"PK": item["PK"], "SK": item["SK"]},
                        UpdateExpression="SET late_fee = :lf",
                        ExpressionAttributeValues={":lf": {"N": str(round(late_fee, 2))}}
                    )
                    updated += 1
                    print(f"[BACKFILL LATE FEE] Updated {s3_key}: ${late_fee:.2f}")
                else:
                    # Set to 0 explicitly so we don't reprocess
                    ddb.update_item(
                        TableName=CONFIG_TABLE,
                        Key={"PK": item["PK"], "SK": item["SK"]},
                        UpdateExpression="SET late_fee = :lf",
                        ExpressionAttributeValues={":lf": {"N": "0"}}
                    )
                    skipped += 1

            except Exception as e:
                errors += 1
                print(f"[BACKFILL LATE FEE] Error: {e}")

        if "LastEvaluatedKey" not in resp:
            break
        kwargs["ExclusiveStartKey"] = resp["LastEvaluatedKey"]

    elapsed = _time.time() - _start
    date_range = f"past {weeks} weeks" if weeks > 0 else "all history"
    print(f"[BACKFILL LATE FEE] Complete ({date_range}): {updated} updated, {skipped} skipped, {errors} errors in {elapsed:.1f}s")
    return {
        "ok": True,
        "weeks": weeks,
        "date_range": date_range,
        "updated": updated,
        "skipped": skipped,
        "errors": errors,
        "elapsed_seconds": round(elapsed, 1)
    }


@app.get("/api/failed/jobs")
def api_get_failed_jobs(user: str = Depends(require_user)):
    """Get list of failed parsing jobs from S3 with error info from .notes.json files."""
    try:
        jobs = []
        all_files = {}  # key -> obj info
        notes_files = {}  # base_name -> notes content
        paginator = s3.get_paginator('list_objects_v2')

        # List all files in failed jobs folder (including .notes.json)
        for page in paginator.paginate(Bucket=BUCKET, Prefix=FAILED_JOBS_PREFIX):
            for obj in page.get("Contents", []) or []:
                key = obj.get("Key", "")
                # Skip the folder itself
                if key == FAILED_JOBS_PREFIX or "/diagnostics/" in key:
                    continue
                filename = key.split("/")[-1] if "/" in key else key
                all_files[filename] = {"key": key, "obj": obj}

        # Find .notes.json and .error.json files and load their content
        for filename, info in all_files.items():
            if filename.endswith(".notes.json"):
                try:
                    notes_obj = s3.get_object(Bucket=BUCKET, Key=info["key"])
                    notes_data = json.loads(notes_obj["Body"].read().decode("utf-8"))
                    # Get the base name (remove .notes.json)
                    base_name = filename[:-11]  # Remove ".notes.json"
                    notes_files[base_name] = notes_data
                except Exception as e:
                    print(f"[FAILED JOBS] Error reading notes file {filename}: {e}")
            elif filename.endswith(".error.json"):
                try:
                    error_obj = s3.get_object(Bucket=BUCKET, Key=info["key"])
                    error_data = json.loads(error_obj["Body"].read().decode("utf-8"))
                    # Get the base name (remove .error.json)
                    base_name = filename[:-11]  # Remove ".error.json"
                    # Merge into notes_files format
                    notes_files[base_name] = {
                        "notes": error_data.get("error_message", ""),
                        "error_type": error_data.get("error_type", "PARSE_ERROR"),
                        "source": {"key": error_data.get("source_key", error_data.get("original_key", ""))},
                        "pipeline_stage": error_data.get("pipeline_stage", ""),
                        "failed_at": error_data.get("failed_at", ""),
                    }
                except Exception as e:
                    print(f"[FAILED JOBS] Error reading error file {filename}: {e}")

        # Build job list from PDF files
        for filename, info in all_files.items():
            # Only process PDF files (skip metadata files)
            if filename.endswith(".notes.json") or filename.endswith(".rework.json") or filename.endswith(".error.json"):
                continue
            # Must end with .pdf to be included
            if not filename.lower().endswith(".pdf"):
                continue

            key = info["key"]
            obj = info["obj"]

            # Generate presigned URL for PDF viewing
            pdf_url = ""
            try:
                pdf_url = s3.generate_presigned_url(
                    'get_object',
                    Params={'Bucket': BUCKET, 'Key': key},
                    ExpiresIn=3600  # 1 hour
                )
            except Exception:
                pass

            # Look for corresponding .notes.json file
            base_name = filename.rsplit(".", 1)[0] if "." in filename else filename
            notes = notes_files.get(base_name, {})

            # Extract error info from notes
            error_type = "REWORK" if "REWORK" in filename else "PARSE_ERROR"
            error_details = notes.get("notes", "")
            vendor = notes.get("Bill From", "")
            if vendor and error_details:
                error_details = f"[{vendor}] {error_details}"
            elif vendor:
                error_details = f"Vendor: {vendor}"

            jobs.append({
                "key": key,
                "filename": filename,
                "size": obj.get("Size", 0),
                "last_modified": obj.get("LastModified").isoformat() if obj.get("LastModified") else "",
                "pdf_url": pdf_url,
                "error_type": error_type,
                "error_details": error_details,
                "vendor": vendor,
            })
        # Sort by last_modified descending (newest first)
        jobs.sort(key=lambda x: x.get("last_modified", ""), reverse=True)
        return {"jobs": jobs, "count": len(jobs)}
    except Exception as e:
        print(f"[FAILED JOBS] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/failed/errors")
def api_get_parser_errors(limit: int = 100, user: str = Depends(require_user)):
    """Get parser errors from .notes.json and .error.json files in Failed Jobs folder."""
    try:
        items = []
        paginator = s3.get_paginator('list_objects_v2')

        # List all .notes.json and .error.json files in failed jobs folder
        for page in paginator.paginate(Bucket=BUCKET, Prefix=FAILED_JOBS_PREFIX):
            for obj in page.get("Contents", []) or []:
                key = obj.get("Key", "")
                filename = key.split("/")[-1] if "/" in key else key

                # Process .notes.json files (REWORK rejections)
                if filename.endswith(".notes.json"):
                    try:
                        notes_obj = s3.get_object(Bucket=BUCKET, Key=key)
                        notes_data = json.loads(notes_obj["Body"].read().decode("utf-8"))

                        # Get source PDF info
                        source = notes_data.get("source", {})
                        pdf_key = source.get("key", "")

                        # Generate presigned URL for PDF viewing
                        pdf_url = ""
                        if pdf_key:
                            try:
                                pdf_url = s3.generate_presigned_url(
                                    'get_object',
                                    Params={'Bucket': BUCKET, 'Key': pdf_key},
                                    ExpiresIn=3600
                                )
                            except Exception:
                                pass

                        items.append({
                            "pdf_key": pdf_key,
                            "pdf_url": pdf_url,
                            "error_type": "REWORK",
                            "error_details": notes_data.get("notes", ""),
                            "vendor": notes_data.get("Bill From", ""),
                            "timestamp": notes_data.get("generated_utc", ""),
                            "last_modified": obj.get("LastModified").isoformat() if obj.get("LastModified") else "",
                            "source_key": pdf_key,
                            "pipeline_stage": "rework",
                        })
                    except Exception as e:
                        print(f"[FAILED ERRORS] Error reading notes {filename}: {e}")

                # Process .error.json files (parser failures/timeouts)
                elif filename.endswith(".error.json"):
                    try:
                        error_obj = s3.get_object(Bucket=BUCKET, Key=key)
                        error_data = json.loads(error_obj["Body"].read().decode("utf-8"))

                        # Get PDF key from error data
                        source_key = error_data.get("source_key", "")
                        pdf_key = error_data.get("original_key", source_key)

                        # Generate presigned URL for PDF viewing
                        pdf_url = ""
                        if pdf_key:
                            try:
                                pdf_url = s3.generate_presigned_url(
                                    'get_object',
                                    Params={'Bucket': BUCKET, 'Key': pdf_key},
                                    ExpiresIn=3600
                                )
                            except Exception:
                                pass

                        items.append({
                            "pdf_key": pdf_key,
                            "pdf_url": pdf_url,
                            "error_type": error_data.get("error_type", "PARSE_ERROR"),
                            "error_details": error_data.get("error_message", ""),
                            "vendor": "",  # Not available in error files
                            "timestamp": error_data.get("failed_at", ""),
                            "last_modified": obj.get("LastModified").isoformat() if obj.get("LastModified") else "",
                            "source_key": source_key,
                            "pipeline_stage": error_data.get("pipeline_stage", "parser"),
                        })
                    except Exception as e:
                        print(f"[FAILED ERRORS] Error reading error file {filename}: {e}")

                if len(items) >= limit:
                    break
            if len(items) >= limit:
                break

        # Sort by last_modified descending (newest first)
        items.sort(key=lambda x: x.get("last_modified", ""), reverse=True)
        return {"errors": items, "count": len(items)}
    except Exception as e:
        print(f"[FAILED JOBS] Error fetching errors: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/failed/retry")
def api_retry_failed_job(key: str = Form(...), user: str = Depends(require_user)):
    """Retry a failed job by moving it back to the pending parsing queue."""
    try:
        # Validate key - must be from Failed_Jobs prefix
        _require_valid_s3_key(key, allowed_prefixes=("Bill_Parser_Failed_Jobs/",), operation="retry")
        # Copy the failed file back to pending parsing
        filename = key.split("/")[-1] if "/" in key else key
        new_key = f"{INPUT_PREFIX}{filename}"
        s3.copy_object(
            Bucket=BUCKET,
            CopySource={"Bucket": BUCKET, "Key": key},
            Key=new_key
        )
        # Delete the failed copy
        s3.delete_object(Bucket=BUCKET, Key=key)
        return {"success": True, "message": f"Moved to pending: {new_key}"}
    except Exception as e:
        print(f"[FAILED JOBS] Retry error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/failed/delete")
def api_delete_failed_job(key: str = Form(...), user: str = Depends(require_user)):
    """Delete a failed job from S3."""
    try:
        # Validate key - must be from Failed_Jobs prefix
        _require_valid_s3_key(key, allowed_prefixes=("Bill_Parser_Failed_Jobs/",), operation="delete")
        s3.delete_object(Bucket=BUCKET, Key=key)
        return {"success": True, "message": f"Deleted: {key}"}
    except Exception as e:
        print(f"[FAILED JOBS] Delete error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- Catalog + Config APIs --------
def _find_latest_data(prefix: str) -> str | None:
    paginator = s3.get_paginator("list_objects_v2")
    pages = paginator.paginate(Bucket=BUCKET, Prefix=prefix)
    best_key = None
    best_ts = None
    for page in pages:
        for o in page.get("Contents", []) or []:
            k = o.get("Key", "")
            if not (k.endswith("data.json") or k.endswith("data.json.gz")):
                continue
            lm = o.get("LastModified")
            if best_ts is None or (lm and lm > best_ts):
                best_ts = lm
                best_key = k
    return best_key


def _read_s3_text(bucket: str, key: str) -> str:
    obj = s3.get_object(Bucket=bucket, Key=key)
    raw = obj["Body"].read()
    if key.lower().endswith(".gz"):
        try:
            return gzip.decompress(raw).decode("utf-8", errors="ignore")
        except Exception:
            pass
    try:
        return raw.decode("utf-8", errors="ignore")
    except Exception:
        return raw.decode("latin-1", errors="ignore")


def _load_dim_records(prefix: str) -> list[dict]:
    # Try standardized filename first to avoid expensive S3 pagination
    standard_key = f"{prefix}latest.json.gz"
    try:
        txt = _read_s3_text(BUCKET, standard_key)
        print(f"[DIM LOAD] Using standard key: {standard_key}")
    except Exception:
        # Fall back to pagination to find latest file (for backward compatibility)
        print(f"[DIM LOAD] Standard key not found, falling back to pagination for prefix: {prefix}")
        key = _find_latest_data(prefix)
        if not key:
            return []
        txt = _read_s3_text(BUCKET, key)
    # Try JSON array or object with records
    try:
        parsed = json.loads(txt)
        if isinstance(parsed, list):
            return [r for r in parsed if isinstance(r, dict)]
        if isinstance(parsed, dict):
            recs = parsed.get("records")
            if isinstance(recs, list):
                return [r for r in recs if isinstance(r, dict)]
    except Exception:
        pass
    # Fallback: JSONL
    out = []
    for ln in txt.splitlines():
        ln = ln.strip()
        if not ln:
            continue
        try:
            r = json.loads(ln)
            if isinstance(r, dict):
                out.append(r)
        except Exception:
            continue
    if out:
        return out
    # Fallback: CSV (comma or tab)
    try:
        import csv
        from io import StringIO
        # pick delimiter by sniffing
        sample = "\n".join(txt.splitlines()[:5])
        dialect = csv.Sniffer().sniff(sample, delimiters=",\t|")
        reader = csv.DictReader(StringIO(txt), dialect=dialect)
        rows = []
        for i, row in enumerate(reader):
            if not row:
                continue
            rows.append({k.strip(): (v.strip() if isinstance(v, str) else v) for k, v in row.items()})
            if i >= 5000:
                break
        return rows
    except Exception:
        return []


def _s3_get_json(bucket: str, key: str) -> Any:
    try:
        obj = s3.get_object(Bucket=bucket, Key=key)
        body = obj["Body"].read().decode("utf-8", errors="ignore")
        return json.loads(body)
    except Exception:
        return None


def _s3_put_json(bucket: str, key: str, data: Any) -> None:
    body = (json.dumps(data, ensure_ascii=False) + "\n").encode("utf-8")
    s3.put_object(Bucket=bucket, Key=key, Body=body, ContentType="application/json")


# -------- GL Override helpers (cached to avoid repeated S3 reads per POST) --------
_POST_HELPER_CACHE: dict[str, dict] = {}  # key -> {"ts": float, "data": ...}
_POST_HELPER_TTL = 600  # 10 minutes

def _load_gl_number_to_id_map() -> dict:
    """Build a map from GL account number (formatted or raw) to Entrata GL Account ID using DIM_GL."""
    ck = "gl_num_to_id"
    cached = _POST_HELPER_CACHE.get(ck)
    if cached and (time.time() - cached["ts"] < _POST_HELPER_TTL):
        return cached["data"]
    recs = _load_dim_records(DIM_GL_PREFIX)
    m: dict[str, str] = {}
    for r in recs:
        # candidate ids
        gid = (
            r.get("glAccountId") or r.get("GL_ACCOUNT_ID") or r.get("id") or r.get("GlAccountId") or r.get("GL_ID")
        )
        if not gid:
            continue
        gid = str(gid).strip()
        # numbers
        num_raw = (
            r.get("glAccountNumber") or r.get("GL Account Number") or r.get("GL_ACCOUNT_NUMBER") or r.get("number") or r.get("ACCOUNT_NO") or r.get("GL_NUMBER") or r.get("ACCOUNT_NUMBER")
        )
        num_fmt = (
            r.get("formattedGlAccountNumber") or r.get("Formatted GL Account Number") or r.get("FORMATTED_GL_ACCOUNT_NUMBER") or r.get("FORMATTED_ACCOUNT_NUMBER") or r.get("formatted")
        )
        for n in [num_fmt, num_raw]:
            if n:
                key = str(n).strip()
                if key:
                    m[key] = gid
    _POST_HELPER_CACHE[ck] = {"ts": time.time(), "data": m}
    return m


def _load_gl_name_to_id_map() -> dict:
    """Build a map from GL account name to Entrata GL Account ID using DIM_GL."""
    ck = "gl_name_to_id"
    cached = _POST_HELPER_CACHE.get(ck)
    if cached and (time.time() - cached["ts"] < _POST_HELPER_TTL):
        return cached["data"]
    recs = _load_dim_records(DIM_GL_PREFIX)
    m: dict[str, str] = {}
    for r in recs:
        gid = (
            r.get("glAccountId") or r.get("GL_ACCOUNT_ID") or r.get("id") or r.get("GlAccountId") or r.get("GL_ID")
        )
        if not gid:
            continue
        gid = str(gid).strip()
        name = (
            r.get("glAccountName") or r.get("GL Account Name") or r.get("GL_ACCOUNT_NAME") or r.get("name") or r.get("NAME") or r.get("DESCRIPTION") or r.get("Account Name") or r.get("ACCOUNT_NAME") or r.get("GLAccountDescription") or r.get("DESCRIPTION_LONG")
        )
        if name:
            raw = str(name).strip()
            key = raw.upper()
            if key:
                m[key] = gid
    _POST_HELPER_CACHE[ck] = {"ts": time.time(), "data": m}
    return m

def _index_accounts_to_track_by_key() -> dict[tuple[str, str, str], dict]:
    """Return a dict keyed by (propertyId,vendorId,accountNumber) from Accounts-To-Track config rows."""
    ck = "att_index"
    cached = _POST_HELPER_CACHE.get(ck)
    if cached and (time.time() - cached["ts"] < _POST_HELPER_TTL):
        return cached["data"]
    base = _get_accounts_to_track()
    out: dict[tuple[str, str, str], dict] = {}
    if isinstance(base, list):
        for r in base:
            if not isinstance(r, dict):
                continue
            pid = str(r.get("propertyId") or "").strip()
            vid = str(r.get("vendorId") or "").strip()
            acct = str(r.get("accountNumber") or "").strip()
            if pid or vid or acct:
                out[(pid, vid, acct)] = r
    _POST_HELPER_CACHE[ck] = {"ts": time.time(), "data": out}
    return out


def _parse_date_any(s: str) -> dt.date | None:
    if not s:
        return None
    s = str(s).strip()
    fmts = ["%Y-%m-%d", "%m/%d/%Y", "%Y/%m/%d", "%m/%d/%y"]
    for f in fmts:
        try:
            return datetime.strptime(s, f).date()
        except Exception:
            pass
    return None


def _normalize_date_display(s: str) -> str:
    """Normalize any date string to MM/DD/YYYY format for consistent display."""
    if not s:
        return s
    d = _parse_date_any(s)
    if d:
        return d.strftime("%m/%d/%Y")
    return s  # Return as-is if unparseable


def _month_key(d: dt.date) -> str:
    return f"{d.year:04d}-{d.month:02d}"


def _iter_stage_objects(prefix_root: str, start: dt.date, end: dt.date):
    # Prefix layout supports both simple prefix (stage4) and y/m/d partition (stage6 has yyyy=...)
    # We'll try both yyyy=YYYY/mm=MM/dd=DD and YYYY/MM/DD patterns.
    # Limit to BUCKET
    cur = start
    while cur <= end:
        y = cur.year
        m = cur.month
        d = cur.day
        prefixes = [
            f"{prefix_root}yyyy={y}/mm={m:02d}/dd={d:02d}/",
            f"{prefix_root}{y}/{m:02d}/{d:02d}/",
        ]
        for p in prefixes:
            try:
                resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=p, MaxKeys=200)
                for obj in resp.get("Contents", []) or []:
                    yield obj["Key"]
            except Exception:
                pass
        cur += dt.timedelta(days=1)


def _read_first_record_from_s3(keys: list[str]) -> list[dict]:
    """Read only the first JSON record from each JSONL file (much faster for header info).
    Uses parallel threads for better performance with many files.
    """
    if not keys:
        return []

    def read_one(key: str) -> dict | None:
        try:
            # Use streaming to read first line - need larger buffer for files with embedded PDF
            obj = s3.get_object(Bucket=BUCKET, Key=key)
            # Read up to 512KB to handle files with embedded base64 PDF data
            chunk = obj["Body"].read(524288).decode("utf-8", errors="ignore")
            first_line = chunk.split("\n")[0].strip()
            if first_line:
                try:
                    return json.loads(first_line)
                except Exception:
                    pass
        except Exception:
            pass
        return None

    # Use ThreadPoolExecutor to parallelize S3 reads
    out = []
    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = {executor.submit(read_one, key): key for key in keys}
        for future in as_completed(futures):
            s3_key = futures[future]
            result = future.result()
            if result:
                result["__s3_key__"] = s3_key
                out.append(result)
    return out


def _read_json_records_from_s3(keys: list[str]) -> list[dict]:
    out = []
    for key in keys:
        try:
            obj = s3.get_object(Bucket=BUCKET, Key=key)
            body = obj["Body"].read()
            # jsonl or json
            txt = body.decode("utf-8", errors="ignore")
            if "\n" in txt:
                for line in txt.splitlines():
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        out.append(json.loads(line))
                    except Exception:
                        pass
            else:
                try:
                    data = json.loads(txt)
                    if isinstance(data, list):
                        out.extend([x for x in data if isinstance(x, dict)])
                    elif isinstance(data, dict):
                        out.append(data)
                except Exception:
                    pass
        except Exception:
            continue
    return out


# -------- Snowflake helpers --------
SNOWFLAKE_CREDENTIALS_CACHE = None

def _get_snowflake_credentials() -> dict | None:
    """Fetch Snowflake credentials from AWS Secrets Manager (with caching).
    Supports key-pair auth: if the config references a private_key_secret,
    the private key is fetched separately and decoded for Snowflake connector.
    """
    global SNOWFLAKE_CREDENTIALS_CACHE

    if SNOWFLAKE_CREDENTIALS_CACHE:
        return SNOWFLAKE_CREDENTIALS_CACHE

    try:
        secrets_client = boto3.client('secretsmanager', region_name='us-east-1')
        secret_name = 'jrk-bill-review/snowflake'

        response = secrets_client.get_secret_value(SecretId=secret_name)
        secret_string = response.get('SecretString')

        if not secret_string:
            print("[SNOWFLAKE] No secret string found")
            return None

        credentials = json.loads(secret_string)

        # If config references a separate private key secret, fetch and decode it
        pk_secret_name = credentials.get('private_key_secret')
        if pk_secret_name and not credentials.get('password'):
            try:
                pk_resp = secrets_client.get_secret_value(SecretId=pk_secret_name)
                pk_pem = pk_resp.get('SecretString', '')
                # Normalize: strip BOM (unicode + UTF-8 bytes), CRLF -> LF, whitespace
                pk_pem = pk_pem.lstrip('\ufeff').lstrip('\xef\xbb\xbf').strip()
                pk_pem = pk_pem.replace('\r\n', '\n').replace('\r', '\n')
                pk_bytes_raw = pk_pem.encode('ascii')
                if pk_bytes_raw:
                    from cryptography.hazmat.primitives import serialization
                    pk_obj = serialization.load_pem_private_key(pk_bytes_raw, password=None)
                    pk_bytes = pk_obj.private_bytes(
                        encoding=serialization.Encoding.DER,
                        format=serialization.PrivateFormat.PKCS8,
                        encryption_algorithm=serialization.NoEncryption(),
                    )
                    credentials['private_key'] = pk_bytes
                    print(f"[SNOWFLAKE] Loaded private key from {pk_secret_name}")
            except Exception as e:
                print(f"[SNOWFLAKE] Error loading private key: {e}")

        SNOWFLAKE_CREDENTIALS_CACHE = credentials
        print(f"[SNOWFLAKE] Loaded credentials for account: {credentials.get('account')}")
        return credentials

    except Exception as e:
        print(f"[SNOWFLAKE] Error fetching credentials: {e}")
        return None


def _snowflake_connect(credentials: dict):
    """Create a Snowflake connection using either password or key-pair auth."""
    connect_args = {
        'account': credentials.get('account'),
        'user': credentials.get('user'),
        'database': credentials.get('database'),
        'schema': credentials.get('schema'),
        'warehouse': credentials.get('warehouse'),
        'insecure_mode': True,  # Skip OCSP check (fails in containers)
    }
    if credentials.get('role'):
        connect_args['role'] = credentials['role']
    if credentials.get('private_key'):
        connect_args['private_key'] = credentials['private_key']
    elif credentials.get('password'):
        connect_args['password'] = credentials['password']
    return snowflake.connector.connect(**connect_args)


def _write_to_snowflake(batch_id: str, master_bills: list[dict], memo: str, run_date: str) -> tuple[bool, str, int]:
    """Write master bills to Snowflake. Returns (success, message, rows_inserted)"""
    try:
        credentials = _get_snowflake_credentials()
        if not credentials:
            return False, "Failed to load Snowflake credentials", 0

        # Connect to Snowflake
        conn = _snowflake_connect(credentials)

        cursor = conn.cursor()

        # Prepare data for batch insert (with Source_Type column)
        rows_to_insert = []
        for mb in master_bills:
            # Determine Source_Type from source line items
            source_type = None  # NULL = ACTUAL (existing behavior)
            if mb.get('has_non_actual'):
                entry_types = set()
                for sl in mb.get('source_line_items', []):
                    et = sl.get('entry_type', '')
                    if et:
                        entry_types.add(et)
                if entry_types:
                    has_actual = any(not sl.get('entry_type') for sl in mb.get('source_line_items', []))
                    source_type = "MIXED" if len(entry_types) > 1 or has_actual else entry_types.pop()

            row = (
                str(mb.get('property_id', '')),
                str(mb.get('ar_code_mapping', '')),
                str(mb.get('utility_name', '')),
                str(mb.get('utility_amount', 0)),  # Keep as string to match existing schema
                str(mb.get('billback_month_start', '')),
                str(mb.get('billback_month_end', '')),
                str(run_date),
                str(memo),
                str(batch_id),  # Batch_ID for traceability
                source_type  # Source_Type: NULL=ACTUAL, ACCRUAL, MANUAL, TRUE-UP, MIXED
            )
            rows_to_insert.append(row)

        # Insert into Snowflake (with Source_Type column)
        insert_sql = """
        INSERT INTO "_Master_Bills_Prod"
        ("Property_ID", "AR_Code_Mapping", "Utility_Name", "Utility_Amount",
         "Billback_Month_Start", "Billback_Month_End", "RunDate", "Memo", "Batch_ID",
         "Source_Type")
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """

        cursor.executemany(insert_sql, rows_to_insert)
        conn.commit()

        rows_inserted = len(rows_to_insert)

        cursor.close()
        conn.close()

        print(f"[SNOWFLAKE] Successfully inserted {rows_inserted} rows for batch {batch_id}")
        return True, f"Inserted {rows_inserted} rows", rows_inserted

    except Exception as e:
        print(f"[SNOWFLAKE] Error writing to Snowflake: {e}")
        import traceback
        traceback.print_exc()
        return False, str(e), 0


# -------- Accrual / Manual Entry Helpers --------

def _read_historical_from_snowflake(property_id: str, account_number: str, charge_code: str, utility_name: str) -> list[dict]:
    """Query Snowflake _Master_Bills_Prod for historical amounts matching property + charge code + utility."""
    try:
        credentials = _get_snowflake_credentials()
        if not credentials:
            return []

        conn = _snowflake_connect(credentials)

        cursor = conn.cursor()
        query = """
        SELECT "Billback_Month_Start", "Utility_Amount"
        FROM "_Master_Bills_Prod"
        WHERE "Property_ID" = %s
          AND "AR_Code_Mapping" = %s
          AND "Utility_Name" = %s
        ORDER BY "Billback_Month_Start" ASC
        """
        cursor.execute(query, (property_id, charge_code, utility_name))
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        results = []
        for row in rows:
            billback_start = str(row[0]) if row[0] else ""
            amount = float(row[1]) if row[1] else 0.0
            period = ""
            if billback_start:
                parts = billback_start.split("/")
                if len(parts) == 3:
                    period = f"{parts[0]}/{parts[2]}"
            results.append({"period": period, "amount": amount})

        print(f"[ACCRUAL] Snowflake returned {len(results)} historical records for {property_id}/{charge_code}/{utility_name}")
        return results

    except Exception as e:
        print(f"[ACCRUAL] Snowflake historical query error: {e}")
        return []


# -------- INVOICES_MAT Precomputed Cache --------

def _load_invoice_history_cache():
    """Load and index INVOICES_MAT data into in-memory cache for instant accrual lookups."""
    cache = _INVOICE_HISTORY_CACHE
    if cache.get("loading"):
        print("[INVOICE CACHE] Already loading, skipping")
        return

    cache["loading"] = True
    t0 = time.time()

    try:
        # Step 1: Build property_id -> property_code mapping from dim_property
        prop_code_map = {}
        try:
            dim_rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            for r in dim_rows:
                pid = str(
                    r.get("propertyId") or r.get("PROPERTY_ID") or r.get("Property ID")
                    or r.get("id") or r.get("PROPERTYID") or r.get("PROP_ID") or ""
                ).strip()
                pcode = str(
                    r.get("LOOKUP_CODE") or r.get("lookup_code") or r.get("LookupCode")
                    or r.get("PROPERTY_CODE") or r.get("code") or r.get("CODE") or ""
                ).strip()
                if pid and pcode:
                    prop_code_map[pid] = pcode
            print(f"[INVOICE CACHE] Built prop_code_map: {len(prop_code_map)} properties")
        except Exception as e:
            print(f"[INVOICE CACHE] Error loading dim_property: {e}")

        # Step 2: Query INVOICES_MAT (aggregated)
        credentials = _get_snowflake_credentials()
        if not credentials:
            print("[INVOICE CACHE] No Snowflake credentials, skipping")
            cache["loading"] = False
            return

        conn = _snowflake_connect(credentials)

        cursor = conn.cursor()
        query = """
        SELECT VENDOR_NAME, PROPERTY_CODE, PROPERTY_NAME,
               GL_ACCOUNT, GL_ACCOUNT_NAME, POST_MONTH,
               SUM(AMOUNT) as TOTAL_AMOUNT,
               COUNT(*) as LINE_COUNT,
               INVOICE_NUMBER
        FROM RAW.ENTRATA.INVOICES_MAT
        WHERE POST_MONTH >= '2024-01-01'
          AND AMOUNT IS NOT NULL
        GROUP BY VENDOR_NAME, PROPERTY_CODE, PROPERTY_NAME,
                 GL_ACCOUNT, GL_ACCOUNT_NAME, POST_MONTH,
                 INVOICE_NUMBER
        ORDER BY PROPERTY_CODE, VENDOR_NAME, POST_MONTH
        """
        cursor.execute(query)
        rows = cursor.fetchall()
        cursor.close()
        conn.close()

        # Step 3: Index into nested dicts (both vendor-level and account-level)
        data = {}  # {property_code: {vendor_name_lower: [records]}}
        account_data = {}  # {property_code: {account_number: [records]}}
        vendor_index = {}  # {property_code: [vendor_name_lower, ...]}
        row_count = 0
        account_count = 0

        for row in rows:
            vendor_name_raw = str(row[0] or "").strip()
            prop_code = str(row[1] or "").strip()
            gl_account = str(row[3] or "").strip()
            gl_name = str(row[4] or "").strip()
            post_month = row[5]  # date object
            total_amount = float(row[6]) if row[6] else 0.0
            line_count = int(row[7]) if row[7] else 0
            invoice_number_raw = str(row[8] or "").strip()

            if not vendor_name_raw or not prop_code or not post_month:
                continue

            # Convert POST_MONTH (date like 2025-01-01) to "2025-01"
            if hasattr(post_month, 'strftime'):
                month_key = post_month.strftime("%Y-%m")
            else:
                month_key = str(post_month)[:7]

            vendor_lower = vendor_name_raw.lower().strip()

            rec = {
                "month": month_key,
                "amount": total_amount,
                "gl_account": gl_account,
                "gl_name": gl_name,
                "vendor_raw": vendor_name_raw,
                "line_count": line_count,
            }

            # Vendor-level index (aggregated - same as before)
            if prop_code not in data:
                data[prop_code] = {}
            if vendor_lower not in data[prop_code]:
                data[prop_code][vendor_lower] = []
            data[prop_code][vendor_lower].append(rec)

            # Account-level index (extract account number from INVOICE_NUMBER)
            # Format: "5300134601885 12/24" or just "5300134601885"
            if invoice_number_raw:
                acct_num = invoice_number_raw.split()[0].strip() if " " in invoice_number_raw else invoice_number_raw.strip()
                if acct_num:
                    if prop_code not in account_data:
                        account_data[prop_code] = {}
                    if acct_num not in account_data[prop_code]:
                        account_data[prop_code][acct_num] = []
                    account_data[prop_code][acct_num].append(rec)
                    account_count += 1

            row_count += 1

        # Step 4: Build vendor index per property for fast scanning
        for prop_code, vendors in data.items():
            vendor_index[prop_code] = list(vendors.keys())

        # Update cache atomically
        with _INVOICE_HISTORY_LOCK:
            cache["data"] = data
            cache["account_data"] = account_data
            cache["prop_code_map"] = prop_code_map
            cache["vendor_index"] = vendor_index
            cache["last_refresh"] = time.time()

        elapsed = time.time() - t0
        vendor_count = sum(len(v) for v in vendor_index.values())
        unique_accounts = sum(len(a) for a in account_data.values())
        print(f"[INVOICE CACHE] Loaded {row_count} records, {len(data)} properties, {vendor_count} vendors, {unique_accounts} unique accounts in {elapsed:.1f}s")

    except Exception as e:
        print(f"[INVOICE CACHE] Error loading: {e}")
        import traceback
        traceback.print_exc()
    finally:
        cache["loading"] = False


# -------- Vendor Name Matching for INVOICES_MAT --------

# Tokens to strip when doing fuzzy vendor name matching
_VENDOR_STRIP_TOKENS = {"inc", "llc", "ltd", "co", "corp", "lp", "llp", "the", "of", "and", "dba"}

def _normalize_vendor_tokens(name: str) -> set:
    """Tokenize and normalize a vendor name for fuzzy matching."""
    # Split on spaces, commas, punctuation
    tokens = re.split(r'[\s,.\-/&()]+', name.lower().strip())
    # Remove empty strings and common suffixes
    return {t for t in tokens if t and t not in _VENDOR_STRIP_TOKENS}


def _match_vendor_in_invoices(property_id: str, vendor_name: str, account_number: str = "") -> dict | None:
    """Find the best matching vendor in INVOICES_MAT for a tracked account.

    Returns: {"matched_vendor": str, "match_type": str, "confidence": float, "history": list} or None
    """
    cache = _INVOICE_HISTORY_CACHE
    if not cache.get("data") or not vendor_name:
        return None

    # Resolve property_code from property_id
    prop_code = cache["prop_code_map"].get(property_id, "")
    if not prop_code:
        return None

    prop_data = cache["data"].get(prop_code)
    if not prop_data:
        return None

    vendor_list = cache["vendor_index"].get(prop_code, [])
    if not vendor_list:
        return None

    vendor_lower = vendor_name.lower().strip()
    best_match = None
    best_confidence = 0.0
    best_type = ""

    # Strategy a: Exact match (case-insensitive)
    if vendor_lower in prop_data:
        return {
            "matched_vendor": prop_data[vendor_lower][0]["vendor_raw"],
            "match_type": "exact",
            "confidence": 1.0,
            "history": prop_data[vendor_lower],
        }

    # Strategy b: Contains match (tracked name is substring of cached, or vice versa)
    for cached_vendor in vendor_list:
        if vendor_lower in cached_vendor or cached_vendor in vendor_lower:
            # Prefer the longer overlap (more specific match)
            overlap = min(len(vendor_lower), len(cached_vendor)) / max(len(vendor_lower), len(cached_vendor))
            confidence = 0.8 + (overlap * 0.15)
            if confidence > best_confidence:
                best_confidence = confidence
                best_match = cached_vendor
                best_type = "contains"

    if best_match and best_confidence >= 0.85:
        return {
            "matched_vendor": prop_data[best_match][0]["vendor_raw"],
            "match_type": best_type,
            "confidence": round(best_confidence, 3),
            "history": prop_data[best_match],
        }

    # Strategy c: Token overlap (Jaccard similarity)
    tracked_tokens = _normalize_vendor_tokens(vendor_name)
    if tracked_tokens:
        for cached_vendor in vendor_list:
            cached_tokens = _normalize_vendor_tokens(cached_vendor)
            if not cached_tokens:
                continue
            intersection = tracked_tokens & cached_tokens
            union = tracked_tokens | cached_tokens
            jaccard = len(intersection) / len(union) if union else 0
            if jaccard >= 0.5 and jaccard > best_confidence:
                best_confidence = jaccard
                best_match = cached_vendor
                best_type = "token"

    if best_match and best_type == "token":
        return {
            "matched_vendor": prop_data[best_match][0]["vendor_raw"],
            "match_type": best_type,
            "confidence": round(best_confidence, 3),
            "history": prop_data[best_match],
        }

    # Strategy d: Regex pattern from tracked vendor name tokens
    if tracked_tokens:
        # Build regex: join tokens with .* separator
        sorted_tokens = sorted(tracked_tokens, key=len, reverse=True)
        pattern_str = ".*".join(re.escape(t) for t in sorted_tokens[:4])  # limit to top 4 tokens
        try:
            pattern = re.compile(pattern_str, re.IGNORECASE)
            for cached_vendor in vendor_list:
                if pattern.search(cached_vendor):
                    # Score based on how much of the tracked name the regex covers
                    confidence = 0.5 + (len(tracked_tokens) * 0.1)
                    confidence = min(confidence, 0.75)
                    if confidence > best_confidence:
                        best_confidence = confidence
                        best_match = cached_vendor
                        best_type = "regex"
        except re.error:
            pass

    if best_match:
        return {
            "matched_vendor": prop_data[best_match][0]["vendor_raw"],
            "match_type": best_type,
            "confidence": round(best_confidence, 3),
            "history": prop_data[best_match],
        }

    return None


def _read_historical_from_invoices_mat(property_id: str, account_number: str, vendor_name: str) -> tuple[list[dict], dict | None]:
    """Read historical amounts from precomputed INVOICES_MAT cache.

    Priority: account-number match (exact per-account amounts) > vendor-name match (aggregated).
    Returns: (list of {"period": "MM/YYYY", "amount": float}, match_info dict or None)
    """
    cache = _INVOICE_HISTORY_CACHE
    history_records = None
    match_info = None

    # Priority 1: Direct account number lookup (gives per-account amounts)
    if account_number and cache.get("account_data"):
        prop_code = cache.get("prop_code_map", {}).get(property_id, "")
        if prop_code:
            acct_clean = account_number.strip()
            prop_accounts = cache["account_data"].get(prop_code, {})
            if acct_clean in prop_accounts:
                history_records = prop_accounts[acct_clean]
                # Get the vendor name from the records
                vendor_raw = history_records[0].get("vendor_raw", "") if history_records else ""
                match_info = {
                    "matched_vendor": vendor_raw,
                    "match_type": "account",
                    "confidence": 1.0,
                }

    # Priority 2: Vendor name fuzzy match (falls back to aggregated vendor-level data)
    if not history_records and vendor_name:
        match = _match_vendor_in_invoices(property_id, vendor_name, account_number)
        if match and match.get("history"):
            history_records = match["history"]
            match_info = {
                "matched_vendor": match["matched_vendor"],
                "match_type": match["match_type"],
                "confidence": match["confidence"],
            }

    if not history_records:
        return [], None

    # Aggregate by month (a vendor/account may have multiple GL accounts per month)
    month_totals = {}
    for rec in history_records:
        month_key = rec["month"]  # "2025-01"
        month_totals[month_key] = month_totals.get(month_key, 0.0) + rec["amount"]

    # Convert "2025-01" to "01/2025" period format
    results = []
    for month_key in sorted(month_totals.keys()):
        parts = month_key.split("-")
        if len(parts) == 2:
            period = f"{parts[1]}/{parts[0]}"  # "01/2025"
        else:
            period = month_key
        results.append({"period": period, "amount": month_totals[month_key]})

    # Extract GL info from the most recent record (for display)
    gl_counts = {}
    for rec in history_records:
        gl_key = (rec.get("gl_account", ""), rec.get("gl_name", ""))
        gl_counts[gl_key] = gl_counts.get(gl_key, 0) + 1
    if gl_counts:
        top_gl = max(gl_counts, key=gl_counts.get)
        match_info["gl_account"] = top_gl[0]
        match_info["gl_name"] = top_gl[1]

    return results, match_info


def _get_historical_from_assignments(property_id: str, account_number: str, vendor_name: str) -> list[dict]:
    """Fallback: scan jrk-bill-ubi-assignments and load S3 files to find historical amounts."""
    try:
        response = ddb.scan(TableName="jrk-bill-ubi-assignments")
        assignments = response.get("Items", [])
        while "LastEvaluatedKey" in response:
            response = ddb.scan(
                TableName="jrk-bill-ubi-assignments",
                ExclusiveStartKey=response["LastEvaluatedKey"]
            )
            assignments.extend(response.get("Items", []))

        s3_cache = {}
        period_amounts = {}

        for assignment in assignments:
            s3_key = assignment.get("s3_key", {}).get("S", "")
            line_hash = assignment.get("line_hash", {}).get("S", "")
            ubi_period = assignment.get("ubi_period", {}).get("S", "")
            amount = float(assignment.get("amount", {}).get("N", "0"))

            if not s3_key or not line_hash:
                continue

            if s3_key not in s3_cache:
                try:
                    obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                    body = obj["Body"].read()
                    if s3_key.endswith('.gz'):
                        import gzip
                        body = gzip.decompress(body)
                    s3_cache[s3_key] = body.decode('utf-8')
                except Exception:
                    continue

            for line_str in s3_cache[s3_key].strip().split("\n"):
                try:
                    parsed = json.loads(line_str)
                    computed_hash = _compute_stable_line_hash(parsed)
                    if computed_hash != line_hash:
                        continue
                    p_id = parsed.get("EnrichedPropertyID", parsed.get("Property ID", ""))
                    acc_num = parsed.get("Account Number", parsed.get("AccountNumber", ""))
                    if p_id == property_id and acc_num == account_number:
                        period_key = ubi_period.split(" to ")[0].strip() if ubi_period else ""
                        if period_key:
                            period_amounts[period_key] = period_amounts.get(period_key, 0) + amount
                except Exception:
                    continue

        results = [{"period": p, "amount": a} for p, a in sorted(period_amounts.items())]
        print(f"[ACCRUAL] Assignments fallback returned {len(results)} historical records for {property_id}/{account_number}")
        return results

    except Exception as e:
        print(f"[ACCRUAL] Assignments historical query error: {e}")
        return []


def _calculate_accrual(historical_amounts: list[dict], annual_inflation_rate: float = 0.03) -> dict:
    """Calculate accrual from historical amounts with annual inflation adjustment."""
    if not historical_amounts:
        return {
            "calculated_amount": 0, "historical_months": 0,
            "avg_amount": 0, "inflation_amount": 0, "monthly_amounts": []
        }

    amounts = [h["amount"] for h in historical_amounts if h.get("amount", 0) != 0]
    if not amounts:
        return {
            "calculated_amount": 0, "historical_months": len(historical_amounts),
            "avg_amount": 0, "inflation_amount": 0, "monthly_amounts": historical_amounts
        }

    avg = sum(amounts) / len(amounts)
    monthly_inflation = annual_inflation_rate / 12
    calculated = round(avg * (1 + monthly_inflation), 2)
    inflation_amount = round(calculated - avg, 2)

    return {
        "calculated_amount": calculated, "historical_months": len(amounts),
        "avg_amount": round(avg, 2), "inflation_amount": inflation_amount,
        "monthly_amounts": historical_amounts
    }


# -------- S3 helpers for large data (master bills) --------
MASTER_BILLS_S3_KEY = "config/master-bills-latest.json"

def _s3_get_master_bills() -> list[dict]:
    """Load master bills from S3 (handles large datasets)"""
    try:
        obj = s3.get_object(Bucket=BUCKET, Key=MASTER_BILLS_S3_KEY)
        data = json.loads(obj["Body"].read().decode("utf-8"))
        print(f"[S3 GET] Loaded {len(data) if isinstance(data, list) else 0} master bills from S3")
        return data if isinstance(data, list) else []
    except Exception as e:
        error_str = str(e)
        if "NoSuchKey" in error_str or "does not exist" in error_str.lower() or "404" in error_str:
            print(f"[S3 GET] Master bills file not found, returning empty list")
        else:
            print(f"[S3 GET] Error loading master bills: {e}")
        return []

def _s3_put_master_bills(master_bills: list[dict]) -> bool:
    """Save master bills to S3 (no size limit)"""
    try:
        data_json = json.dumps(master_bills, ensure_ascii=False)
        data_size = len(data_json.encode('utf-8'))
        print(f"[S3 PUT] Saving {len(master_bills)} master bills, {data_size} bytes")

        s3.put_object(
            Bucket=BUCKET,
            Key=MASTER_BILLS_S3_KEY,
            Body=data_json.encode("utf-8"),
            ContentType="application/json"
        )
        print(f"[S3 PUT] Successfully saved master bills to S3")
        return True
    except Exception as e:
        print(f"[S3 PUT] ERROR saving master bills: {e}")
        import traceback
        traceback.print_exc()
        return False


# -------- S3-primary storage for accounts-to-track (no DynamoDB size limits) --------
# DynamoDB has a 400KB item limit which accounts-to-track has exceeded.
# S3 is now the primary storage; DynamoDB is optional cache for small configs.
DDB_SIZE_LIMIT = 350000  # 350KB threshold - skip DynamoDB if larger to avoid failures

def _s3_get_accounts_to_track() -> list[dict]:
    """Load accounts-to-track from S3 (primary storage, no size limit)"""
    try:
        obj = s3.get_object(Bucket=CONFIG_BUCKET, Key=ACCOUNTS_TRACK_KEY)
        data = json.loads(obj["Body"].read().decode("utf-8"))
        count = len(data) if isinstance(data, list) else 0
        print(f"[S3 GET] Loaded {count} accounts-to-track from S3")
        return data if isinstance(data, list) else []
    except Exception as e:
        error_str = str(e)
        if "NoSuchKey" in error_str or "does not exist" in error_str.lower() or "404" in error_str:
            print(f"[S3 GET] accounts-to-track file not found, returning empty list")
        else:
            print(f"[S3 GET] Error loading accounts-to-track: {e}")
        return []

def _s3_put_accounts_to_track(arr: list[dict]) -> bool:
    """Save accounts-to-track to S3 (primary storage, no size limit)"""
    try:
        data_json = json.dumps(arr, ensure_ascii=False)
        data_size = len(data_json.encode('utf-8'))
        print(f"[S3 PUT] Saving {len(arr)} accounts-to-track, {data_size} bytes")

        s3.put_object(
            Bucket=CONFIG_BUCKET,
            Key=ACCOUNTS_TRACK_KEY,
            Body=data_json.encode("utf-8"),
            ContentType="application/json"
        )
        print(f"[S3 PUT] Successfully saved accounts-to-track to S3")
        return True
    except Exception as e:
        print(f"[S3 PUT] ERROR saving accounts-to-track: {e}")
        import traceback
        traceback.print_exc()
        return False

def _get_accounts_to_track() -> list[dict]:
    """Get accounts-to-track: S3 primary, DynamoDB fallback"""
    # Try S3 first (primary storage)
    data = _s3_get_accounts_to_track()
    if data:
        return data
    # Fallback to DynamoDB (for backwards compatibility during migration)
    ddb_data = _ddb_get_config("accounts-to-track")
    if ddb_data:
        print(f"[ACCOUNTS] Loaded from DynamoDB fallback: {len(ddb_data)} items")
        return ddb_data
    return []

def _put_accounts_to_track(arr: list[dict]) -> bool:
    """Save accounts-to-track: S3 primary, DynamoDB cache if small enough"""
    # SAFETY: Deduplicate before saving to prevent duplicate entries
    seen_keys = set()
    deduped = []
    dup_count = 0
    for item in arr:
        # Build composite key from camelCase or snake_case field names (handle both)
        pid = str(item.get("propertyId") or item.get("property_id") or "").strip()
        vid = str(item.get("vendorId") or item.get("vendor_id") or "").strip()
        acct = str(item.get("accountNumber") or item.get("account_number") or "").strip()
        key = f"{pid}|{vid}|{acct}"
        if key in seen_keys:
            dup_count += 1
            continue  # Skip duplicate
        seen_keys.add(key)
        deduped.append(item)
    if dup_count > 0:
        print(f"[ACCOUNTS] Removed {dup_count} duplicate entries before saving")
    arr = deduped

    # Always save to S3 first (primary storage, no size limit)
    s3_ok = _s3_put_accounts_to_track(arr)
    if not s3_ok:
        print(f"[ACCOUNTS] CRITICAL: Failed to save to S3!")
        return False

    # Optionally cache in DynamoDB if data is small enough (under 350KB)
    data_size = len(json.dumps(arr, ensure_ascii=False).encode('utf-8'))
    if data_size < DDB_SIZE_LIMIT:
        try:
            _ddb_put_config("accounts-to-track", arr)
            print(f"[ACCOUNTS] Also cached in DynamoDB ({data_size} bytes)")
        except Exception as e:
            # DynamoDB cache failure is non-fatal since S3 succeeded
            print(f"[ACCOUNTS] DynamoDB cache failed (non-fatal): {e}")
    else:
        print(f"[ACCOUNTS] Skipping DynamoDB cache - data too large ({data_size} bytes > {DDB_SIZE_LIMIT})")

    return True


# -------- DynamoDB helpers for config --------
def _ddb_get_config(config_id: str) -> list[dict] | None:
    try:
        resp = ddb.get_item(
            TableName=CONFIG_TABLE,
            Key={
                "PK": {"S": f"CONFIG#{config_id}"},
                "SK": {"S": "v1"}
            }
        )
        if "Item" not in resp:
            return None
        item = resp["Item"]
        data_str = item.get("Data", {}).get("S") or item.get("Data", {}).get("S")
        if not data_str:
            return None
        parsed = json.loads(data_str)
        return parsed if isinstance(parsed, list) else None
    except Exception:
        return None


def _ddb_put_config(config_id: str, arr: list[dict]) -> bool:
    try:
        data_json = json.dumps(arr, ensure_ascii=False)
        data_size = len(data_json.encode('utf-8'))
        print(f"[DDB PUT CONFIG] Saving config '{config_id}' with {len(arr)} items, {data_size} bytes")

        if data_size > 400000:
            print(f"[DDB PUT CONFIG] WARNING: Data size {data_size} bytes may exceed DynamoDB limit!")

        ddb.put_item(
            TableName=CONFIG_TABLE,
            Item={
                "PK": {"S": f"CONFIG#{config_id}"},
                "SK": {"S": "v1"},
                "UpdatedAt": {"S": datetime.utcnow().isoformat() + "Z"},
                "Data": {"S": data_json}
            }
        )
        print(f"[DDB PUT CONFIG] Successfully saved config '{config_id}'")
        return True
    except Exception as e:
        print(f"[DDB PUT CONFIG] ERROR saving config '{config_id}': {e}")
        import traceback
        traceback.print_exc()
        return False


def _ddb_get_draft(bill_id: str) -> dict | None:
    """Load bill draft from S3 JSONL file"""
    try:
        # bill_id is the S3 key
        obj = s3.get_object(Bucket=BUCKET, Key=bill_id)
        body = obj["Body"].read().decode("utf-8")

        # Parse JSONL (each line is a JSON record)
        line_data = []
        for line in body.strip().split("\n"):
            if line.strip():
                try:
                    rec = json.loads(line)
                    line_data.append(rec)
                except Exception:
                    pass

        return {
            "bill_id": bill_id,
            "line_data": line_data,
            "updated_utc": datetime.utcnow().isoformat(),
            "updated_by": ""
        }
    except Exception as e:
        print(f"[_ddb_get_draft] Error loading {bill_id}: {e}")
        return None


def _ddb_put_draft(draft: dict) -> bool:
    """Save bill draft back to S3 JSONL file"""
    try:
        bill_id = draft.get("bill_id", "")
        line_data = draft.get("line_data", [])

        if not bill_id:
            return False

        # Convert to JSONL (one JSON record per line)
        jsonl_lines = [json.dumps(rec, ensure_ascii=False) for rec in line_data]
        jsonl_content = "\n".join(jsonl_lines)

        # Write back to S3
        s3.put_object(
            Bucket=BUCKET,
            Key=bill_id,
            Body=jsonl_content.encode("utf-8"),
            ContentType="application/x-ndjson"
        )
        return True
    except Exception as e:
        print(f"[_ddb_put_draft] Error saving {draft.get('bill_id', 'unknown')}: {e}")
        return False


# -------- Check Slips DynamoDB Helpers --------

def _generate_check_slip_id() -> str:
    """Generate unique check slip ID."""
    import uuid
    return f"cs-{uuid.uuid4().hex[:12]}"


def _ddb_create_check_slip(slip: dict) -> bool:
    """Create a new check slip in DynamoDB."""
    try:
        ddb.put_item(
            TableName=CHECK_SLIPS_TABLE,
            Item={
                "check_slip_id": {"S": slip["check_slip_id"]},
                "created_date": {"S": slip["created_date"]},
                "created_at": {"S": slip["created_at"]},
                "created_by": {"S": slip["created_by"]},
                "vendor_id": {"S": slip["vendor_id"]},
                "vendor_name": {"S": slip["vendor_name"]},
                "vendor_code": {"S": slip.get("vendor_code", "")},
                "total_amount": {"N": str(slip["total_amount"])},
                "invoice_count": {"N": str(slip["invoice_count"])},
                "invoices": {"S": json.dumps(slip["invoices"])},  # Store as JSON string
                "status": {"S": slip["status"]},
                "approved_by": {"S": slip.get("approved_by", "")},
                "approved_at": {"S": slip.get("approved_at", "")},
                "notes": {"S": slip.get("notes", "")}
            }
        )
        return True
    except Exception as e:
        print(f"[_ddb_create_check_slip] Error: {e}")
        return False


def _fallback_find_jsonl_for_invoice(s3_key: str, account_number: str) -> str | None:
    """Search Stage 7/8/99 for a JSONL matching this account when the original key is stale."""
    y, m, d = _extract_ymd_from_key(s3_key)
    for prefix in [POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX, HIST_ARCHIVE_PREFIX]:
        search_prefix = f"{prefix}yyyy={y}/mm={m}/dd={d}/"
        try:
            paginator = s3.get_paginator("list_objects_v2")
            for page in paginator.paginate(Bucket=BUCKET, Prefix=search_prefix):
                for obj in page.get("Contents", []):
                    k = obj["Key"]
                    if k.endswith(".jsonl") and account_number in k and k != s3_key:
                        return k
        except Exception:
            continue
    return None


def _ddb_get_check_slip(check_slip_id: str) -> dict | None:
    """Get a check slip by ID."""
    try:
        resp = ddb.get_item(
            TableName=CHECK_SLIPS_TABLE,
            Key={"check_slip_id": {"S": check_slip_id}}
        )
        item = resp.get("Item")
        if not item:
            return None
        # Parse pdf_errors if present
        pdf_errors_str = item.get("pdf_errors", {}).get("S", "[]")
        try:
            pdf_errors = json.loads(pdf_errors_str) if pdf_errors_str else []
        except:
            pdf_errors = []

        return {
            "check_slip_id": item.get("check_slip_id", {}).get("S", ""),
            "created_date": item.get("created_date", {}).get("S", ""),
            "created_at": item.get("created_at", {}).get("S", ""),
            "created_by": item.get("created_by", {}).get("S", ""),
            "vendor_id": item.get("vendor_id", {}).get("S", ""),
            "vendor_name": item.get("vendor_name", {}).get("S", ""),
            "vendor_code": item.get("vendor_code", {}).get("S", ""),
            "total_amount": float(item.get("total_amount", {}).get("N", "0")),
            "invoice_count": int(item.get("invoice_count", {}).get("N", "0")),
            "invoices": json.loads(item.get("invoices", {}).get("S", "[]")),
            "status": item.get("status", {}).get("S", ""),
            "approved_by": item.get("approved_by", {}).get("S", ""),
            "approved_at": item.get("approved_at", {}).get("S", ""),
            "notes": item.get("notes", {}).get("S", ""),
            "pdf_errors": pdf_errors,
            "pdf_generated_at": item.get("pdf_generated_at", {}).get("S", "")
        }
    except Exception as e:
        print(f"[_ddb_get_check_slip] Error: {e}")
        return None


def _ddb_update_check_slip_status(check_slip_id: str, status: str, approved_by: str = "", approved_at: str = "", notes: str = "") -> bool:
    """Update check slip status (PENDING -> APPROVED or rejected/deleted)."""
    try:
        update_expr = "SET #st = :status"
        expr_names = {"#st": "status"}
        expr_vals = {":status": {"S": status}}

        if approved_by:
            update_expr += ", approved_by = :ab"
            expr_vals[":ab"] = {"S": approved_by}
        if approved_at:
            update_expr += ", approved_at = :at"
            expr_vals[":at"] = {"S": approved_at}
        if notes:
            update_expr += ", notes = :notes"
            expr_vals[":notes"] = {"S": notes}

        ddb.update_item(
            TableName=CHECK_SLIPS_TABLE,
            Key={"check_slip_id": {"S": check_slip_id}},
            UpdateExpression=update_expr,
            ExpressionAttributeNames=expr_names,
            ExpressionAttributeValues=expr_vals
        )
        return True
    except Exception as e:
        print(f"[_ddb_update_check_slip_status] Error: {e}")
        return False


def _ddb_update_check_slip_pdf_errors(check_slip_id: str, pdf_errors: list) -> bool:
    """Update check slip with PDF generation errors."""
    try:
        ddb.update_item(
            TableName=CHECK_SLIPS_TABLE,
            Key={"check_slip_id": {"S": check_slip_id}},
            UpdateExpression="SET pdf_errors = :errors, pdf_generated_at = :gen_at",
            ExpressionAttributeValues={
                ":errors": {"S": json.dumps(pdf_errors)},
                ":gen_at": {"S": dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")}
            }
        )
        return True
    except Exception as e:
        print(f"[_ddb_update_check_slip_pdf_errors] Error: {e}")
        return False


def _ddb_delete_check_slip(check_slip_id: str) -> bool:
    """Delete a check slip from DynamoDB."""
    try:
        ddb.delete_item(
            TableName=CHECK_SLIPS_TABLE,
            Key={"check_slip_id": {"S": check_slip_id}}
        )
        return True
    except Exception as e:
        print(f"[_ddb_delete_check_slip] Error: {e}")
        return False


def _ddb_list_check_slips_by_status_date(status: str, date_str: str) -> list[dict]:
    """List check slips by status and date (uses GSI)."""
    try:
        # Scan with filter (since we may not have GSI yet, use scan)
        resp = ddb.scan(
            TableName=CHECK_SLIPS_TABLE,
            FilterExpression="#st = :status AND created_date = :date",
            ExpressionAttributeNames={"#st": "status"},
            ExpressionAttributeValues={
                ":status": {"S": status},
                ":date": {"S": date_str}
            }
        )
        items = resp.get("Items", [])
        result = []
        for item in items:
            pdf_errors_str = item.get("pdf_errors", {}).get("S", "[]")
            try:
                pdf_errors = json.loads(pdf_errors_str) if pdf_errors_str else []
            except:
                pdf_errors = []
            result.append({
                "check_slip_id": item.get("check_slip_id", {}).get("S", ""),
                "created_date": item.get("created_date", {}).get("S", ""),
                "created_at": item.get("created_at", {}).get("S", ""),
                "created_by": item.get("created_by", {}).get("S", ""),
                "vendor_id": item.get("vendor_id", {}).get("S", ""),
                "vendor_name": item.get("vendor_name", {}).get("S", ""),
                "total_amount": float(item.get("total_amount", {}).get("N", "0")),
                "invoice_count": int(item.get("invoice_count", {}).get("N", "0")),
                "invoices": json.loads(item.get("invoices", {}).get("S", "[]")),
                "status": item.get("status", {}).get("S", ""),
                "approved_by": item.get("approved_by", {}).get("S", ""),
                "approved_at": item.get("approved_at", {}).get("S", ""),
                "notes": item.get("notes", {}).get("S", ""),
                "pdf_errors": pdf_errors,
                "pdf_generated_at": item.get("pdf_generated_at", {}).get("S", "")
            })
        return result
    except Exception as e:
        print(f"[_ddb_list_check_slips_by_status_date] Error: {e}")
        return []


def _ddb_list_check_slips_by_user(created_by: str, status: str = "") -> list[dict]:
    """List check slips created by a specific user."""
    try:
        filter_expr = "created_by = :user"
        expr_vals = {":user": {"S": created_by}}

        if status:
            filter_expr += " AND #st = :status"
            expr_names = {"#st": "status"}
            expr_vals[":status"] = {"S": status}
        else:
            expr_names = {}

        scan_params = {
            "TableName": CHECK_SLIPS_TABLE,
            "FilterExpression": filter_expr,
            "ExpressionAttributeValues": expr_vals
        }
        if expr_names:
            scan_params["ExpressionAttributeNames"] = expr_names

        resp = ddb.scan(**scan_params)
        items = resp.get("Items", [])
        result = []
        for item in items:
            pdf_errors_str = item.get("pdf_errors", {}).get("S", "[]")
            try:
                pdf_errors = json.loads(pdf_errors_str) if pdf_errors_str else []
            except:
                pdf_errors = []
            result.append({
                "check_slip_id": item.get("check_slip_id", {}).get("S", ""),
                "created_date": item.get("created_date", {}).get("S", ""),
                "created_at": item.get("created_at", {}).get("S", ""),
                "created_by": item.get("created_by", {}).get("S", ""),
                "vendor_id": item.get("vendor_id", {}).get("S", ""),
                "vendor_name": item.get("vendor_name", {}).get("S", ""),
                "total_amount": float(item.get("total_amount", {}).get("N", "0")),
                "invoice_count": int(item.get("invoice_count", {}).get("N", "0")),
                "invoices": json.loads(item.get("invoices", {}).get("S", "[]")),
                "status": item.get("status", {}).get("S", ""),
                "approved_by": item.get("approved_by", {}).get("S", ""),
                "approved_at": item.get("approved_at", {}).get("S", ""),
                "notes": item.get("notes", {}).get("S", ""),
                "pdf_errors": pdf_errors,
                "pdf_generated_at": item.get("pdf_generated_at", {}).get("S", "")
            })
        return result
    except Exception as e:
        print(f"[_ddb_list_check_slips_by_user] Error: {e}")
        return []


def _ddb_add_invoice_to_check_slip(pdf_id: str, check_slip_id: str) -> bool:
    """Mark an invoice as belonging to a check slip."""
    try:
        ddb.put_item(
            TableName=CHECK_SLIP_INVOICES_TABLE,
            Item={
                "pdf_id": {"S": pdf_id},
                "check_slip_id": {"S": check_slip_id},
                "added_at": {"S": dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")}
            }
        )
        return True
    except Exception as e:
        print(f"[_ddb_add_invoice_to_check_slip] Error: {e}")
        return False


def _ddb_remove_invoice_from_check_slip(pdf_id: str) -> bool:
    """Remove an invoice from any check slip (releases it back to the pool)."""
    try:
        ddb.delete_item(
            TableName=CHECK_SLIP_INVOICES_TABLE,
            Key={"pdf_id": {"S": pdf_id}}
        )
        return True
    except Exception as e:
        print(f"[_ddb_remove_invoice_from_check_slip] Error: {e}")
        return False


def _ddb_get_invoices_in_check_slips() -> set[str]:
    """Get all pdf_ids that are already in check slips."""
    try:
        resp = ddb.scan(
            TableName=CHECK_SLIP_INVOICES_TABLE,
            ProjectionExpression="pdf_id"
        )
        items = resp.get("Items", [])
        # Handle pagination
        while resp.get("LastEvaluatedKey"):
            resp = ddb.scan(
                TableName=CHECK_SLIP_INVOICES_TABLE,
                ProjectionExpression="pdf_id",
                ExclusiveStartKey=resp["LastEvaluatedKey"]
            )
            items.extend(resp.get("Items", []))
        return {item.get("pdf_id", {}).get("S", "") for item in items if item.get("pdf_id", {}).get("S")}
    except Exception as e:
        print(f"[_ddb_get_invoices_in_check_slips] Error: {e}")
        return set()


def _ddb_list_all_check_slips_for_date(date_str: str) -> list[dict]:
    """List all check slips (any status) for a given date."""
    try:
        resp = ddb.scan(
            TableName=CHECK_SLIPS_TABLE,
            FilterExpression="created_date = :date",
            ExpressionAttributeValues={":date": {"S": date_str}}
        )
        items = resp.get("Items", [])
        result = []
        for item in items:
            pdf_errors_str = item.get("pdf_errors", {}).get("S", "[]")
            try:
                pdf_errors = json.loads(pdf_errors_str) if pdf_errors_str else []
            except:
                pdf_errors = []
            result.append({
                "check_slip_id": item.get("check_slip_id", {}).get("S", ""),
                "created_date": item.get("created_date", {}).get("S", ""),
                "created_at": item.get("created_at", {}).get("S", ""),
                "created_by": item.get("created_by", {}).get("S", ""),
                "vendor_id": item.get("vendor_id", {}).get("S", ""),
                "vendor_name": item.get("vendor_name", {}).get("S", ""),
                "total_amount": float(item.get("total_amount", {}).get("N", "0")),
                "invoice_count": int(item.get("invoice_count", {}).get("N", "0")),
                "invoices": json.loads(item.get("invoices", {}).get("S", "[]")),
                "status": item.get("status", {}).get("S", ""),
                "approved_by": item.get("approved_by", {}).get("S", ""),
                "approved_at": item.get("approved_at", {}).get("S", ""),
                "notes": item.get("notes", {}).get("S", ""),
                "pdf_errors": pdf_errors,
                "pdf_generated_at": item.get("pdf_generated_at", {}).get("S", "")
            })
        return result
    except Exception as e:
        print(f"[_ddb_list_all_check_slips_for_date] Error: {e}")
        return []


@app.get("/api/catalog/vendors")
def api_catalog_vendors(user: str = Depends(require_user), response: Response = None, refresh: str = "0"):
    cache_key = ("catalog_vendors",)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if refresh != "1" and ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        return {"items": ent.get("data", [])}
    # Try to load from vendor cache (has vendorCode) with fallback to dim_vendor
    out = []
    seen_ids = set()
    try:
        vend_cache_obj = s3.get_object(Bucket="api-vendor", Key="vendors/latest.json")
        vend_cache_data = json.loads(vend_cache_obj["Body"].read().decode("utf-8"))
        vendor_list = vend_cache_data.get("vendors", [])
        for v in vendor_list:
            name = str(v.get("name", "")).strip()
            vid = str(v.get("vendorId", "")).strip()
            vcode = str(v.get("vendorCode", "")).strip()
            if (vid or name) and vid not in seen_ids:
                out.append({"id": vid, "name": name, "code": vcode})
                seen_ids.add(vid)
    except Exception as e:
        print(f"[api_catalog_vendors] ERROR loading vendor cache: {e}, falling back to dim_vendor")
        rows = _load_dim_records(DIM_VENDOR_PREFIX)
        for r in rows:
            vid = (
                r.get("vendorId") or r.get("VENDOR_ID") or r.get("Vendor ID") or r.get("id") or r.get("vendor_id") or r.get("VENDORID")
            )
            name = (
                r.get("name") or r.get("NAME") or r.get("vendorName") or r.get("Vendor Name") or r.get("Vendor") or r.get("VENDOR_NAME") or r.get("VENDOR")
            )
            code = r.get("vendorCode") or r.get("VENDOR_CODE") or r.get("code") or r.get("Code") or r.get("VENDORCODE")
            vid_str = str(vid or "").strip()
            name_str = str(name or "").strip()
            # Dedupe by vendor ID (not name) - multiple vendors can have same name (e.g. City of Tacoma)
            if (vid or name) and vid_str not in seen_ids:
                out.append({"id": vid_str, "name": name_str, "code": str(code or "").strip()})
                seen_ids.add(vid_str)
    out = sorted(out, key=lambda x: x["name"].upper())
    _CACHE[cache_key] = {"ts": now, "data": out}
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"items": out}


@app.get("/api/catalog/properties")
def api_catalog_properties(user: str = Depends(require_user), response: Response = None, refresh: str = "0"):
    cache_key = ("catalog_properties",)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if refresh != "1" and ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        return {"items": ent.get("data", [])}
    rows = _load_dim_records(DIM_PROPERTY_PREFIX)
    items = []
    for r in rows:
        pid = (
            r.get("propertyId") or r.get("PROPERTY_ID") or r.get("Property ID") or r.get("id") or r.get("PROPERTYID") or r.get("PROP_ID")
        )
        name = (
            r.get("name") or r.get("NAME") or r.get("propertyName") or r.get("Property Name") or r.get("PROPERTY_NAME") or r.get("Property") or r.get("PROPERTY")
        )
        lookup_code = (
            r.get("LOOKUP_CODE") or r.get("lookup_code") or r.get("LookupCode") or r.get("PROPERTY_CODE") or r.get("code") or r.get("CODE") or ""
        )
        if pid or name:
            items.append({"id": str(pid or "").strip(), "name": str(name or "").strip(), "lookup_code": str(lookup_code or "").strip()})
    _CACHE[cache_key] = {"ts": now, "data": items}
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"items": items}


@app.get("/api/catalog/gl-accounts")
def api_catalog_gl_accounts(user: str = Depends(require_user), response: Response = None, refresh: str = "0"):
    cache_key = ("catalog_gl_accounts",)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if refresh != "1" and ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        return {"items": ent.get("data", [])}
    rows = _load_dim_records(DIM_GL_PREFIX)
    items = []
    for r in rows:
        gl_id = (
            r.get("glAccountId") or r.get("GL_ACCOUNT_ID") or r.get("GL Account ID") or r.get("id") or r.get("ID") or r.get("GLACCOUNTID")
        )
        num_raw = (
            r.get("glAccountNumber") or r.get("GL Account Number") or r.get("GL_ACCOUNT_NUMBER") or r.get("number") or r.get("ACCOUNT_NO") or r.get("GL_NUMBER") or r.get("ACCOUNT_NUMBER")
        )
        num_fmt = (
            r.get("formattedGlAccountNumber") or r.get("Formatted GL Account Number") or r.get("FORMATTED_GL_ACCOUNT_NUMBER") or r.get("FORMATTED_ACCOUNT_NUMBER") or r.get("formatted")
        )
        name = (
            r.get("glAccountName") or r.get("GL Account Name") or r.get("GL_ACCOUNT_NAME") or r.get("name") or r.get("NAME") or r.get("DESCRIPTION") or r.get("Account Name") or r.get("ACCOUNT_NAME") or r.get("GLAccountDescription") or r.get("DESCRIPTION_LONG")
        )
        if (num_raw or num_fmt or name):
            items.append({
                "id": str(gl_id or "").strip(),
                "number": str((num_fmt or num_raw or "")).strip(),  # prefer formatted for selection
                "rawNumber": str(num_raw or "").strip(),
                "formatted": str(num_fmt or (num_raw or "")).strip(),
                "name": str(name or "").strip()
            })
    # Sort by name for consistent alphabetical display
    items = sorted(items, key=lambda x: x["name"].upper())
    _CACHE[cache_key] = {"ts": now, "data": items}
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"items": items}


@app.get("/api/config/accounts-to-track")
def api_get_accounts_to_track(user: str = Depends(require_user), response: Response = None, refresh: str = "0"):
    cache_key = ("accounts_to_track",)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if refresh != "1" and ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        return {"items": ent.get("data", [])}
    arr = _get_accounts_to_track()
    # Ensure is_tracked and is_ubi flags exist
    for item in arr:
        if "is_tracked" not in item:
            item["is_tracked"] = True  # Default: tracked
        if "is_ubi" not in item:
            item["is_ubi"] = False  # Default: not UBI
    _CACHE[cache_key] = {"ts": now, "data": arr}
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"items": arr}


@app.post("/api/config/accounts-to-track")
async def api_save_accounts_to_track(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    # Basic normalization
    norm = []
    for r in items:
        norm.append({
            "vendorId": str(r.get("vendorId") or "").strip(),
            "vendorName": str(r.get("vendorName") or "").strip(),
            "accountNumber": str(r.get("accountNumber") or "").strip(),
            "propertyId": str(r.get("propertyId") or "").strip(),
            "propertyName": str(r.get("propertyName") or "").strip(),
            "glAccountNumber": str(r.get("glAccountNumber") or "").strip(),
            "glAccountName": str(r.get("glAccountName") or "").strip(),
            "daysBetweenBills": int(str(r.get("daysBetweenBills") or "0").strip() or 0),
            "is_tracked": bool(r.get("is_tracked", True)),
            "is_ubi": bool(r.get("is_ubi", False)),
            "comment": str(r.get("comment") or "").strip()
        })
    # Use S3-primary storage (no DynamoDB size limit issues)
    ok = _put_accounts_to_track(norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    # Clear cache
    cache_key = ("accounts_to_track",)
    _CACHE.pop(cache_key, None)
    return {"ok": True, "saved": len(norm)}


@app.post("/api/config/account-comment")
async def api_update_account_comment(request: Request, user: str = Depends(require_user)):
    """Update comment for a specific account."""
    try:
        form = await request.form()
        account_number = form.get("account_number", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        comment = form.get("comment", "").strip()

        if not account_number or not vendor_name:
            return JSONResponse({"error": "account_number and vendor_name are required"}, status_code=400)

        # Load current config (S3-primary)
        arr = _get_accounts_to_track()

        # Find existing account and update comment
        found = False
        for item in arr:
            item_acct = str(item.get("accountNumber") or item.get("account_number") or "").strip()
            item_vendor = str(item.get("vendorName") or item.get("vendor_name") or "").strip()
            if item_acct == account_number and item_vendor == vendor_name:
                item["comment"] = comment
                found = True
                print(f"[ACCOUNT COMMENT] Updated comment for {account_number} ({vendor_name}): {comment[:50]}...")
                break

        if not found:
            return JSONResponse({"error": "Account not found in tracker"}, status_code=404)

        # Save back
        ok = _put_accounts_to_track(arr)
        if not ok:
            return JSONResponse({"error": "Failed to save"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        return {"ok": True, "comment": comment}
    except Exception as e:
        print(f"[ACCOUNT COMMENT] Error: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/config/toggle-ubi-tracking")
async def api_toggle_ubi_tracking(request: Request, user: str = Depends(require_user)):
    """Toggle UBI tracking for a single account."""
    try:
        form = await request.form()
        account_number = form.get("account_number", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        property_name = form.get("property_name", "").strip()
        vendor_id = form.get("vendor_id", "").strip()
        property_id = form.get("property_id", "").strip()

        if not account_number:
            return JSONResponse({"error": "account_number is required"}, status_code=400)

        # Load current config (S3-primary)
        arr = _get_accounts_to_track()

        # Find existing account using composite key (propertyId+vendorId+accountNumber)
        # Fall back to name-based matching for backwards compatibility
        found = False
        found_item = None
        for item in arr:
            item_acct = str(item.get("accountNumber") or item.get("account_number") or "").strip()
            item_vid = str(item.get("vendorId") or "").strip()
            item_pid = str(item.get("propertyId") or "").strip()
            item_vname = str(item.get("vendorName") or item.get("vendor_name") or "").strip()
            item_pname = str(item.get("propertyName") or item.get("property_name") or "").strip()

            # Match by ID composite key
            id_match = (item_acct == account_number and
                        ((vendor_id and item_vid == vendor_id) or (not vendor_id and item_vname == vendor_name)) and
                        ((property_id and item_pid == property_id) or (not property_id and item_pname == property_name)))
            if id_match:
                # Toggle is_ubi (the correct field name)
                current = item.get("is_ubi", False)
                item["is_ubi"] = not current
                found = True
                found_item = item
                break

        # If not found, add new account with is_ubi=True (using correct camelCase schema)
        if not found:
            new_account = {
                "accountNumber": account_number,
                "vendorId": vendor_id,
                "vendorName": vendor_name,
                "propertyId": property_id,
                "propertyName": property_name,
                "glAccountNumber": "",
                "glAccountName": "",
                "daysBetweenBills": 30,
                "is_tracked": True,
                "is_ubi": True,
                "comment": f"Added via UBI toggle by {user}"
            }
            arr.append(new_account)
            found_item = new_account

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        ubi_status = found_item.get("is_ubi", False) if found_item else False
        return {"ok": True, "account_number": account_number, "is_ubi": ubi_status}

    except Exception as e:
        print(f"[UBI TOGGLE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/config/add-to-tracker")
async def api_add_to_tracker(request: Request, user: str = Depends(require_user)):
    """Add an account to the accounts-to-track configuration (legacy endpoint)."""
    try:
        form = await request.form()
        account_number = form.get("account_number", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        vendor_id = form.get("vendor_id", "").strip()
        property_name = form.get("property_name", "").strip()
        property_id = form.get("property_id", "").strip()
        gl_account = form.get("gl_account", "").strip()
        gl_account_name = form.get("gl_account_name", "").strip()
        days_between_bills_str = form.get("days_between_bills", "30").strip() or "30"

        if not account_number:
            return JSONResponse({"error": "account_number is required"}, status_code=400)

        try:
            days_between_bills = int(days_between_bills_str)
        except ValueError:
            days_between_bills = 30

        # Load current config (S3-primary)
        arr = _get_accounts_to_track()

        # Check if account already exists using composite key (camelCase field names)
        found = False
        for item in arr:
            item_acct = str(item.get("accountNumber") or item.get("account_number") or "").strip()
            item_vid = str(item.get("vendorId") or "").strip()
            item_pid = str(item.get("propertyId") or "").strip()
            item_vname = str(item.get("vendorName") or item.get("vendor_name") or "").strip()
            item_pname = str(item.get("propertyName") or item.get("property_name") or "").strip()

            # Match by ID composite key or name composite key
            id_match = (item_acct == account_number and
                        ((vendor_id and item_vid == vendor_id) or (not vendor_id and item_vname == vendor_name)) and
                        ((property_id and item_pid == property_id) or (not property_id and item_pname == property_name)))
            if id_match:
                # Update existing with correct camelCase field names
                item["vendorId"] = vendor_id or item_vid
                item["vendorName"] = vendor_name or item_vname
                item["propertyId"] = property_id or item_pid
                item["propertyName"] = property_name or item_pname
                item["glAccountNumber"] = gl_account or item.get("glAccountNumber", "")
                item["glAccountName"] = gl_account_name or item.get("glAccountName", "")
                item["daysBetweenBills"] = days_between_bills
                item["is_tracked"] = True
                # Migrate old field names if present
                item.pop("account_number", None)
                item.pop("vendor_name", None)
                item.pop("property_name", None)
                item.pop("gl_account", None)
                item.pop("gl_account_name", None)
                item.pop("days_between_bills", None)
                item.pop("ubi_tracking", None)
                found = True
                break

        if not found:
            # Add new account with correct camelCase schema
            new_account = {
                "accountNumber": account_number,
                "vendorId": vendor_id,
                "vendorName": vendor_name,
                "propertyId": property_id,
                "propertyName": property_name,
                "glAccountNumber": gl_account,
                "glAccountName": gl_account_name,
                "daysBetweenBills": days_between_bills,
                "is_tracked": True,
                "is_ubi": False,
                "comment": f"Added via add-to-tracker by {user}"
            }
            arr.append(new_account)

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        return {"ok": True, "account_number": account_number}

    except Exception as e:
        print(f"[ADD TO TRACKER] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/config/add-to-ubi")
async def api_add_to_ubi(request: Request, user: str = Depends(require_user)):
    """Add charge code to UBI mapping configuration with full 4-field composite key."""
    try:
        form = await request.form()
        # Get all required fields for 4-field composite key
        vendor_id = form.get("vendor_id", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        account_number = form.get("account_number", "").strip()
        property_id = form.get("property_id", "").strip()
        property_name = form.get("property_name", "").strip()
        gl_account = form.get("gl_account", "").strip()
        gl_account_name = form.get("gl_account_name", "").strip()
        charge_code = form.get("charge_code", "").strip()
        utility_name = form.get("utility_name", "").strip()
        notes = form.get("notes", "").strip()

        # Validate required fields for 4-field composite key
        if not vendor_id or not account_number or not property_id or not gl_account or not charge_code:
            return JSONResponse({
                "error": "vendor_id, account_number, property_id, gl_account, and charge_code are required"
            }, status_code=400)

        print(f"[ADD TO UBI] Adding mapping: vendor={vendor_id}, account={account_number}, property={property_id}, gl={gl_account}, charge_code={charge_code}")

        # Load current ubi-mapping config
        arr = _ddb_get_config("ubi-mapping")
        if arr is None:
            arr = []
        if not isinstance(arr, list):
            arr = []

        # Check if this exact combination already exists (4-field composite key)
        exists = any(
            str(item.get("vendorId", "")).strip() == vendor_id and
            str(item.get("accountNumber", "")).strip() == account_number and
            str(item.get("propertyId", "")).strip() == property_id and
            str(item.get("glAccountNumber", "")).strip() == gl_account
            for item in arr
        )

        if exists:
            print(f"[ADD TO UBI] Mapping already exists, skipping")
            return {"ok": True, "message": "UBI mapping already exists", "added": 0}

        # Add the new UBI mapping entry with ALL fields using camelCase (to match ubi-mapping GET endpoint)
        new_mapping = {
            "vendorId": vendor_id,
            "vendorName": vendor_name,
            "accountNumber": account_number,
            "propertyId": property_id,
            "propertyName": property_name,
            "glAccountNumber": gl_account,
            "glAccountName": gl_account_name,
            "chargeCode": charge_code,
            "utilityName": utility_name,
            "isUbi": True,  # Mark as UBI when adding via this endpoint
            "notes": notes
        }
        arr.append(new_mapping)

        print(f"[ADD TO UBI] Saving {len(arr)} total mappings to DDB/S3")

        # Save UBI mapping to DDB
        ddb_ok = False
        try:
            ddb_ok = _ddb_put_config("ubi-mapping", arr)
            print(f"[ADD TO UBI] DDB save: {ddb_ok}")
        except Exception as e:
            print(f"[ADD TO UBI] DDB error: {e}")

        # Clear UBI mapping cache
        cache_key = ("ubi_mapping",)
        _CACHE.pop(cache_key, None)

        if not ddb_ok:
            return JSONResponse({"error": "save_failed"}, status_code=500)

        return {"ok": True, "added": 1, "mapping": new_mapping}

    except Exception as e:
        print(f"[ADD TO UBI] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- UBI Account Management --------
@app.post("/api/ubi/add-to-tracker")
async def api_add_to_tracker(request: Request, user: str = Depends(require_user)):
    """Add account to tracker (monitoring only) - sets is_tracked=true, is_ubi=false"""
    try:
        form = await request.form()
        vendor_id = form.get("vendor_id", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        account_number = form.get("account_number", "").strip()
        property_id = form.get("property_id", "").strip()
        property_name = form.get("property_name", "").strip()

        # Look up property_id from property_name if not provided
        if not property_id and property_name:
            rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            props = []
            for r in rows:
                pid = (r.get("propertyId") or r.get("PROPERTY_ID") or r.get("Property ID") or r.get("id") or r.get("PROPERTYID") or r.get("PROP_ID"))
                name = (r.get("name") or r.get("NAME") or r.get("propertyName") or r.get("Property Name") or r.get("PROPERTY_NAME") or r.get("Property") or r.get("PROPERTY"))
                if pid or name:
                    props.append({"id": str(pid or "").strip(), "name": str(name or "").strip()})

            print(f"[ADD TO TRACKER] Looking up property_id for '{property_name}' in {len(props)} properties")
            for p in props:
                if p.get("name", "").strip() == property_name:
                    property_id = p.get("id", "").strip()
                    print(f"[ADD TO TRACKER] Found property_id: {property_id}")
                    break
            if not property_id:
                print(f"[ADD TO TRACKER] Could not find property_id for '{property_name}'")

        # Look up vendor_id from vendor_name if not provided
        if not vendor_id and vendor_name:
            rows = _load_dim_records(DIM_VENDOR_PREFIX)
            vendors = []
            for r in rows:
                vid = (r.get("vendorId") or r.get("VENDOR_ID") or r.get("Vendor ID") or r.get("id") or r.get("vendor_id") or r.get("VENDORID"))
                name = (r.get("name") or r.get("NAME") or r.get("vendorName") or r.get("Vendor Name") or r.get("Vendor") or r.get("VENDOR_NAME") or r.get("VENDOR"))
                if vid or name:
                    vendors.append({"id": str(vid or "").strip(), "name": str(name or "").strip()})

            print(f"[ADD TO TRACKER] Looking up vendor_id for '{vendor_name}' in {len(vendors)} vendors")
            for v in vendors:
                if v.get("name", "").strip() == vendor_name:
                    vendor_id = v.get("id", "").strip()
                    print(f"[ADD TO TRACKER] Found vendor_id: {vendor_id}")
                    break
            if not vendor_id:
                print(f"[ADD TO TRACKER] Could not find vendor_id for '{vendor_name}'")

        # Auto-generate account number if missing (some vendors have no account #)
        if not account_number and vendor_id:
            account_number = f"NOACT-{vendor_id}"
            print(f"[ADD TO TRACKER] Auto-generated account_number: {account_number}")

        print(f"[ADD TO TRACKER] Final values: vendor_id={vendor_id}, account_number={account_number}, property_id={property_id}")

        if not vendor_id or not account_number or not property_id:
            error_msg = f"vendor_id={vendor_id or 'MISSING'}, account_number={account_number or 'MISSING'}, property_id={property_id or 'MISSING'} (could not resolve from names: vendor_name={vendor_name}, property_name={property_name})"
            print(f"[ADD TO TRACKER] ERROR: {error_msg}")
            return JSONResponse({"error": error_msg}, status_code=400)

        # Load accounts-to-track (S3-primary)
        arr = _get_accounts_to_track()

        # Check if account exists (match by ID or by name  prevents duplicates)
        found = False
        for item in arr:
            id_match = (str(item.get("vendorId", "")).strip() == vendor_id and
                        str(item.get("accountNumber", "")).strip() == account_number and
                        str(item.get("propertyId", "")).strip() == property_id)
            name_match = (str(item.get("vendorName", "")).strip() == vendor_name and
                          str(item.get("accountNumber", "")).strip() == account_number and
                          str(item.get("propertyName", "")).strip() == property_name)
            if id_match or (name_match and vendor_name and property_name):
                # Update existing account
                item["is_tracked"] = True
                found = True
                break

        if not found:
            # Create new account
            arr.append({
                "vendorId": vendor_id,
                "vendorName": vendor_name,
                "accountNumber": account_number,
                "propertyId": property_id,
                "propertyName": property_name,
                "glAccountNumber": "",
                "glAccountName": "",
                "daysBetweenBills": 30,
                "is_tracked": True,
                "is_ubi": False,
                "notes": f"Added to tracker by {user}"
            })

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        return {"ok": True, "existed": found}

    except Exception as e:
        print(f"[ADD TO TRACKER] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/ubi/add-to-ubi")
async def api_add_account_to_ubi(request: Request, user: str = Depends(require_user)):
    """Add account to UBI program - sets is_ubi=true (creates account if doesn't exist)"""
    try:
        form = await request.form()
        vendor_id = form.get("vendor_id", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        account_number = form.get("account_number", "").strip()
        property_id = form.get("property_id", "").strip()
        property_name = form.get("property_name", "").strip()

        # Look up property_id from property_name if not provided
        if not property_id and property_name:
            rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            props = []
            for r in rows:
                pid = (r.get("propertyId") or r.get("PROPERTY_ID") or r.get("Property ID") or r.get("id") or r.get("PROPERTYID") or r.get("PROP_ID"))
                name = (r.get("name") or r.get("NAME") or r.get("propertyName") or r.get("Property Name") or r.get("PROPERTY_NAME") or r.get("Property") or r.get("PROPERTY"))
                if pid or name:
                    props.append({"id": str(pid or "").strip(), "name": str(name or "").strip()})

            print(f"[ADD TO UBI] Looking up property_id for '{property_name}' in {len(props)} properties")
            for p in props:
                if p.get("name", "").strip() == property_name:
                    property_id = p.get("id", "").strip()
                    print(f"[ADD TO UBI] Found property_id: {property_id}")
                    break
            if not property_id:
                print(f"[ADD TO UBI] Could not find property_id for '{property_name}'")

        # Look up vendor_id from vendor_name if not provided
        if not vendor_id and vendor_name:
            rows = _load_dim_records(DIM_VENDOR_PREFIX)
            vendors = []
            for r in rows:
                vid = (r.get("vendorId") or r.get("VENDOR_ID") or r.get("Vendor ID") or r.get("id") or r.get("vendor_id") or r.get("VENDORID"))
                name = (r.get("name") or r.get("NAME") or r.get("vendorName") or r.get("Vendor Name") or r.get("Vendor") or r.get("VENDOR_NAME") or r.get("VENDOR"))
                if vid or name:
                    vendors.append({"id": str(vid or "").strip(), "name": str(name or "").strip()})

            print(f"[ADD TO UBI] Looking up vendor_id for '{vendor_name}' in {len(vendors)} vendors")
            for v in vendors:
                if v.get("name", "").strip() == vendor_name:
                    vendor_id = v.get("id", "").strip()
                    print(f"[ADD TO UBI] Found vendor_id: {vendor_id}")
                    break
            if not vendor_id:
                print(f"[ADD TO UBI] Could not find vendor_id for '{vendor_name}'")

        # Auto-generate account number if missing (some vendors have no account #)
        if not account_number and vendor_id:
            account_number = f"NOACT-{vendor_id}"
            print(f"[ADD TO UBI] Auto-generated account_number: {account_number}")

        print(f"[ADD TO UBI] Final values: vendor_id={vendor_id}, account_number={account_number}, property_id={property_id}")

        if not vendor_id or not account_number or not property_id:
            error_msg = f"vendor_id={vendor_id or 'MISSING'}, account_number={account_number or 'MISSING'}, property_id={property_id or 'MISSING'} (could not resolve from names: vendor_name={vendor_name}, property_name={property_name})"
            print(f"[ADD TO UBI] ERROR: {error_msg}")
            return JSONResponse({"error": error_msg}, status_code=400)

        # Load accounts-to-track (S3-primary)
        arr = _get_accounts_to_track()

        # Check if account exists (match by ID or by name  prevents duplicates)
        found = False
        for item in arr:
            id_match = (str(item.get("vendorId", "")).strip() == vendor_id and
                        str(item.get("accountNumber", "")).strip() == account_number and
                        str(item.get("propertyId", "")).strip() == property_id)
            name_match = (str(item.get("vendorName", "")).strip() == vendor_name and
                          str(item.get("accountNumber", "")).strip() == account_number and
                          str(item.get("propertyName", "")).strip() == property_name)
            if id_match or (name_match and vendor_name and property_name):
                # Update existing account
                item["is_ubi"] = True
                found = True
                break

        if not found:
            # Create new account with is_ubi=True
            arr.append({
                "vendorId": vendor_id,
                "vendorName": vendor_name,
                "accountNumber": account_number,
                "propertyId": property_id,
                "propertyName": property_name,
                "glAccountNumber": "",
                "glAccountName": "",
                "daysBetweenBills": 30,
                "is_tracked": False,  # Not tracked (yet)
                "is_ubi": True,        # UBI enabled
                "notes": f"Added to UBI by {user}"
            })

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        return {"ok": True, "message": "Account added to UBI program"}

    except Exception as e:
        print(f"[ADD TO UBI] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/ubi/remove-from-tracker")
async def api_remove_from_tracker(request: Request, user: str = Depends(require_admin)):
    """Remove account from tracker - sets is_tracked=false. Admin only."""
    try:
        form = await request.form()
        vendor_id = form.get("vendor_id", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        account_number = form.get("account_number", "").strip()
        property_id = form.get("property_id", "").strip()
        property_name = form.get("property_name", "").strip()

        # Load accounts-to-track (S3-primary)
        arr = _get_accounts_to_track()

        # Find and update ALL matching accounts (handles duplicates)
        found = False
        for item in arr:
            # Match by account_number + vendor_name + property_name
            if (str(item.get("accountNumber", "")).strip() == account_number and
                str(item.get("vendorName", "")).strip() == vendor_name and
                str(item.get("propertyName", "")).strip() == property_name):
                item["is_tracked"] = False
                found = True
                # Don't break  mark ALL duplicates

        if not found:
            return JSONResponse({"error": "Account not found"}, status_code=404)

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear caches
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)
        # Also clear completion tracker cache so refresh picks up the change
        keys_to_clear = [k for k in _CACHE if isinstance(k, tuple) and len(k) >= 1 and k[0] == "completion_tracker"]
        for k in keys_to_clear:
            _CACHE.pop(k, None)

        return {"ok": True}

    except Exception as e:
        print(f"[REMOVE FROM TRACKER] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/ubi/remove-from-ubi")
async def api_remove_from_ubi(request: Request, user: str = Depends(require_user)):
    """Remove account from UBI - sets is_ubi=false"""
    try:
        form = await request.form()
        vendor_id = form.get("vendor_id", "").strip()
        vendor_name = form.get("vendor_name", "").strip()
        account_number = form.get("account_number", "").strip()
        property_id = form.get("property_id", "").strip()
        property_name = form.get("property_name", "").strip()

        # Load accounts-to-track (S3-primary)
        arr = _get_accounts_to_track()

        # Find and update ALL matching accounts (handles duplicates)
        found = False
        for item in arr:
            # Match by account_number + vendor_name + property_name
            if (str(item.get("accountNumber", "")).strip() == account_number and
                str(item.get("vendorName", "")).strip() == vendor_name and
                str(item.get("propertyName", "")).strip() == property_name):
                item["is_ubi"] = False
                found = True
                # Don't break  mark ALL duplicates

        if not found:
            return JSONResponse({"error": "Account not found"}, status_code=404)

        # Save using S3-primary storage
        if not _put_accounts_to_track(arr):
            return JSONResponse({"error": "save_failed"}, status_code=500)

        # Clear cache
        cache_key = ("accounts_to_track",)
        _CACHE.pop(cache_key, None)

        return {"ok": True}

    except Exception as e:
        print(f"[REMOVE FROM UBI] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- UBI Billback Line Item Management --------
@app.post("/api/billback/update-line-item")
async def api_update_line_item(request: Request, user: str = Depends(require_user)):
    """Update line item with charge code, amount overrides, and exclusions"""
    try:
        form = await request.form()
        bill_id = form.get("bill_id", "").strip()
        line_index = int(form.get("line_index", "-1"))

        if not bill_id or line_index < 0:
            return JSONResponse({"error": "bill_id and line_index required"}, status_code=400)

        # Get draft
        draft = _ddb_get_draft(bill_id)
        if not draft or "line_data" not in draft:
            return JSONResponse({"error": "bill not found"}, status_code=404)

        line_data = draft.get("line_data", [])
        if line_index >= len(line_data):
            return JSONResponse({"error": "line_index out of range"}, status_code=400)

        line = line_data[line_index]

        # Update charge code fields
        if "charge_code" in form:
            line["Charge Code"] = form.get("charge_code", "").strip()
        if "charge_code_source" in form:
            line["Charge Code Source"] = form.get("charge_code_source", "mapping").strip()
        if "charge_code_overridden" in form:
            line["Charge Code Overridden"] = form.get("charge_code_overridden", "").lower() in ["true", "1"]
        if "charge_code_override_reason" in form:
            line["Charge Code Override Reason"] = form.get("charge_code_override_reason", "").strip()
        # Store utility type override - this is the canonical field read by master bills
        # When user overrides charge code with a utility-specific mapping (e.g., Water vs Sewer),
        # we update "Utility Type" so it propagates to master bills and other displays
        if "utility_name" in form:
            utility_override = form.get("utility_name", "").strip()
            if utility_override:
                line["Utility Type"] = utility_override
                # Also update Mapped Utility Name for display consistency
                line["Mapped Utility Name"] = utility_override
                # Also keep a record of the original for reference
                if "Utility Type" not in line or line.get("_original_utility_type") is None:
                    line["_original_utility_type"] = line.get("Utility Type", line.get("Utility Name", ""))

        # Update amount override fields
        if "current_amount" in form:
            try:
                line["Current Amount"] = float(form.get("current_amount", "0"))
            except Exception:
                pass
        if "amount_overridden" in form:
            line["Amount Overridden"] = form.get("amount_overridden", "").lower() in ["true", "1"]
        if "amount_override_reason" in form:
            line["Amount Override Reason"] = form.get("amount_override_reason", "").strip()

        # Update exclusion fields
        if "is_excluded_from_ubi" in form:
            line["Is Excluded From UBI"] = int(form.get("is_excluded_from_ubi", "0"))
        if "exclusion_reason" in form:
            line["Exclusion Reason"] = form.get("exclusion_reason", "").strip()

        # Update line and save
        line_data[line_index] = line
        draft["line_data"] = line_data
        draft["updated_utc"] = datetime.utcnow().isoformat()
        draft["updated_by"] = user

        _ddb_put_draft(draft)

        return {"ok": True, "line": line}

    except Exception as e:
        print(f"[UPDATE LINE ITEM] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/assign-periods")
async def api_assign_billback_periods(request: Request, user: str = Depends(require_user)):
    """Assign billback periods to a line item"""
    try:
        form = await request.form()
        bill_id = form.get("bill_id", "").strip()
        line_index = int(form.get("line_index", "-1"))
        assignments_json = form.get("assignments", "[]")

        if not bill_id or line_index < 0:
            return JSONResponse({"error": "bill_id and line_index required"}, status_code=400)

        # Parse assignments
        import json
        try:
            assignments = json.loads(assignments_json)
        except Exception:
            return JSONResponse({"error": "invalid assignments JSON"}, status_code=400)

        if not isinstance(assignments, list):
            return JSONResponse({"error": "assignments must be a list"}, status_code=400)

        # Get draft
        draft = _ddb_get_draft(bill_id)
        if not draft or "line_data" not in draft:
            return JSONResponse({"error": "bill not found"}, status_code=404)

        line_data = draft.get("line_data", [])
        if line_index >= len(line_data):
            return JSONResponse({"error": "line_index out of range"}, status_code=400)

        # Update billback assignments
        line_data[line_index]["billback_assignments"] = assignments
        line_data[line_index]["updated_by"] = user
        line_data[line_index]["updated_utc"] = datetime.utcnow().isoformat()

        draft["line_data"] = line_data
        draft["updated_utc"] = datetime.utcnow().isoformat()
        draft["updated_by"] = user

        _ddb_put_draft(draft)

        return {"ok": True}

    except Exception as e:
        print(f"[ASSIGN PERIODS] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/billback/send-to-post")
async def api_send_bill_to_post(request: Request, user: str = Depends(require_user)):
    """Move a bill back from Stage 7 (PostEntrata) to Stage 6 (PreEntrata) for reprocessing."""
    try:
        form = await request.form()
        bill_id = form.get("bill_id", "").strip()

        if not bill_id:
            return JSONResponse({"error": "bill_id required"}, status_code=400)

        # Validate key - must be from PostEntrata prefix and no path traversal
        _require_valid_s3_key(bill_id, allowed_prefixes=(POST_ENTRATA_PREFIX,), operation="send-to-post")

        # Build Stage 6 destination key
        stage6_key = bill_id.replace(POST_ENTRATA_PREFIX, STAGE6_PREFIX)

        # Copy from Stage 7 to Stage 6
        s3.copy_object(
            Bucket=BUCKET,
            CopySource={"Bucket": BUCKET, "Key": bill_id},
            Key=stage6_key
        )

        # Delete from Stage 7 (BILLBACK) to complete the move
        s3.delete_object(Bucket=BUCKET, Key=bill_id)

        print(f"[SEND TO POST] Moved {bill_id} to {stage6_key} by {user}")

        return {"ok": True, "stage6_key": stage6_key, "moved": True}
    except Exception as e:
        print(f"[SEND TO POST] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- GL Code to Charge Code Mapping (Property-Aware) --------
@app.get("/api/config/gl-charge-code-mapping")
def api_get_gl_charge_code_mapping(user: str = Depends(require_user)):
    """Get property-aware GL code to charge code mappings"""
    arr = _ddb_get_config("gl-charge-code-mapping")
    if arr is None:
        arr = []
    if not isinstance(arr, list):
        arr = []
    return {"items": arr}


@app.post("/api/config/gl-charge-code-mapping")
async def api_save_gl_charge_code_mapping(request: Request, user: str = Depends(require_user)):
    """Save property-aware GL code to charge code mappings"""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)

    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)

    # Normalize items
    norm = []
    for r in items:
        norm.append({
            "property_id": str(r.get("property_id") or "").strip(),
            "property_name": str(r.get("property_name") or "").strip(),
            "gl_code": str(r.get("gl_code") or "").strip(),
            "gl_code_name": str(r.get("gl_code_name") or "").strip(),
            "gl_account_id": str(r.get("gl_account_id") or "").strip(),
            "charge_code": str(r.get("charge_code") or "").strip(),
            "utility_name": str(r.get("utility_name") or "").strip(),
            "is_billable": bool(r.get("is_billable", True)),
            "notes": str(r.get("notes") or "").strip()
        })

    # Save to DDB
    ddb_ok = False
    try:
        ddb_ok = _ddb_put_config("gl-charge-code-mapping", norm)
    except Exception as e:
        print(f"[GL MAPPING] Error: {e}")

    if not ddb_ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)

    return {"ok": True}


# -------- Master Bills (Aggregation) --------
@app.post("/api/master-bills/generate")
async def api_generate_master_bills(request: Request, user: str = Depends(require_user)):
    """Generate master bills by aggregating line items from Stage 8 (UBI Assigned)"""
    try:
        from datetime import datetime, timedelta
        from concurrent.futures import ThreadPoolExecutor, as_completed
        import hashlib

        print("[GENERATE MASTER BILLS] Starting generation...")
        payload = await request.json()
        # Now uses MM/YYYY format directly from dropdown
        start_period = payload.get("start_period", "").strip() if isinstance(payload, dict) else ""
        end_period = payload.get("end_period", "").strip() if isinstance(payload, dict) else ""
        days_back = int(payload.get("days_back", 365)) if isinstance(payload, dict) else 365
        print(f"[GENERATE MASTER BILLS] Period filter: {start_period} to {end_period}")

        # Build property_id  lookup_code map from dim_property
        _prop_lookup = {}
        try:
            _prop_rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            for _pr in _prop_rows:
                _pid = str(_pr.get("propertyId") or _pr.get("PROPERTY_ID") or _pr.get("Property ID") or _pr.get("id") or "").strip()
                _lc = str(_pr.get("LOOKUP_CODE") or _pr.get("lookup_code") or _pr.get("LookupCode") or _pr.get("PROPERTY_CODE") or "").strip()
                if _pid and _lc:
                    _prop_lookup[_pid] = _lc
            print(f"[GENERATE MASTER BILLS] Loaded {len(_prop_lookup)} property lookup codes")
        except Exception as e:
            print(f"[GENERATE MASTER BILLS] Warning: could not load property lookup codes: {e}")

        # Scan Stage 8 (UBI_ASSIGNED_PREFIX) for assigned line items
        print("[GENERATE MASTER BILLS] Scanning Stage 8 for assigned items...")

        # Build date-partitioned prefixes
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(days_back):
            d = today - timedelta(days=i)
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        # Collect all S3 keys
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception as e:
                pass  # Skip inaccessible prefixes

        print(f"[GENERATE MASTER BILLS] Found {len(all_keys)} files in Stage 8")

        # Load all assigned line items from Stage 8
        all_line_items = []

        def process_file(key):
            """Process a single S3 file and extract assigned line items."""
            try:
                body = _read_s3_text(BUCKET, key)
                items = []
                for line in body.splitlines():
                    line = (line or '').strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                        # Only include lines with ubi_period (assigned items)
                        if rec.get("ubi_period"):
                            rec["__stage8_key__"] = key  # Track Stage 8 source (separate from baked __s3_key__)
                            items.append(rec)
                    except Exception:
                        continue
                return items
            except Exception as e:
                print(f"[GENERATE MASTER BILLS] Error processing {key}: {e}")
                return []

        # Process files concurrently
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                all_line_items.extend(future.result())

        print(f"[GENERATE MASTER BILLS] Found {len(all_line_items)} assigned line items")

        # Process each assigned line item
        master_bills = {}  # key: property_id|charge_code|utility_name|ubi_period

        print("[GENERATE MASTER BILLS] Processing assigned items...")
        for idx, line_data in enumerate(all_line_items):
            try:
                # Handle multi-period format (ubi_assignments array)
                ubi_assignments = line_data.get("ubi_assignments", [])
                if not ubi_assignments:
                    # Legacy format: create single-item list from ubi_period field
                    legacy_period = line_data.get("ubi_period", "")
                    legacy_amount = float(line_data.get("ubi_amount", 0))
                    if legacy_amount == 0:
                        charge_str = str(line_data.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                        legacy_amount = float(charge_str) if charge_str else 0.0
                    if legacy_period:
                        ubi_assignments = [{"period": legacy_period, "amount": legacy_amount}]

                if not ubi_assignments:
                    continue

                # Process each UBI period assignment for this line
                for ubi_asn in ubi_assignments:
                    ubi_period = ubi_asn.get("period", "")

                    # IMPORTANT: Check if amount was overridden AFTER assignment
                    # If Amount Overridden is True, always use Current Amount (even if it's 0)
                    # This handles the case where user zeroed out an amount after UBI assignment
                    amount_overridden_flag = line_data.get("Amount Overridden") or line_data.get("amount_overridden")
                    if amount_overridden_flag:
                        # User explicitly set this amount - use their value
                        amount = float(line_data.get("Current Amount") or line_data.get("current_amount") or 0)
                    else:
                        # Use stored assignment amount, with fallbacks
                        amount = float(ubi_asn.get("amount", 0))
                        if amount == 0:
                            # Check for Current Amount first, then fall back to Line Item Charge
                            if line_data.get("Current Amount") is not None:
                                amount = float(line_data.get("Current Amount", 0))
                            else:
                                charge_str = str(line_data.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                                amount = float(charge_str) if charge_str else 0.0

                    if not ubi_period:
                        continue

                    # Extract line item details (use enriched fields from Stage 7)
                    property_id = line_data.get("EnrichedPropertyID", line_data.get("Property ID", ""))
                    property_name = line_data.get("EnrichedPropertyName", line_data.get("Property Name", ""))
                    charge_code = line_data.get("Charge Code", "")
                    utility_name = line_data.get("Utility Type", line_data.get("Utility Name", ""))
                    gl_code = line_data.get("EnrichedGLAccountNumber", line_data.get("GL Account Number", ""))
                    gl_name = line_data.get("EnrichedGLAccountName", line_data.get("GL Account Name", ""))
                    description = line_data.get("Line Item Description", "")
                    # Get bill_id from __stage8_key__ (the actual Stage 8 S3 key for unassign API)
                    bill_id_from_s3 = line_data.get("__stage8_key__", "")
                    line_index_from_s3 = int(line_data.get("Line Index", 0))
                    account_number = line_data.get("Account Number", line_data.get("AccountNumber", ""))
                    vendor_name = line_data.get("EnrichedVendorName", line_data.get("Vendor Name", ""))

                    # Skip excluded line items
                    is_excluded = line_data.get("Is Excluded From UBI", 0)
                    exclusion_reason = line_data.get("Exclusion Reason", "")
                    if is_excluded:
                        print(f"[GENERATE MASTER BILLS] Skipping excluded line (Reason: {exclusion_reason})")
                        continue

                    # Check for overrides
                    amount_overridden = line_data.get("Amount Overridden", False)
                    charge_code_overridden = line_data.get("Charge Code Overridden", False)
                    amount_override_reason = line_data.get("Amount Override Reason", "")
                    charge_code_override_reason = line_data.get("Charge Code Override Reason", "")

                    if not property_id:
                        print(f"[GENERATE MASTER BILLS] Skipping line: missing property_id")
                        continue

                    # Use default charge code if missing (allows lines to be included even without mapping)
                    if not charge_code or charge_code == "N/A":
                        charge_code = "UNMAPPED"
                        print(f"[GENERATE MASTER BILLS] Line missing charge code, using UNMAPPED for property {property_id}")

                    # Parse period (format: "12/2025 to 12/2025" or "01/2025 to 03/2025")
                    period_parts = ubi_period.split(" to ")
                    period_start_str = period_parts[0].strip() if len(period_parts) > 0 else ""
                    period_end_str = period_parts[1].strip() if len(period_parts) > 1 else period_start_str

                    # Convert to actual dates (first day of first month, last day of last month)
                    import calendar

                    try:
                        # Parse MM/YYYY format
                        start_month, start_year = period_start_str.split("/")
                        end_month, end_year = period_end_str.split("/")

                        # First day of start month
                        period_start = f"{start_month}/01/{start_year}"

                        # Last day of end month
                        last_day = calendar.monthrange(int(end_year), int(end_month))[1]
                        period_end = f"{end_month}/{last_day:02d}/{end_year}"

                    except (ValueError, IndexError) as e:
                        print(f"[GENERATE MASTER BILLS] Error parsing period {ubi_period}: {e}")
                        continue

                    # Filter by period range if provided
                    # Both start_period, end_period, and period_start_str are MM/YYYY format
                    # Convert all to YYYY-MM for proper string comparison
                    if start_period or end_period:
                        # Convert period_start_str (MM/YYYY) to YYYY-MM for comparison
                        try:
                            p_month, p_year = period_start_str.split("/")
                            period_yyyymm = f"{p_year}-{p_month.zfill(2)}"
                        except Exception:
                            continue

                        if start_period:
                            # Convert start_period (MM/YYYY) to YYYY-MM
                            try:
                                s_month, s_year = start_period.split("/")
                                start_yyyymm = f"{s_year}-{s_month.zfill(2)}"
                                if period_yyyymm < start_yyyymm:
                                    continue
                            except Exception:
                                pass
                        if end_period:
                            # Convert end_period (MM/YYYY) to YYYY-MM
                            try:
                                e_month, e_year = end_period.split("/")
                                end_yyyymm = f"{e_year}-{e_month.zfill(2)}"
                                if period_yyyymm > end_yyyymm:
                                    continue
                            except Exception:
                                pass

                    # Create master bill key using original period strings for consistency
                    mb_key = f"{property_id}|{charge_code}|{utility_name}|{period_start_str}|{period_end_str}"

                    if mb_key not in master_bills:
                        master_bills[mb_key] = {
                            "master_bill_id": mb_key,
                            "property_id": property_id,
                            "property_name": property_name,
                            "property_lookup_code": _prop_lookup.get(str(property_id), ""),
                            "ar_code_mapping": charge_code,
                            "utility_name": utility_name,
                            "billback_month_start": period_start,
                            "billback_month_end": period_end,
                            "utility_amount": 0,
                            "source_line_items": [],
                            "created_utc": datetime.utcnow().isoformat(),
                            "created_by": user,
                            "status": "draft"
                        }

                    # Add to aggregate amount
                    master_bills[mb_key]["utility_amount"] += amount

                    # Compute line hash for unassign functionality
                    # __s3_key__ still contains the original baked Stage 4 key (not overwritten)
                    # so hash matches what the unassign endpoint computes from the raw Stage 8 file
                    line_hash = _compute_stable_line_hash(line_data)

                    # Extract service period dates
                    service_start = (line_data.get("Bill Period Start") or line_data.get("billPeriodStart") or "").strip()
                    service_end = (line_data.get("Bill Period End") or line_data.get("billPeriodEnd") or "").strip()
                    bill_date = (line_data.get("Bill Date") or line_data.get("Invoice Date") or line_data.get("billDate") or "").strip()

                    # Get original PDF key for viewing
                    pdf_key = (line_data.get("source_input_key") or line_data.get("PDF_LINK") or line_data.get("__pdf_s3_key__") or line_data.get("pdfKey") or "").strip()

                    # Add source line item
                    master_bills[mb_key]["source_line_items"].append({
                        "bill_id": bill_id_from_s3,
                        "line_index": line_index_from_s3,
                        "line_hash": line_hash,
                        "account_number": account_number,
                        "vendor_name": vendor_name,
                        "gl_code": gl_code,
                        "gl_code_name": gl_name,
                        "utility_name": utility_name,
                        "description": description,
                        "amount": amount,
                        "service_start": _normalize_date_display(service_start),
                        "service_end": _normalize_date_display(service_end),
                        "bill_date": _normalize_date_display(bill_date),
                        "pdf_key": pdf_key,
                        "overridden": amount_overridden or charge_code_overridden,
                        "override_reason": " | ".join(filter(None, [
                            amount_override_reason,
                            charge_code_override_reason
                        ]))
                    })

                if (idx + 1) % 100 == 0:
                    print(f"[GENERATE MASTER BILLS] Processed {idx + 1}/{len(all_line_items)} line items...")

            except Exception as e:
                print(f"[GENERATE MASTER BILLS] Error processing assignment {idx}: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Merge manual/accrual entries for the same period range
        try:
            me_items = []
            me_response = ddb.scan(TableName=MANUAL_ENTRIES_TABLE)
            me_items = me_response.get("Items", [])
            while me_response.get("LastEvaluatedKey"):
                me_response = ddb.scan(
                    TableName=MANUAL_ENTRIES_TABLE,
                    ExclusiveStartKey=me_response["LastEvaluatedKey"]
                )
                me_items.extend(me_response.get("Items", []))

            manual_count = 0
            for me_item in me_items:
                me_period = me_item.get("period", {}).get("S", "")
                me_amount = float(me_item.get("amount", {}).get("N", "0"))
                me_property_id = me_item.get("property_id", {}).get("S", "")
                me_property_name = me_item.get("property_name", {}).get("S", "")
                me_charge_code = me_item.get("charge_code", {}).get("S", "")
                me_utility_name = me_item.get("utility_name", {}).get("S", "")
                me_entry_type = me_item.get("entry_type", {}).get("S", "")
                me_reason_code = me_item.get("reason_code", {}).get("S", "")
                me_note = me_item.get("note", {}).get("S", "")
                me_account_number = me_item.get("account_number", {}).get("S", "")
                me_vendor_name = me_item.get("vendor_name", {}).get("S", "")
                me_entry_id = me_item.get("entry_id", {}).get("S", "")

                if not me_period or not me_property_id:
                    continue

                # Apply same period filter
                if start_period or end_period:
                    try:
                        p_month, p_year = me_period.split("/")
                        period_yyyymm = f"{p_year}-{p_month.zfill(2)}"
                    except:
                        continue
                    if start_period:
                        try:
                            s_month, s_year = start_period.split("/")
                            if period_yyyymm < f"{s_year}-{s_month.zfill(2)}":
                                continue
                        except:
                            pass
                    if end_period:
                        try:
                            e_month, e_year = end_period.split("/")
                            if period_yyyymm > f"{e_year}-{e_month.zfill(2)}":
                                continue
                        except:
                            pass

                # Build period dates
                import calendar
                try:
                    p_month, p_year = me_period.split("/")
                    period_start = f"{p_month}/01/{p_year}"
                    last_day = calendar.monthrange(int(p_year), int(p_month))[1]
                    period_end = f"{p_month}/{last_day:02d}/{p_year}"
                except:
                    continue

                mb_key = f"{me_property_id}|{me_charge_code}|{me_utility_name}|{me_period}|{me_period}"

                if mb_key not in master_bills:
                    master_bills[mb_key] = {
                        "master_bill_id": mb_key,
                        "property_id": me_property_id,
                        "property_name": me_property_name,
                        "ar_code_mapping": me_charge_code,
                        "utility_name": me_utility_name,
                        "billback_month_start": period_start,
                        "billback_month_end": period_end,
                        "utility_amount": 0,
                        "source_line_items": [],
                        "has_non_actual": False,
                        "created_utc": datetime.utcnow().isoformat(),
                        "created_by": user,
                        "status": "draft"
                    }

                master_bills[mb_key]["utility_amount"] += me_amount
                master_bills[mb_key]["has_non_actual"] = True

                master_bills[mb_key]["source_line_items"].append({
                    "bill_id": me_entry_id,
                    "line_index": 0,
                    "account_number": me_account_number,
                    "vendor_name": me_vendor_name,
                    "gl_code": me_item.get("gl_account_number", {}).get("S", ""),
                    "gl_code_name": me_item.get("gl_account_name", {}).get("S", ""),
                    "description": f"{me_entry_type}: {me_reason_code}",
                    "amount": me_amount,
                    "overridden": False,
                    "override_reason": "",
                    "entry_type": me_entry_type,
                    "reason_code": me_reason_code,
                    "note": me_note
                })
                manual_count += 1

            print(f"[GENERATE MASTER BILLS] Merged {manual_count} manual/accrual entries")

        except Exception as e:
            print(f"[GENERATE MASTER BILLS] Error merging manual entries: {e}")
            import traceback
            traceback.print_exc()

        # Convert to list and save
        master_bills_list = list(master_bills.values())

        print(f"[GENERATE MASTER BILLS] Created {len(master_bills_list)} master bills from {len(all_line_items)} line items + manual entries")

        # Store master bills in S3 (handles large datasets without size limits)
        save_ok = _s3_put_master_bills(master_bills_list)
        if not save_ok:
            print(f"[GENERATE MASTER BILLS] ERROR: Failed to save master bills to S3!")
            return JSONResponse({"error": "Failed to save master bills - please try again"}, status_code=500)

        total_amount = sum(mb["utility_amount"] for mb in master_bills_list)
        print(f"[GENERATE MASTER BILLS] Total amount: ${total_amount:.2f}")

        return {
            "ok": True,
            "count": len(master_bills_list),
            "total_amount": total_amount
        }

    except Exception as e:
        print(f"[GENERATE MASTER BILLS] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/master-bills/list")
def api_list_master_bills(user: str = Depends(require_user)):
    """List all generated master bills, including manual entries"""
    master_bills = _s3_get_master_bills()
    if not isinstance(master_bills, list):
        master_bills = []

    # Build property name  lookup_code and property name  property_id maps
    _prop_name_to_lookup = {}
    _prop_name_to_id = {}
    try:
        _prop_cache = _CACHE.get(("catalog_properties",), {}).get("data")
        if not _prop_cache:
            _prop_rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            for _pr in _prop_rows:
                _pname = str(_pr.get("name") or _pr.get("NAME") or _pr.get("PROPERTY_NAME") or _pr.get("propertyName") or _pr.get("Property Name") or "").strip().lower()
                _lc = str(_pr.get("LOOKUP_CODE") or _pr.get("lookup_code") or _pr.get("LookupCode") or _pr.get("PROPERTY_CODE") or "").strip()
                _pid = str(_pr.get("id") or _pr.get("ID") or _pr.get("propertyId") or _pr.get("PROPERTY_ID") or "").strip()
                if _pname and _lc:
                    _prop_name_to_lookup[_pname] = _lc
                if _pname and _pid:
                    _prop_name_to_id[_pname] = _pid
        else:
            for _pi in _prop_cache:
                if _pi.get("name") and _pi.get("lookup_code"):
                    _prop_name_to_lookup[_pi["name"].strip().lower()] = _pi["lookup_code"]
                if _pi.get("name") and _pi.get("id"):
                    _prop_name_to_id[_pi["name"].strip().lower()] = str(_pi["id"])
    except Exception:
        pass

    # Also build from accounts tracker (more reliable for property name  ID)
    try:
        _accounts = _get_accounts_to_track()
        for _acc in _accounts:
            _pname = str(_acc.get("propertyName", "")).strip().lower()
            _pid = str(_acc.get("propertyId", "")).strip()
            if _pname and _pid:
                _prop_name_to_id[_pname] = _pid
    except Exception:
        pass

    # Debug: log all master bill IDs
    print(f"[MASTER BILLS LIST] Found {len(master_bills)} generated master bills")

    # Convert master_bills list to dict for aggregation (manual entries merge into matching keys)
    mb_by_key = {}
    for mb in master_bills:
        mb_key = f"{mb.get('property_id', '')}|{mb.get('ar_code_mapping', '')}|{mb.get('utility_name', '')}|{mb.get('ubi_period', '')}"
        mb_by_key[mb_key] = mb

    # Merge in manual entries (aggregate by property_id + charge_code + utility + period)
    try:
        import calendar
        manual_count = 0
        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(TableName=MANUAL_BILLBACK_TABLE):
            for item in page.get('Items', []):
                # Convert DynamoDB item to master bill format
                ubi_period = item.get('ubi_period', {}).get('S', '')  # MM/YYYY format
                start_date = item.get('start_date', {}).get('S', '')
                end_date = item.get('end_date', {}).get('S', '')

                # Convert ubi_period to billback dates if start/end not provided
                if not start_date or not end_date:
                    try:
                        # Parse MM/YYYY format
                        period_month, period_year = ubi_period.split("/")
                        # First day of month
                        start_date = f"{int(period_month):02d}/01/{period_year}"
                        # Last day of month
                        last_day = calendar.monthrange(int(period_year), int(period_month))[1]
                        end_date = f"{int(period_month):02d}/{last_day:02d}/{period_year}"
                    except (ValueError, IndexError):
                        start_date = ubi_period
                        end_date = ubi_period

                # Normalize to MM/DD/YYYY format
                start_date = _normalize_date_mmddyyyy(start_date)
                end_date = _normalize_date_mmddyyyy(end_date)

                _manual_pname = item.get('property_name', {}).get('S', '')
                _manual_pid = _prop_name_to_id.get(_manual_pname.strip().lower(), '')
                _manual_cc = item.get('charge_code', {}).get('S', '')
                _manual_utility = item.get('utility_type', {}).get('S', '')
                _manual_amount = float(item.get('amount', {}).get('N', '0'))
                _manual_acct = item.get('account_number', {}).get('S', '')
                _manual_vendor = item.get('vendor_name', {}).get('S', '')
                _manual_desc = item.get('description', {}).get('S', 'Manual Entry')
                _manual_entry_id = item.get('entry_id', {}).get('S', '')

                # Build aggregation key matching generated master bills
                agg_key = f"{_manual_pid}|{_manual_cc}|{_manual_utility}|{ubi_period}"

                line_item = {
                    'description': _manual_desc,
                    'amount': _manual_amount,
                    'account_number': _manual_acct,
                    'vendor_name': _manual_vendor,
                    'is_manual': True
                }

                if agg_key in mb_by_key:
                    # Merge into existing master bill (aggregate)
                    existing = mb_by_key[agg_key]
                    existing['utility_amount'] += _manual_amount
                    existing['line_items'].append(line_item)
                    existing['source_line_items'].append(line_item)
                    existing['is_manual'] = True  # Mark as having manual entries
                else:
                    # Create new master bill entry
                    manual_mb = {
                        'master_bill_id': f"manual-{_manual_entry_id}",
                        'property_id': _manual_pid,
                        'property_name': _manual_pname,
                        'property_lookup_code': _prop_name_to_lookup.get(_manual_pname.strip().lower(), ''),
                        'ar_code_mapping': _manual_cc,
                        'utility_name': _manual_utility,
                        'utility_amount': _manual_amount,
                        'ubi_period': ubi_period,
                        'billback_month_start': start_date,
                        'billback_month_end': end_date,
                        'vendor_name': _manual_vendor,
                        'account_number': _manual_acct,
                        'line_items': [line_item],
                        'source_line_items': [line_item],
                        'status': 'manual',
                        'is_manual': True,
                        'manual_source': item.get('source', {}).get('S', 'Manual Upload'),
                        'uploaded_by': item.get('uploaded_by', {}).get('S', ''),
                        'uploaded_at': item.get('uploaded_at', {}).get('S', '')
                    }
                    mb_by_key[agg_key] = manual_mb
                manual_count += 1

        # Rebuild master_bills from the aggregated dict
        master_bills = list(mb_by_key.values())
        print(f"[MASTER BILLS LIST] Added {manual_count} manual entries (aggregated)")
    except Exception as e:
        print(f"[MASTER BILLS LIST] Error loading manual entries: {e}")

    print(f"[MASTER BILLS LIST] Returning {len(master_bills)} total master bills")
    return {"items": master_bills, "count": len(master_bills)}


@app.get("/api/master-bills/manual-template")
def api_manual_template():
    """Return a downloadable CSV template for manual billback uploads.
    Required: property_name, charge_code, amount, ubi_period, utility_type, start_date, end_date
    Optional: vendor_name, account_number, description
    Date format: MM/DD/YYYY, UBI Period format: MM/YYYY
    Includes reference sections with all valid values for easy copy-paste.
    """
    import io
    from datetime import datetime

    # Load valid properties with codes
    properties = []  # list of (name, code) tuples
    try:
        dim_props = _load_dim_records(DIM_PROPERTY_PREFIX)
        for r in dim_props:
            name = r.get("PROPERTY_NAME") or r.get("name") or r.get("NAME") or r.get("propertyName") or ""
            code = (
                r.get("LOOKUP_CODE") or r.get("lookup_code") or r.get("LookupCode")
                or r.get("PROPERTY_CODE") or r.get("code") or r.get("CODE") or ""
            )
            if name and str(name).strip():
                properties.append((str(name).strip(), str(code).strip()))
        # Deduplicate by name, keep first code found
        seen = set()
        unique_props = []
        for name, code in properties:
            if name not in seen:
                seen.add(name)
                unique_props.append((name, code))
        properties = sorted(unique_props, key=lambda x: x[0])
    except:
        properties = [("(Error loading properties)", "")]

    # Load valid charge codes from CONFIG table - keep code and utility paired
    charge_code_pairs = []  # List of (code, utility) tuples to maintain pairing
    try:
        cc_config = _ddb_get_config("charge-codes")
        if isinstance(cc_config, list):
            for r in cc_config:
                code = str(r.get("chargeCode") or "").strip()
                utility = str(r.get("utilityName") or "").strip()
                if code:
                    charge_code_pairs.append((code, utility))
        # Remove duplicates while preserving order
        seen = set()
        unique_pairs = []
        for pair in charge_code_pairs:
            if pair[0] not in seen:
                seen.add(pair[0])
                unique_pairs.append(pair)
        # Sort: billback codes first (contain " - "), then others, both alphabetically
        def sort_key(pair):
            code = pair[0]
            # Primary billback codes have " - " pattern (e.g., "UBILL - UTILITY BILLING")
            is_billback = " - " in code and "Not Billed Back" not in pair[1]
            # Sort billback codes first (0), then non-billback (1)
            return (0 if is_billback else 1, code)
        charge_code_pairs = sorted(unique_pairs, key=sort_key)
    except:
        charge_code_pairs = [("(Error)", "loading charge codes")]

    # Valid utility types
    utility_types = ["Electric", "Gas", "Water", "Sewer", "Trash", "Cable", "Internet", "Phone", "Storm Water", "Environmental Fee", "Pest Control", "HOA Fee", "Other"]

    # Dynamic dates based on current month
    now = datetime.now()
    current_period = f"{now.month:02d}/{now.year}"
    example_start = f"{now.month:02d}/01/{now.year}"
    example_end = f"{now.month:02d}/28/{now.year}"

    # Build CSV content
    output = io.StringIO()

    # ============ SECTION 1: DATA ENTRY AREA ============
    # Include all columns (required + optional)
    output.write("property_name,property_code,charge_code,amount,ubi_period,utility_type,start_date,end_date,vendor_name,account_number,description\n")

    # Add a few example rows using real data
    example_props = properties[:3] if len(properties) >= 3 else [("Property Name Here", "PROP1"), ("Property Name Here", "PROP2"), ("Property Name Here", "PROP3")]
    example_ccs = [p[0] for p in charge_code_pairs[:3]] if len(charge_code_pairs) >= 3 else ["5706-0000", "5710-0000", "5565-0000"]

    output.write(f"{example_props[0][0]},{example_props[0][1]},{example_ccs[0]},1234.56,{current_period},Electric,{example_start},{example_end},,,Common area electric\n")
    output.write(f"{example_props[1][0] if len(example_props) > 1 else example_props[0][0]},{example_props[1][1] if len(example_props) > 1 else example_props[0][1]},{example_ccs[1] if len(example_ccs) > 1 else example_ccs[0]},567.89,{current_period},Water,{example_start},{example_end},,,\n")
    output.write(f"{example_props[2][0] if len(example_props) > 2 else example_props[0][0]},{example_props[2][1] if len(example_props) > 2 else example_props[0][1]},{example_ccs[2] if len(example_ccs) > 2 else example_ccs[0]},890.12,{current_period},Gas,{example_start},{example_end},,,\n")

    # Add blank rows for data entry (11 columns = 10 commas)
    for _ in range(10):
        output.write(",,,,,,,,,,\n")

    # ============ SECTION 2: REFERENCE - MUST DELETE BEFORE UPLOAD ============
    output.write("\n")
    output.write("\n")
    output.write("DELETE EVERYTHING BELOW THIS LINE BEFORE UPLOADING,,,,,,,,,,\n")
    output.write("=========================================================,,,,,,,,,,\n")
    output.write("\n")
    output.write("INSTRUCTIONS:,,,,,,,,,,\n")
    output.write("1. Enter your data in the rows at the TOP,,,,,,,,,,\n")
    output.write("2. Copy values from reference lists below,,,,,,,,,,\n")
    output.write("3. DELETE all rows from here down before upload,,,,,,,,,,\n")
    output.write("4. Date format: MM/DD/YYYY,,,,,,,,,,\n")
    output.write("5. UBI Period format: MM/YYYY,,,,,,,,,,\n")
    output.write("6. Required: property_name + property_code + charge_code + amount + ubi_period + utility_type + start_date + end_date,,,,,,,,,,\n")
    output.write("7. Optional: vendor_name + account_number + description,,,,,,,,,,\n")
    output.write("\n")

    # Utility Types reference
    output.write("----- UTILITY TYPES (copy from column A) -----,,,,,,,,,,\n")
    for ut in utility_types:
        output.write(f"{ut},,,,,,,,,,\n")

    output.write("\n")

    # Charge Codes reference (with utility name for reference in column B)
    output.write("----- CHARGE CODES (copy from column A) -----,Maps to Utility:,,,,,,,,,\n")
    for code, utility in charge_code_pairs:
        output.write(f"{code},{utility},,,,,,,,,\n")

    output.write("\n")

    # Properties reference (name in column A, code in column B)
    output.write("----- PROPERTIES (name in A / code in B) -----,Property Code:,,,,,,,,,\n")
    for prop_name, prop_code in properties:
        safe_prop = f'"{prop_name}"' if ',' in prop_name else prop_name
        output.write(f"{safe_prop},{prop_code},,,,,,,,,\n")

    csv_content = output.getvalue()

    return StreamingResponse(
        BytesIO(csv_content.encode('utf-8')),
        media_type="text/csv",
        headers={"Content-Disposition": "attachment; filename=manual_billback_template.csv"}
    )


@app.get("/api/master-bills/detail")
def api_master_bill_detail(id: str, user: str = Depends(require_user)):
    """Get detail of a specific master bill with drill-down"""
    # Handle manual entries  they live in DDB, not S3
    if id.startswith("manual-"):
        import calendar
        entry_id = id[len("manual-"):]
        print(f"[MASTER BILL DETAIL] Looking up manual entry: {entry_id}")
        try:
            resp = ddb.get_item(
                TableName=MANUAL_BILLBACK_TABLE,
                Key={"entry_id": {"S": entry_id}}
            )
            item = resp.get("Item")
            if not item:
                return JSONResponse({"error": "Manual entry not found"}, status_code=404)

            ubi_period = item.get('ubi_period', {}).get('S', '')
            start_date = item.get('start_date', {}).get('S', '')
            end_date = item.get('end_date', {}).get('S', '')
            if not start_date or not end_date:
                try:
                    period_month, period_year = ubi_period.split("/")
                    start_date = f"{int(period_month):02d}/01/{period_year}"
                    last_day = calendar.monthrange(int(period_year), int(period_month))[1]
                    end_date = f"{int(period_month):02d}/{last_day:02d}/{period_year}"
                except (ValueError, IndexError):
                    start_date = ubi_period
                    end_date = ubi_period
            start_date = _normalize_date_mmddyyyy(start_date)
            end_date = _normalize_date_mmddyyyy(end_date)

            amount = float(item.get('amount', {}).get('N', '0'))
            return {
                'master_bill_id': id,
                'property_name': item.get('property_name', {}).get('S', ''),
                'ar_code_mapping': item.get('charge_code', {}).get('S', ''),
                'utility_name': item.get('utility_type', {}).get('S', ''),
                'utility_amount': amount,
                'ubi_period': ubi_period,
                'billback_month_start': start_date,
                'billback_month_end': end_date,
                'vendor_name': item.get('vendor_name', {}).get('S', ''),
                'account_number': item.get('account_number', {}).get('S', ''),
                'source_line_items': [{
                    'description': item.get('description', {}).get('S', 'Manual Entry'),
                    'amount': amount,
                    'account_number': item.get('account_number', {}).get('S', ''),
                    'vendor_name': item.get('vendor_name', {}).get('S', ''),
                    'gl_code': item.get('charge_code', {}).get('S', ''),
                    'gl_code_name': item.get('utility_type', {}).get('S', ''),
                    'is_manual': True
                }],
                'status': 'manual',
                'is_manual': True,
                'manual_source': item.get('source', {}).get('S', 'Manual Upload'),
                'uploaded_by': item.get('uploaded_by', {}).get('S', ''),
                'uploaded_at': item.get('uploaded_at', {}).get('S', '')
            }
        except Exception as e:
            print(f"[MASTER BILL DETAIL] Error loading manual entry: {e}")
            return JSONResponse({"error": f"Error loading manual entry: {e}"}, status_code=500)

    master_bills = _s3_get_master_bills()
    if not isinstance(master_bills, list):
        return JSONResponse({"error": "not found"}, status_code=404)

    print(f"[MASTER BILL DETAIL] Looking for ID: {id}")

    for mb in master_bills:
        if mb.get("master_bill_id") == id:
            return mb

    return JSONResponse({"error": "not found"}, status_code=404)


@app.get("/api/master-bills/diagnose")
def api_diagnose_master_bills(period: str = "", user: str = Depends(require_user)):
    """
    Diagnose why some properties show in Completion Tracker but not in Master Bills.
    Returns detailed analysis of missing properties and the reasons.
    Now reads from S3 Stage 8 instead of DynamoDB.
    """
    from datetime import datetime, timedelta
    from concurrent.futures import ThreadPoolExecutor, as_completed

    print(f"[DIAGNOSE] Starting diagnosis for period: {period or 'all'}")

    # 1. Get properties from Stage 8 (assigned items)
    tracker_properties = {}  # property_id -> {name, accounts: set of account_numbers}
    skip_reasons = {}  # property_id -> {reason: count}

    try:
        # Build date-partitioned prefixes for the last 365 days
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(365):
            d = today - timedelta(days=i)
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        # Collect all S3 keys from Stage 8
        all_keys = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            all_keys.append(key)
            except Exception:
                pass

        print(f"[DIAGNOSE] Found {len(all_keys)} files in Stage 8")

        def process_file(key):
            """Process a single Stage 8 file."""
            results = []
            try:
                body = _read_s3_text(BUCKET, key)
                for line in body.splitlines():
                    line = (line or '').strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)

                        # Handle multi-period format
                        ubi_assignments = rec.get("ubi_assignments", [])
                        if ubi_assignments:
                            for asn in ubi_assignments:
                                asn_period = asn.get("period", "")
                                if period and asn_period != period:
                                    continue
                                if asn_period:
                                    results.append({
                                        "rec": rec,
                                        "period": asn_period,
                                        "amount": asn.get("amount", 0)
                                    })
                        else:
                            # Legacy format
                            rec_period = rec.get("ubi_period", "")
                            if period and rec_period != period:
                                continue
                            if rec_period:
                                results.append({
                                    "rec": rec,
                                    "period": rec_period,
                                    "amount": rec.get("ubi_amount", 0)
                                })
                    except Exception:
                        continue
            except Exception:
                pass
            return results

        # Process files concurrently
        all_items = []
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                all_items.extend(future.result())

        print(f"[DIAGNOSE] Found {len(all_items)} assignments for period {period or 'all'}")

        # Analyze each assignment
        for item in all_items:
            rec = item["rec"]
            property_id = rec.get("EnrichedPropertyID", rec.get("Property ID", ""))
            property_name = rec.get("EnrichedPropertyName", rec.get("Property Name", ""))
            account_number = rec.get("Account Number", rec.get("AccountNumber", ""))
            charge_code = rec.get("Charge Code", "")
            is_excluded = rec.get("Is Excluded From UBI", 0)

            if not property_id:
                continue

            # Track this property
            if property_id not in tracker_properties:
                tracker_properties[property_id] = {
                    "property_name": property_name,
                    "accounts": set(),
                    "total_assignments": 0,
                    "valid_for_master_bill": 0
                }
            tracker_properties[property_id]["accounts"].add(account_number)
            tracker_properties[property_id]["total_assignments"] += 1

            # Check validity for master bill
            skip_reason = None
            if is_excluded:
                skip_reason = "excluded_from_ubi"

            # Track unmapped charge codes
            if not charge_code or charge_code == "N/A":
                if property_id not in skip_reasons:
                    skip_reasons[property_id] = {}
                skip_reasons[property_id]["unmapped_charge_code"] = skip_reasons[property_id].get("unmapped_charge_code", 0) + 1

            if skip_reason:
                if property_id not in skip_reasons:
                    skip_reasons[property_id] = {}
                skip_reasons[property_id][skip_reason] = skip_reasons[property_id].get(skip_reason, 0) + 1
            else:
                tracker_properties[property_id]["valid_for_master_bill"] += 1

    except Exception as e:
        print(f"[DIAGNOSE] Error loading assignments from Stage 8: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)

    # 2. Get properties from generated Master Bills
    master_bills = _s3_get_master_bills()
    if not isinstance(master_bills, list):
        master_bills = []

    master_bill_properties = set()
    for mb in master_bills:
        # Filter by period if provided
        if period:
            mb_period_start = mb.get("billback_month_start", "")  # Format: MM/DD/YYYY
            try:
                # Extract MM/YYYY from MM/DD/YYYY
                parts = mb_period_start.split("/")
                mb_period = f"{parts[0]}/{parts[2]}" if len(parts) == 3 else ""
                if mb_period != period:
                    continue
            except Exception:
                continue
        master_bill_properties.add(mb.get("property_id", ""))

    print(f"[DIAGNOSE] Tracker shows {len(tracker_properties)} properties with assignments")
    print(f"[DIAGNOSE] Master Bills has {len(master_bill_properties)} properties")

    # 3. Find missing properties and explain why
    missing_properties = []
    for prop_id, prop_data in tracker_properties.items():
        if prop_id not in master_bill_properties:
            reasons = skip_reasons.get(prop_id, {})
            missing_properties.append({
                "property_id": prop_id,
                "property_name": prop_data["property_name"],
                "account_count": len(prop_data["accounts"]),
                "total_assignments": prop_data["total_assignments"],
                "valid_for_master_bill": prop_data["valid_for_master_bill"],
                "skip_reasons": reasons
            })

    # Sort by property name
    missing_properties.sort(key=lambda x: x.get("property_name", ""))

    return {
        "period": period,
        "tracker_property_count": len(tracker_properties),
        "master_bills_property_count": len(master_bill_properties),
        "missing_count": len(missing_properties),
        "missing_properties": missing_properties,
        "summary": {
            "total_excluded_from_ubi": sum(
                p.get("skip_reasons", {}).get("excluded_from_ubi", 0)
                for p in missing_properties
            ),
            "total_unmapped_charge_codes": sum(
                p.get("skip_reasons", {}).get("unmapped_charge_code", 0)
                for p in missing_properties
            )
        }
    }


@app.post("/api/master-bills/exclude-line")
async def api_exclude_master_bill_line(request: Request, user: str = Depends(require_user)):
    """Exclude a line item from a master bill"""
    try:
        payload = await request.json()
        master_bill_id = payload.get("master_bill_id", "").strip()
        line_hash = payload.get("line_hash", "").strip()
        is_excluded = bool(payload.get("is_excluded", False))
        exclusion_reason = payload.get("exclusion_reason", "").strip()

        if not master_bill_id or not line_hash:
            return JSONResponse({"error": "master_bill_id and line_hash required"}, status_code=400)

        # Load master bills
        master_bills = _s3_get_master_bills()
        if not isinstance(master_bills, list):
            return JSONResponse({"error": "master bills not found"}, status_code=404)

        # Find and update the master bill
        updated = False
        for mb in master_bills:
            if mb.get("master_bill_id") == master_bill_id:
                # Find and update the line item
                source_lines = mb.get("source_line_items", [])
                for line in source_lines:
                    if line.get("line_hash") == line_hash:
                        line["is_excluded"] = is_excluded
                        line["exclusion_reason"] = exclusion_reason if is_excluded else ""
                        updated = True
                        break

                if updated:
                    # Recalculate utility_amount excluding excluded lines
                    new_total = sum(
                        line.get("amount", 0)
                        for line in source_lines
                        if not line.get("is_excluded", False)
                    )
                    mb["utility_amount"] = new_total
                break

        if not updated:
            return JSONResponse({"error": "line item not found"}, status_code=404)

        # Save updated master bills to S3
        _s3_put_master_bills(master_bills)

        return {"ok": True, "message": "Line item exclusion updated"}

    except Exception as e:
        print(f"[EXCLUDE LINE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/master-bills/reclassify")
async def api_reclassify_master_bills(request: Request, user: str = Depends(require_user)):
    """Reclassify charge code and/or utility type on one or more master bill rows.
    Merges rows that end up with the same key after reclassification."""
    try:
        payload = await request.json()
        master_bill_ids = payload.get("master_bill_ids", [])
        new_utility = (payload.get("new_utility") or "").strip()
        new_charge_code = (payload.get("new_charge_code") or "").strip()
        if not master_bill_ids:
            return JSONResponse({"error": "master_bill_ids required"}, status_code=400)
        if not new_utility and not new_charge_code:
            return JSONResponse({"error": "new_utility and/or new_charge_code required"}, status_code=400)

        master_bills = _s3_get_master_bills()
        if not isinstance(master_bills, list):
            return JSONResponse({"error": "no master bills found"}, status_code=404)

        changed = 0
        ids_set = set(master_bill_ids)
        for mb in master_bills:
            if mb.get("master_bill_id") in ids_set:
                did_change = False

                if new_utility:
                    old_utility = mb.get("utility_name", "")
                    if old_utility != new_utility:
                        mb["utility_name"] = new_utility
                        for line in mb.get("source_line_items", []):
                            if line.get("utility_name") == old_utility:
                                line["utility_name"] = new_utility
                        did_change = True

                if new_charge_code:
                    old_cc = mb.get("ar_code_mapping", "")
                    if old_cc != new_charge_code:
                        mb["ar_code_mapping"] = new_charge_code
                        did_change = True

                if did_change:
                    # Recompute master_bill_id: property_id|charge_code|utility_name|start|end
                    parts = mb["master_bill_id"].split("|")
                    if len(parts) == 5:
                        if new_charge_code:
                            parts[1] = new_charge_code
                        if new_utility:
                            parts[2] = new_utility
                        mb["master_bill_id"] = "|".join(parts)
                    changed += 1

        # Also update manual entries in DDB
        for mid in master_bill_ids:
            if mid.startswith("manual-"):
                entry_id = mid[len("manual-"):]
                try:
                    update_parts = []
                    expr_vals = {}
                    if new_utility:
                        update_parts.append("utility_type = :ut")
                        expr_vals[":ut"] = {"S": new_utility}
                    if new_charge_code:
                        update_parts.append("charge_code = :cc")
                        expr_vals[":cc"] = {"S": new_charge_code}
                    if update_parts:
                        ddb.update_item(
                            TableName=MANUAL_BILLBACK_TABLE,
                            Key={"entry_id": {"S": entry_id}},
                            UpdateExpression="SET " + ", ".join(update_parts),
                            ExpressionAttributeValues=expr_vals
                        )
                except Exception as e:
                    print(f"[RECLASSIFY] Error updating manual entry {entry_id}: {e}")

        # Merge master bills that now share the same key
        merged = {}
        for mb in master_bills:
            mb_key = mb.get("master_bill_id", "")
            if mb_key in merged:
                merged[mb_key]["utility_amount"] += mb.get("utility_amount", 0)
                merged[mb_key]["source_line_items"].extend(mb.get("source_line_items", []))
            else:
                merged[mb_key] = mb

        master_bills = list(merged.values())
        _s3_put_master_bills(master_bills)

        desc = []
        if new_charge_code:
            desc.append(f"charge_code='{new_charge_code}'")
        if new_utility:
            desc.append(f"utility='{new_utility}'")
        print(f"[RECLASSIFY] User {user} reclassified {changed} bills ({', '.join(desc)}), {len(master_bills)} total after merge")
        return {"ok": True, "changed": changed, "total": len(master_bills)}

    except Exception as e:
        print(f"[RECLASSIFY] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/master-bills/override-amount")
async def api_override_master_bill_amount(request: Request, user: str = Depends(require_user)):
    """
    Override the amount for a line item in Stage 8.
    This updates the source data so the next master bill generation will use the new amount.
    """
    try:
        form = await request.form()
        s3_key = form.get("s3_key", "").strip()
        line_hash = form.get("line_hash", "").strip()
        new_amount_str = form.get("new_amount", "").strip()
        override_reason = form.get("override_reason", "").strip()

        if not s3_key or not line_hash:
            return JSONResponse({"error": "s3_key and line_hash required"}, status_code=400)

        if not override_reason:
            return JSONResponse({"error": "override_reason required"}, status_code=400)

        try:
            new_amount = float(new_amount_str)
        except (ValueError, TypeError):
            return JSONResponse({"error": "new_amount must be a valid number"}, status_code=400)

        print(f"[OVERRIDE AMOUNT] s3_key={s3_key[:60]}..., line_hash={line_hash[:16]}..., new_amount={new_amount}")

        # Read the Stage 8 file
        try:
            txt = _read_s3_text(BUCKET, s3_key)
            lines = [json.loads(l) for l in txt.strip().split('\n') if l.strip()]
        except Exception as e:
            print(f"[OVERRIDE AMOUNT] Error reading {s3_key}: {e}")
            return JSONResponse({"error": f"Could not read file: {e}"}, status_code=404)

        # Find and update the line item by hash
        updated = False
        for line in lines:
            computed_hash = _compute_stable_line_hash(line)
            if computed_hash == line_hash:
                # Store original amount for reference
                original_amount = line.get("Line Item Charge", line.get("Current Amount", "0"))
                line["_original_amount"] = original_amount

                # Update amount fields
                line["Current Amount"] = new_amount
                line["Amount Overridden"] = True
                line["Amount Override Reason"] = override_reason
                line["Amount Override By"] = user
                line["Amount Override Date"] = datetime.utcnow().isoformat()

                # Also update ubi_amount in any assignments
                if line.get("ubi_assignments"):
                    for asn in line["ubi_assignments"]:
                        asn["amount"] = new_amount
                if line.get("ubi_amount"):
                    line["ubi_amount"] = new_amount

                updated = True
                print(f"[OVERRIDE AMOUNT] Updated line: original={original_amount}, new={new_amount}")
                break

        if not updated:
            return JSONResponse({"error": "Line item not found in file"}, status_code=404)

        # Write back to Stage 8
        new_content = '\n'.join(json.dumps(rec, ensure_ascii=False) for rec in lines)
        if s3_key.endswith('.gz'):
            body = gzip.compress(new_content.encode('utf-8'))
            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=body, ContentType='application/json', ContentEncoding='gzip')
        else:
            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=new_content.encode('utf-8'), ContentType='application/json')

        print(f"[OVERRIDE AMOUNT] Saved updated file to {s3_key}")

        return {"ok": True, "message": "Amount override saved. Regenerate master bills to see changes."}

    except Exception as e:
        print(f"[OVERRIDE AMOUNT] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# ============================================================================
# MANUAL BILLBACK ENTRIES
# ============================================================================


def _normalize_date_mmddyyyy(date_str: str) -> str:
    """Normalize a date string to MM/DD/YYYY with zero-padded month and day.
    Handles: M/D/YYYY, MM/DD/YYYY, M/DD/YYYY, MM/D/YYYY, and MM-DD-YYYY variants."""
    if not date_str or not date_str.strip():
        return date_str
    s = date_str.strip().replace("-", "/")
    parts = s.split("/")
    if len(parts) == 3:
        try:
            m, d, y = int(parts[0]), int(parts[1]), parts[2]
            if len(y) == 2:
                y = "20" + y
            return f"{m:02d}/{d:02d}/{y}"
        except (ValueError, IndexError):
            pass
    return date_str


MANUAL_BILLBACK_TABLE = os.getenv("MANUAL_BILLBACK_TABLE", "jrk-manual-billback-entries")


@app.post("/api/master-bills/upload-manual")
async def api_upload_manual_billback(
    request: Request,
    file: UploadFile = File(...),
    source: str = Form("Manual Upload"),
    add_to_tracker: str = Form("false"),
    add_to_ubi: str = Form("false"),
    user: str = Depends(require_user)
):
    """Upload manual billback data from CSV."""
    import csv
    import io
    import uuid
    from datetime import datetime

    try:
        # Read CSV content
        content = await file.read()
        text = content.decode('utf-8-sig')  # Handle BOM
        reader = csv.DictReader(io.StringIO(text))

        # Normalize headers
        if reader.fieldnames:
            reader.fieldnames = [h.strip().lower().replace(' ', '_') for h in reader.fieldnames]

        # Required columns
        required = {'property_name', 'charge_code', 'amount', 'ubi_period', 'utility_type', 'start_date', 'end_date'}
        if reader.fieldnames:
            missing = required - set(reader.fieldnames)
            if missing:
                return JSONResponse({"error": f"Missing required columns: {', '.join(missing)}"}, status_code=400)

        # Parse rows
        entries = []
        batch_id = f"manual-{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}-{uuid.uuid4().hex[:8]}"

        for row_num, row in enumerate(reader, start=2):
            try:
                # Parse amount
                amount_str = str(row.get('amount', '0') or '0').replace('$', '').replace(',', '').strip()
                amount = float(amount_str) if amount_str else 0.0

                entry = {
                    'entry_id': f"{batch_id}-{row_num:04d}",
                    'batch_id': batch_id,
                    'property_name': (row.get('property_name') or '').strip(),
                    'property_code': (row.get('property_code') or '').strip(),
                    'charge_code': (row.get('charge_code') or '').strip(),
                    'amount': amount,
                    'ubi_period': (row.get('ubi_period') or '').strip(),
                    'utility_type': (row.get('utility_type') or '').strip(),
                    'start_date': _normalize_date_mmddyyyy((row.get('start_date') or '').strip()),
                    'end_date': _normalize_date_mmddyyyy((row.get('end_date') or '').strip()),
                    'vendor_name': (row.get('vendor_name') or '').strip(),
                    'account_number': (row.get('account_number') or '').strip(),
                    'description': (row.get('description') or '').strip(),
                    'source': source,
                    'uploaded_by': user,
                    'uploaded_at': datetime.utcnow().isoformat() + 'Z',
                    'is_manual': True
                }

                if entry['property_name'] and entry['amount'] != 0:
                    entries.append(entry)

            except Exception as e:
                print(f"[MANUAL UPLOAD] Error parsing row {row_num}: {e}")
                continue

        if not entries:
            return JSONResponse({"error": "No valid entries found in CSV"}, status_code=400)

        # Store entries in DynamoDB
        for entry in entries:
            try:
                item_to_store = {
                        'entry_id': {'S': entry['entry_id']},
                        'batch_id': {'S': entry['batch_id']},
                        'property_name': {'S': entry['property_name']},
                        'charge_code': {'S': entry['charge_code']},
                        'amount': {'N': str(entry['amount'])},
                        'ubi_period': {'S': entry['ubi_period']},
                        'utility_type': {'S': entry['utility_type']},
                        'start_date': {'S': entry['start_date']},
                        'end_date': {'S': entry['end_date']},
                        'vendor_name': {'S': entry['vendor_name']},
                        'account_number': {'S': entry['account_number']},
                        'description': {'S': entry['description']},
                        'source': {'S': entry['source']},
                        'uploaded_by': {'S': entry['uploaded_by']},
                        'uploaded_at': {'S': entry['uploaded_at']},
                        'is_manual': {'BOOL': True}
                    }
                if entry.get('property_code'):
                    item_to_store['property_code'] = {'S': entry['property_code']}
                ddb.put_item(TableName=MANUAL_BILLBACK_TABLE, Item=item_to_store)
            except Exception as e:
                print(f"[MANUAL UPLOAD] Error storing entry {entry['entry_id']}: {e}")

        print(f"[MANUAL UPLOAD] Uploaded {len(entries)} entries in batch {batch_id}")

        # Optionally add accounts to tracker
        accounts_added = 0
        if add_to_tracker == "true":
            try:
                arr = _get_accounts_to_track()
                # Build lookup of existing accounts by ID key (matches gap_analysis pattern)
                existing_keys = set()
                for a in arr:
                    ek = f"{a.get('propertyId', '')}|{a.get('vendorId', '')}|{a.get('accountNumber', '')}"
                    existing_keys.add(ek)

                # Build property name -> ID lookup from dim table (field names match api_catalog_properties)
                prop_name_to_id = {}
                try:
                    dim_props = _load_dim_records(DIM_PROPERTY_PREFIX)
                    for r in dim_props:
                        pname = (r.get("name") or r.get("NAME") or r.get("propertyName") or
                                 r.get("Property Name") or r.get("PROPERTY_NAME") or
                                 r.get("Property") or r.get("PROPERTY") or "").strip()
                        pid = str(r.get("propertyId") or r.get("PROPERTY_ID") or
                                  r.get("Property ID") or r.get("id") or
                                  r.get("PROPERTYID") or r.get("PROP_ID") or "").strip()
                        if pname and pid:
                            prop_name_to_id[pname.lower()] = pid
                except Exception:
                    pass

                # Build vendor name -> ID lookup from dim table (field names match api_catalog_vendors)
                vendor_name_to_id = {}
                try:
                    dim_vendors = _load_dim_records(DIM_VENDOR_PREFIX)
                    for r in dim_vendors:
                        vname = (r.get("name") or r.get("NAME") or r.get("vendorName") or
                                 r.get("Vendor Name") or r.get("Vendor") or
                                 r.get("VENDOR_NAME") or r.get("VENDOR") or "").strip()
                        vid = str(r.get("vendorId") or r.get("VENDOR_ID") or
                                  r.get("Vendor ID") or r.get("id") or
                                  r.get("vendor_id") or r.get("VENDORID") or "").strip()
                        if vname and vid:
                            vendor_name_to_id[vname.lower()] = vid
                except Exception:
                    pass

                # Collect unique accounts from uploaded entries
                seen_upload = set()
                for entry in entries:
                    pname = entry.get("property_name", "").strip()
                    vname = entry.get("vendor_name", "").strip()
                    acct = entry.get("account_number", "").strip()
                    if not pname or not acct:
                        continue

                    pid = prop_name_to_id.get(pname.lower(), "")
                    vid = vendor_name_to_id.get(vname.lower(), "") if vname else ""

                    # Dedup by ID key (matches rest of system)
                    dedup = f"{pid}|{vid}|{acct}"
                    if dedup in seen_upload or dedup in existing_keys:
                        continue
                    seen_upload.add(dedup)

                    arr.append({
                        "vendorId": vid,
                        "vendorName": vname,
                        "accountNumber": acct,
                        "propertyId": pid,
                        "propertyName": pname,
                        "glAccountNumber": entry.get("charge_code", ""),
                        "glAccountName": "",
                        "daysBetweenBills": 365,
                        "is_tracked": True,
                        "is_ubi": add_to_ubi == "true",
                        "status": "active",
                        "comment": f"Added via manual upload by {user}"
                    })
                    accounts_added += 1

                if accounts_added > 0:
                    _put_accounts_to_track(arr)
                    _CACHE.pop(("accounts_to_track",), None)
                    print(f"[MANUAL UPLOAD] Added {accounts_added} accounts to tracker (is_ubi={add_to_ubi == 'true'})")
            except Exception as e:
                print(f"[MANUAL UPLOAD] Error adding accounts to tracker: {e}")

        return {"success": True, "count": len(entries), "batch_id": batch_id, "accounts_added": accounts_added}

    except Exception as e:
        print(f"[MANUAL UPLOAD] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/master-bills/manual-entries")
def api_list_manual_entries(
    period: str = "",
    user: str = Depends(require_user)
):
    """List all manual billback entries, optionally filtered by period."""
    try:
        # Scan all manual entries
        entries = []
        paginator = ddb.get_paginator('scan')

        for page in paginator.paginate(TableName=MANUAL_BILLBACK_TABLE):
            for item in page.get('Items', []):
                entry = {
                    'entry_id': item.get('entry_id', {}).get('S', ''),
                    'batch_id': item.get('batch_id', {}).get('S', ''),
                    'property_name': item.get('property_name', {}).get('S', ''),
                    'charge_code': item.get('charge_code', {}).get('S', ''),
                    'amount': float(item.get('amount', {}).get('N', '0')),
                    'ubi_period': item.get('ubi_period', {}).get('S', ''),
                    'utility_type': item.get('utility_type', {}).get('S', ''),
                    'start_date': item.get('start_date', {}).get('S', ''),
                    'end_date': item.get('end_date', {}).get('S', ''),
                    'vendor_name': item.get('vendor_name', {}).get('S', ''),
                    'account_number': item.get('account_number', {}).get('S', ''),
                    'description': item.get('description', {}).get('S', ''),
                    'source': item.get('source', {}).get('S', ''),
                    'uploaded_by': item.get('uploaded_by', {}).get('S', ''),
                    'uploaded_at': item.get('uploaded_at', {}).get('S', ''),
                    'is_manual': True
                }

                # Filter by period if specified
                if period:
                    if entry['ubi_period'] == period:
                        entries.append(entry)
                else:
                    entries.append(entry)

        return {"entries": entries, "count": len(entries)}

    except Exception as e:
        print(f"[MANUAL ENTRIES] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.delete("/api/master-bills/manual-entry/{entry_id}")
def api_delete_manual_entry(entry_id: str, user: str = Depends(require_user)):
    """Delete a single manual billback entry (tries both tables)."""
    try:
        ddb.delete_item(
            TableName=MANUAL_BILLBACK_TABLE,
            Key={'entry_id': {'S': entry_id}}
        )
        # Also try accrual entries table (covers both entry sources)
        try:
            ddb.delete_item(
                TableName=MANUAL_ENTRIES_TABLE,
                Key={'entry_id': {'S': entry_id}}
            )
        except Exception:
            pass
        # Bust completion tracker cache
        keys_to_bust = [k for k in list(_CACHE.keys()) if isinstance(k, tuple) and len(k) > 0 and k[0] == "completion_tracker"]
        for k in keys_to_bust:
            _CACHE.pop(k, None)
        print(f"[MANUAL DELETE] Deleted entry {entry_id} by {user}")
        return {"success": True, "deleted": entry_id}
    except Exception as e:
        print(f"[MANUAL DELETE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.delete("/api/master-bills/manual-batch/{batch_id}")
def api_delete_manual_batch(batch_id: str, user: str = Depends(require_user)):
    """Delete all entries from a manual upload batch."""
    try:
        # Scan for all entries with this batch_id
        entries_to_delete = []
        paginator = ddb.get_paginator('scan')

        for page in paginator.paginate(
            TableName=MANUAL_BILLBACK_TABLE,
            FilterExpression='batch_id = :bid',
            ExpressionAttributeValues={':bid': {'S': batch_id}}
        ):
            for item in page.get('Items', []):
                entries_to_delete.append(item.get('entry_id', {}).get('S', ''))

        # Delete all matching entries
        deleted = 0
        for entry_id in entries_to_delete:
            if entry_id:
                ddb.delete_item(
                    TableName=MANUAL_BILLBACK_TABLE,
                    Key={'entry_id': {'S': entry_id}}
                )
                deleted += 1

        return {"success": True, "deleted": deleted, "batch_id": batch_id}

    except Exception as e:
        print(f"[MANUAL BATCH DELETE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/master-bills/completion-tracker")
def api_completion_tracker(period: str = "", user: str = Depends(require_user)):
    """
    Get UBI completion tracker data showing which accounts have bills for a given period.
    Returns rollup by property, charge code, and account.
    """
    print(f"[COMPLETION TRACKER] Starting for period: {period or 'all'}")

    # Check cache first (5-minute TTL since this scans many files)
    cache_key = ("completion_tracker", period)
    now = time.time()
    ent = _CACHE.get(cache_key)
    if ent and (now - ent["ts"]) < 300:  # 5 minutes
        print(f"[COMPLETION TRACKER] Returning cached result")
        return ent["data"]

    # 0. Build set of cleansed scraper account IDs for flagging
    import re as _re
    _load_scraper_mappings()
    _scraper_acct_ids_clean = set()
    for _sa in _scraper_account_map.values():
        _aid = _sa.get("account_id", "")
        if _aid:
            _scraper_acct_ids_clean.add(_re.sub(r'[^A-Za-z0-9]', '', _aid).lower())
    print(f"[COMPLETION TRACKER] Loaded {len(_scraper_acct_ids_clean)} scraper account IDs")

    # 1. Get all UBI accounts from accounts_track config (S3-primary)
    accounts_track = _get_accounts_to_track()

    # Filter to only UBI accounts that are still tracked
    ubi_accounts = [acc for acc in accounts_track if acc.get("is_ubi", False) and acc.get("is_tracked", True)]
    print(f"[COMPLETION TRACKER] Found {len(ubi_accounts)} UBI accounts")

    # Load AP mapping to show AP name per property
    ap_mapping_raw = _ddb_get_config("ap-mapping") or []
    ap_by_property = {}  # property_id -> AP name
    for ap in ap_mapping_raw:
        if isinstance(ap, dict):
            pid = str(ap.get("propertyId") or "").strip()
            name = str(ap.get("name") or "").strip()
            if pid and name:
                ap_by_property[pid] = name
    print(f"[COMPLETION TRACKER] Loaded {len(ap_by_property)} AP mappings")

    # Build property name -> ID lookup for fallback when JSONL has empty Property ID
    property_name_to_id = {}
    try:
        prop_rows = _load_dim_records(DIM_PROPERTY_PREFIX)
        for r in prop_rows:
            pid = (r.get("propertyId") or r.get("PROPERTY_ID") or r.get("Property ID") or r.get("id") or r.get("PROPERTYID") or r.get("PROP_ID"))
            pname = (r.get("name") or r.get("NAME") or r.get("propertyName") or r.get("Property Name") or r.get("PROPERTY_NAME") or r.get("Property") or r.get("PROPERTY"))
            if pid and pname:
                property_name_to_id[str(pname).strip().lower()] = str(pid).strip()
        print(f"[COMPLETION TRACKER] Built property name->ID lookup with {len(property_name_to_id)} entries")
    except Exception as e:
        print(f"[COMPLETION TRACKER] Error building property name lookup: {e}")

    # 2. Get assigned accounts from S3 Stage 8 files
    assigned_accounts = set()  # Set of (property_id, account_number, vendor_name) tuples

    try:
        # Scan Stage 8 files for assignments
        # Look at recent 12 months of data
        from datetime import timedelta
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365)

        prefixes_to_scan = set()
        current = start_date
        while current <= end_date:
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={current.year}/mm={current.month:02d}/"
            prefixes_to_scan.add(prefix)
            # Move to next month
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1, day=1)
            else:
                current = current.replace(month=current.month + 1, day=1)

        # List all Stage 8 files IN PARALLEL
        def _list_tracker_prefix(prefix):
            keys = []
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        if obj['Key'].endswith('.jsonl'):
                            keys.append(obj['Key'])
            except Exception as e:
                print(f"[COMPLETION TRACKER] Error listing prefix {prefix}: {e}")
            return keys

        all_keys = []
        list_futures = [_GLOBAL_EXECUTOR.submit(_list_tracker_prefix, p) for p in prefixes_to_scan]
        for future in as_completed(list_futures):
            all_keys.extend(future.result())

        print(f"[COMPLETION TRACKER] Scanning {len(all_keys)} Stage 8 files")

        # Helper function to process a single S3 file
        # Returns (account_periods_dict, vacant_stats_dict, service_dates_dict)
        # account_periods_dict: (prop_id, acct_num, vendor_name) -> set of assigned periods
        # service_dates_dict: (prop_id, acct_num, vendor_name, period) -> {"start": date, "end": date, "s3_key": key}
        def process_s3_file(s3_key):
            acct_periods = {}  # acct_key -> set of periods
            file_vacant_stats = {}  # (prop_id, acct_num, vendor_name) -> {"vacant": N, "house": N}
            file_service_dates = {}  # (prop_id, acct_num, vendor_name, period) -> {"start": date, "end": date, "s3_key": key}

            # Parse property name from filename as fallback (format: PropertyName-VendorName-AccountNum-...)
            filename_prop_name = ""
            try:
                fname = s3_key.split("/")[-1]  # Get just the filename
                if "-" in fname:
                    filename_prop_name = fname.split("-")[0].strip()
            except:
                pass

            try:
                obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                body = obj["Body"].read()
                if s3_key.endswith('.gz'):
                    import gzip
                    body = gzip.decompress(body)
                lines = body.decode('utf-8').strip().split('\n')

                for line in lines:
                    try:
                        parsed = json.loads(line)
                        property_id = parsed.get("EnrichedPropertyID", parsed.get("Property ID", ""))
                        account_number = parsed.get("Account Number", parsed.get("AccountNumber", ""))
                        vendor_name = parsed.get("EnrichedVendorName", parsed.get("Vendor Name", parsed.get("VendorName", "")))

                        # Fallback: if property_id is empty, try to look it up from filename property name
                        if not property_id and filename_prop_name:
                            property_id = property_name_to_id.get(filename_prop_name.lower(), "")

                        if not property_id or not account_number:
                            continue
                        norm_acct = _normalize_account_number(account_number)
                        acct_key = (property_id, norm_acct, vendor_name)

                        # Get service dates from the line item
                        svc_start = parsed.get("Bill Period Start", parsed.get("billPeriodStart", ""))
                        svc_end = parsed.get("Bill Period End", parsed.get("billPeriodEnd", ""))

                        # Collect ALL assigned periods for this account
                        line_periods = set()
                        ubi_assignments = parsed.get("ubi_assignments", [])
                        if ubi_assignments:
                            for asn in ubi_assignments:
                                p = asn.get("period", "")
                                if p:
                                    line_periods.add(p)
                                    # Track service dates per account+period
                                    svc_key = (property_id, account_number, vendor_name, p)
                                    if svc_key not in file_service_dates:
                                        file_service_dates[svc_key] = {"start": svc_start, "end": svc_end, "s3_key": s3_key}
                        else:
                            lp = parsed.get("ubi_period", "")
                            if lp:
                                line_periods.add(lp)
                                svc_key = (property_id, account_number, vendor_name, lp)
                                if svc_key not in file_service_dates:
                                    file_service_dates[svc_key] = {"start": svc_start, "end": svc_end, "s3_key": s3_key}

                        if line_periods:
                            if acct_key not in acct_periods:
                                acct_periods[acct_key] = set()
                            acct_periods[acct_key].update(line_periods)

                        # Track vacant/house status (for all lines, not just matching period)
                        hov = str(parsed.get("House Or Vacant", "")).lower().strip()
                        if acct_key not in file_vacant_stats:
                            file_vacant_stats[acct_key] = {"vacant": 0, "house": 0}
                        if hov == "vacant":
                            file_vacant_stats[acct_key]["vacant"] += 1
                        elif hov == "house":
                            file_vacant_stats[acct_key]["house"] += 1
                    except Exception:
                        continue
            except Exception as e:
                pass  # Silently skip errors for individual files
            return acct_periods, file_vacant_stats, file_service_dates

        # Process files in parallel using ThreadPoolExecutor
        total_assignments = 0
        vacant_stats = {}  # (prop_id, acct_num, vendor_name) -> {"vacant": N, "house": N}
        account_all_periods = {}  # (prop_id, acct_num, vendor_name) -> set of ALL assigned periods
        service_dates = {}  # (prop_id, acct_num, vendor_name, period) -> {"start": date, "end": date, "s3_key": key}
        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = {executor.submit(process_s3_file, key): key for key in all_keys}
            for future in as_completed(futures):
                try:
                    acct_periods, file_vs, file_svc_dates = future.result()
                    for acct_key, periods_set in acct_periods.items():
                        if acct_key not in account_all_periods:
                            account_all_periods[acct_key] = set()
                        account_all_periods[acct_key].update(periods_set)
                        # Mark as assigned for selected period
                        if period and period in periods_set:
                            assigned_accounts.add(acct_key)
                            total_assignments += 1
                        elif not period:
                            assigned_accounts.add(acct_key)
                            total_assignments += 1
                    # Merge vacant stats
                    for acct_key, counts in file_vs.items():
                        if acct_key not in vacant_stats:
                            vacant_stats[acct_key] = {"vacant": 0, "house": 0}
                        vacant_stats[acct_key]["vacant"] += counts["vacant"]
                        vacant_stats[acct_key]["house"] += counts["house"]
                    # Merge service dates
                    for svc_key, svc_info in file_svc_dates.items():
                        if svc_key not in service_dates:
                            service_dates[svc_key] = svc_info
                except Exception as e:
                    pass

        print(f"[COMPLETION TRACKER] Found {total_assignments} assignments for period {period or 'all'}")

    except Exception as e:
        print(f"[COMPLETION TRACKER] Error loading assignments: {e}")
        import traceback
        traceback.print_exc()

    print(f"[COMPLETION TRACKER] Found {len(assigned_accounts)} unique property+account+vendor combinations with assignments")

    # 2a. Also check manual billback entries (jrk-manual-billback-entries DDB table)
    try:
        # Build property name -> ID lookup from ubi_accounts
        _prop_name_to_id = {}
        for acc in ubi_accounts:
            pname = str(acc.get("propertyName", "")).strip().lower()
            pid = str(acc.get("propertyId", "")).strip()
            if pname and pid:
                _prop_name_to_id[pname] = pid

        manual_paginator = ddb.get_paginator('scan')
        manual_added = 0
        for page in manual_paginator.paginate(TableName=MANUAL_BILLBACK_TABLE):
            for item in page.get('Items', []):
                entry_period = item.get('ubi_period', {}).get('S', '')
                if period and entry_period != period:
                    continue
                prop_name = item.get('property_name', {}).get('S', '').strip()
                acct_num = item.get('account_number', {}).get('S', '').strip()
                vendor = item.get('vendor_name', {}).get('S', '').strip()
                if not prop_name or not acct_num:
                    continue
                # Resolve property name to ID
                prop_id = _prop_name_to_id.get(prop_name.lower(), "")
                if not prop_id:
                    continue
                acct_key = (prop_id, acct_num, vendor)
                assigned_accounts.add(acct_key)
                # Also add to account_all_periods for prev/next period indicators
                if entry_period:
                    if acct_key not in account_all_periods:
                        account_all_periods[acct_key] = set()
                    account_all_periods[acct_key].add(entry_period)
                manual_added += 1
        print(f"[COMPLETION TRACKER] Added {manual_added} manual entries to assigned accounts")
    except Exception as e:
        print(f"[COMPLETION TRACKER] Error loading manual entries: {e}")

    # 2a-2. Also check accrual/manual entries (jrk-bill-manual-entries DDB table)
    accrual_entries_map = {}  # (property_id, account_number, vendor_name) -> entry_type
    if period:
        try:
            me_response = ddb.query(
                TableName=MANUAL_ENTRIES_TABLE,
                IndexName="period-index",
                KeyConditionExpression="period = :p",
                ExpressionAttributeValues={":p": {"S": period}}
            )
            me_items = me_response.get("Items", [])
            while me_response.get("LastEvaluatedKey"):
                me_response = ddb.query(
                    TableName=MANUAL_ENTRIES_TABLE,
                    IndexName="period-index",
                    KeyConditionExpression="period = :p",
                    ExpressionAttributeValues={":p": {"S": period}},
                    ExclusiveStartKey=me_response["LastEvaluatedKey"]
                )
                me_items.extend(me_response.get("Items", []))

            for me_item in me_items:
                me_key = (
                    me_item.get("property_id", {}).get("S", ""),
                    _normalize_account_number(me_item.get("account_number", {}).get("S", "")),
                    me_item.get("vendor_name", {}).get("S", "")
                )
                accrual_entries_map[me_key] = me_item.get("entry_type", {}).get("S", "MANUAL")
                # Also mark as assigned so it counts toward completion
                assigned_accounts.add(me_key)

            print(f"[COMPLETION TRACKER] Found {len(accrual_entries_map)} accrual/manual entries for period {period}")
        except Exception as e:
            print(f"[COMPLETION TRACKER] Error loading accrual entries: {e}")

    # 2b. Check ACTUAL Stage 7 files for bills waiting to be assigned (not stale DDB data)
    # This ensures "posted" indicator only shows when there's a real file in BILLBACK
    posted_accounts = {}  # (property_id, account_number, vendor_name) -> list of s3_keys

    # Parse selected period into prev/current/next for comparison
    _sel_prev_period = ""
    _sel_next_period = ""
    if period:
        try:
            pm, py = int(period.split("/")[0]), int(period.split("/")[1])
            prev_m, prev_y = (pm - 1, py) if pm > 1 else (12, py - 1)
            next_m, next_y = (pm + 1, py) if pm < 12 else (1, py + 1)
            _sel_prev_period = f"{prev_m:02d}/{prev_y}"
            _sel_next_period = f"{next_m:02d}/{next_y}"
        except (ValueError, IndexError):
            pass

    # Build set of UBI account keys for filtering Stage 7 scan
    # Normalize account numbers to strip punctuation for reliable matching
    _ubi_acct_lookup = set()
    for acc in ubi_accounts:
        pid = str(acc.get("propertyId", "")).strip()
        acct = str(acc.get("accountNumber", "")).strip()
        if pid and acct:
            _ubi_acct_lookup.add((pid, _normalize_account_number(acct)))

    try:
        # Scan Stage 7 files (last 90 days) to find actual bills waiting to be assigned
        from datetime import timedelta
        stage7_end = datetime.now()
        stage7_start = stage7_end - timedelta(days=90)

        stage7_prefixes = set()
        current = stage7_start
        while current <= stage7_end:
            stage7_prefixes.add(f"{POST_ENTRATA_PREFIX}yyyy={current.year}/mm={current.month:02d}/")
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1, day=1)
            else:
                current = current.replace(month=current.month + 1, day=1)

        # List Stage 7 files
        stage7_keys = []
        for prefix in stage7_prefixes:
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        if obj['Key'].endswith('.jsonl'):
                            stage7_keys.append(obj['Key'])
            except Exception:
                pass

        print(f"[COMPLETION TRACKER] Scanning {len(stage7_keys)} Stage 7 files for posted bills")

        # Read first line of each Stage 7 file to get account info
        def _read_stage7_account(s3_key):
            try:
                obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                body = obj["Body"].read()
                if s3_key.endswith('.gz'):
                    import gzip
                    body = gzip.decompress(body)
                first_line = body.decode('utf-8', errors='ignore').strip().split('\n')[0]
                rec = json.loads(first_line)
                pid = str(rec.get("EnrichedPropertyID") or rec.get("Property ID") or "").strip()
                acct = str(rec.get("Account Number") or rec.get("AccountNumber") or "").strip()
                vname = str(rec.get("EnrichedVendorName") or rec.get("Vendor Name") or "").strip()
                norm_acct = _normalize_account_number(acct)
                # Only include if it's a UBI account
                if pid and acct and (pid, norm_acct) in _ubi_acct_lookup:
                    return (pid, norm_acct, vname), s3_key
            except Exception:
                pass
            return None, None

        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = [executor.submit(_read_stage7_account, key) for key in stage7_keys]
            for future in as_completed(futures):
                try:
                    acct_key, s3_key = future.result()
                    if acct_key and s3_key:
                        if acct_key not in posted_accounts:
                            posted_accounts[acct_key] = []
                        posted_accounts[acct_key].append(s3_key)
                except Exception:
                    pass

        print(f"[COMPLETION TRACKER] Found {len(posted_accounts)} UBI accounts with bills in Stage 7 (BILLBACK)")
    except Exception as e:
        print(f"[COMPLETION TRACKER] Error scanning Stage 7: {e}")

    # 2c. For tracked accounts that have posted bills but NO Stage 8 data,
    #     read the actual JSONL files to get vacant stats
    ubi_account_keys = set()
    for acc in ubi_accounts:
        pid = str(acc.get("propertyId", "")).strip()
        acct = str(acc.get("accountNumber", "")).strip()
        vname = str(acc.get("vendorName", "")).strip()
        if pid and acct:
            ubi_account_keys.add((pid, acct, vname))

    # Find accounts that need vacant stats from posted files (have posted bill but no Stage 8 data)
    accounts_needing_vacant = {}
    for acct_key in ubi_account_keys:
        if acct_key not in vacant_stats and acct_key in posted_accounts:
            # Pick the most recent s3_key (last in list)
            s3_keys = posted_accounts[acct_key]
            if s3_keys:
                accounts_needing_vacant[acct_key] = s3_keys[-1]

    if accounts_needing_vacant:
        print(f"[COMPLETION TRACKER] Reading {len(accounts_needing_vacant)} posted JSONL files for vacant stats")

        def _read_vacant_from_posted(acct_key_and_s3key):
            acct_key, s3_key = acct_key_and_s3key
            vs = {"vacant": 0, "house": 0}
            try:
                obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                body = obj["Body"].read()
                if s3_key.endswith('.gz'):
                    import gzip
                    body = gzip.decompress(body)
                for raw_line in body.decode('utf-8', errors='ignore').strip().split('\n'):
                    try:
                        rec = json.loads(raw_line)
                        hov = str(rec.get("House Or Vacant", "")).lower().strip()
                        if hov == "vacant":
                            vs["vacant"] += 1
                        elif hov == "house":
                            vs["house"] += 1
                    except Exception:
                        continue
            except Exception:
                pass
            return acct_key, vs

        with ThreadPoolExecutor(max_workers=50) as executor:
            futures = [executor.submit(_read_vacant_from_posted, (k, v)) for k, v in accounts_needing_vacant.items()]
            for future in as_completed(futures):
                try:
                    acct_key, vs = future.result()
                    if vs["vacant"] > 0 or vs["house"] > 0:
                        vacant_stats[acct_key] = vs
                except Exception:
                    pass

        print(f"[COMPLETION TRACKER] Vacant stats now cover {len(vacant_stats)} accounts total")

    # 3. Build rollup structure
    # Group by property -> accounts (deduplicate by property+account+vendor)
    properties = {}  # property_id -> {name, accounts: [{account_number, vendor_name, has_bill}]}
    seen_accounts = set()  # Deduplicate entries

    # Build secondary lookup: (property_id, normalized_account) -> True for vendor-agnostic matching
    _assigned_by_prop_acct = set()
    for k in assigned_accounts:
        _assigned_by_prop_acct.add((k[0], k[1]))  # (property_id, normalized_account)
    _posted_by_prop_acct = set()
    for k in posted_accounts:
        _posted_by_prop_acct.add((k[0], k[1]))
    _accrual_by_prop_acct = {}
    for k, v in accrual_entries_map.items():
        _accrual_by_prop_acct[(k[0], k[1])] = v
    # Build period lookup by (property_id, normalized_account) ignoring vendor
    _periods_by_prop_acct = {}
    for k, periods_set in account_all_periods.items():
        pa_key = (k[0], k[1])
        if pa_key not in _periods_by_prop_acct:
            _periods_by_prop_acct[pa_key] = set()
        _periods_by_prop_acct[pa_key].update(periods_set)
    # Service dates by (property_id, normalized_account, period) ignoring vendor
    _svc_by_prop_acct_period = {}
    for k, v in service_dates.items():
        pa_key = (k[0], _normalize_account_number(k[1]), k[3])  # (pid, norm_acct, period)
        if pa_key not in _svc_by_prop_acct_period:
            _svc_by_prop_acct_period[pa_key] = v

    for acc in ubi_accounts:
        property_id = str(acc.get("propertyId", "")).strip()
        property_name = str(acc.get("propertyName", "")).strip()
        account_number = str(acc.get("accountNumber", "")).strip()
        vendor_name = str(acc.get("vendorName", "")).strip()
        norm_acct = _normalize_account_number(account_number)

        if not property_id or not account_number:
            continue

        # Skip duplicate entries for the same property+account (vendor-agnostic dedup)
        dedup_key = (property_id, norm_acct, vendor_name)
        if dedup_key in seen_accounts:
            continue
        seen_accounts.add(dedup_key)

        if property_id not in properties:
            properties[property_id] = {
                "property_id": property_id,
                "property_name": property_name,
                "ap_name": ap_by_property.get(property_id, ""),
                "accounts": [],
                "total": 0,
                "complete": 0,
                "posted_unassigned": 0
            }

        # Match by exact 3-tuple first, then fall back to (property_id, normalized_account) ignoring vendor
        prop_acct_key = (property_id, norm_acct)
        has_bill = dedup_key in assigned_accounts or prop_acct_key in _assigned_by_prop_acct

        # Check which periods this account actually has assignments for (from Stage 8)
        acct_assigned_periods = account_all_periods.get(dedup_key, set()) or _periods_by_prop_acct.get(prop_acct_key, set())

        # Show "posted, not assigned" if there's a posted invoice and THIS period
        # doesn't have an assignment yet (could be assigned to other periods)
        has_posted_bill = (dedup_key in posted_accounts or prop_acct_key in _posted_by_prop_acct) and not has_bill
        has_prev = _sel_prev_period in acct_assigned_periods if _sel_prev_period else False
        has_next = _sel_next_period in acct_assigned_periods if _sel_next_period else False

        # Look up vacant stats for this account (from Stage 8 OR posted files)
        vs = vacant_stats.get(dedup_key, {"vacant": 0, "house": 0})
        if vs["vacant"] == 0 and vs["house"] == 0:
            # Try vendor-agnostic lookup
            for vk, vcounts in vacant_stats.items():
                if vk[0] == property_id and vk[1] == norm_acct:
                    vs = vcounts
                    break
        total_lines = vs["vacant"] + vs["house"]
        vpct = round(vs["vacant"] / total_lines * 100, 1) if total_lines > 0 else 0

        # Check if this account exists in the scraper
        _clean_acct = _re.sub(r'[^A-Za-z0-9]', '', account_number).lower()
        in_scraper = _clean_acct in _scraper_acct_ids_clean if _clean_acct else False

        # Get service dates for this account+period (current period) - try normalized lookup
        svc_key = (property_id, account_number, vendor_name, period)
        svc_info = service_dates.get(svc_key, {})
        if not svc_info:
            svc_info = _svc_by_prop_acct_period.get((property_id, norm_acct, period), {})
        service_start = svc_info.get("start", "")
        service_end = svc_info.get("end", "")
        assignment_s3_key = svc_info.get("s3_key", "")

        # Get service dates for prev/next periods (for tooltip display)
        prev_svc_info = _svc_by_prop_acct_period.get((property_id, norm_acct, _sel_prev_period), {}) if _sel_prev_period else {}
        next_svc_info = _svc_by_prop_acct_period.get((property_id, norm_acct, _sel_next_period), {}) if _sel_next_period else {}

        # Check for accrual/manual entry - try exact then vendor-agnostic
        has_accrual_entry = dedup_key in accrual_entries_map or prop_acct_key in _accrual_by_prop_acct
        accrual_entry_type = accrual_entries_map.get(dedup_key, "") or _accrual_by_prop_acct.get(prop_acct_key, "")

        properties[property_id]["accounts"].append({
            "account_number": account_number,
            "vendor_name": vendor_name,
            "has_bill": has_bill,
            "has_posted_bill": has_posted_bill,
            "has_prev_period": has_prev,
            "has_next_period": has_next,
            "prev_service_start": _normalize_date_display(prev_svc_info.get("start", "")),
            "prev_service_end": _normalize_date_display(prev_svc_info.get("end", "")),
            "next_service_start": _normalize_date_display(next_svc_info.get("start", "")),
            "next_service_end": _normalize_date_display(next_svc_info.get("end", "")),
            "vacant_lines": vs["vacant"],
            "house_lines": vs["house"],
            "vacant_pct": vpct,
            "in_scraper": in_scraper,
            "service_start": _normalize_date_display(service_start),
            "service_end": _normalize_date_display(service_end),
            "s3_key": assignment_s3_key,
            "has_manual_entry": has_accrual_entry,
            "manual_entry_type": accrual_entry_type
        })
        properties[property_id]["total"] += 1
        if has_bill:
            properties[property_id]["complete"] += 1
        elif has_posted_bill:
            properties[property_id]["posted_unassigned"] += 1

    # Calculate percentages and convert to list
    properties_list = []
    for prop in properties.values():
        prop["percentage"] = round((prop["complete"] / prop["total"] * 100), 1) if prop["total"] > 0 else 0
        # Count accounts that are predominantly vacant (>50%)
        prop["vacant_accounts"] = sum(1 for a in prop["accounts"] if a.get("vacant_pct", 0) > 50)
        properties_list.append(prop)

    # Sort by property name
    properties_list.sort(key=lambda x: x.get("property_name", ""))

    # Calculate overall totals
    total_accounts = sum(p["total"] for p in properties_list)
    total_complete = sum(p["complete"] for p in properties_list)
    total_posted_unassigned = sum(p["posted_unassigned"] for p in properties_list)
    overall_percentage = round((total_complete / total_accounts * 100), 1) if total_accounts > 0 else 0

    result = {
        "period": period,
        "overall": {
            "total": total_accounts,
            "complete": total_complete,
            "posted_unassigned": total_posted_unassigned,
            "percentage": overall_percentage
        },
        "properties": properties_list
    }

    print(f"[COMPLETION TRACKER] Returning {len(properties_list)} properties, {total_complete}/{total_accounts} complete, {total_posted_unassigned} posted-unassigned ({overall_percentage}%)")

    # Cache the result for 5 minutes
    _CACHE[cache_key] = {"ts": time.time(), "data": result}

    return result


# -------- Accrual / Manual Entries API --------

@app.get("/api/accrual/calculate")
def api_accrual_calculate(property_id: str = "", account_number: str = "", vendor_name: str = "", period: str = "", user: str = Depends(require_user)):
    """Calculate a suggested accrual for a missing account based on historical data."""
    try:
        if not property_id or not account_number:
            return JSONResponse({"error": "property_id and account_number required"}, status_code=400)

        print(f"[ACCRUAL CALC] property_id={property_id}, account={account_number}, vendor={vendor_name}, period={period}")

        accounts_track = _ddb_get_config("accounts-to-track")
        if accounts_track is None:
            accounts_track = _s3_get_json(CONFIG_BUCKET, ACCOUNTS_TRACK_KEY)
        if not isinstance(accounts_track, list):
            accounts_track = []

        charge_code = ""
        utility_name = ""
        gl_account_number = ""
        gl_account_name = ""
        vendor_id = ""

        for acc in accounts_track:
            if (str(acc.get("propertyId", "")).strip() == property_id and
                str(acc.get("accountNumber", "")).strip() == account_number):
                charge_code = str(acc.get("chargeCode", "")).strip()
                utility_name = str(acc.get("utilityType", acc.get("utilityName", ""))).strip()
                vendor_id = str(acc.get("vendorId", "")).strip()
                gl_account_number = str(acc.get("glAccountNumber", "")).strip()
                gl_account_name = str(acc.get("glAccountName", "")).strip()
                if not vendor_name:
                    vendor_name = str(acc.get("vendorName", "")).strip()
                break

        if not charge_code:
            gl_mapping = _ddb_get_config("gl-charge-code-mapping")
            if isinstance(gl_mapping, list):
                for m in gl_mapping:
                    if (str(m.get("property_id", "")).strip() == property_id and
                        str(m.get("utility_name", "")).strip().lower() == utility_name.lower()):
                        charge_code = str(m.get("charge_code", "")).strip()
                        break

        historical = []
        source = "none"
        match_info = None

        # Use precomputed INVOICES_MAT cache (instant, regex matching)
        # This is the only source - no live Snowflake or DDB fallbacks that cause timeouts
        if vendor_name:
            historical, match_info = _read_historical_from_invoices_mat(property_id, account_number, vendor_name)
            if historical:
                source = "invoices_mat"
                # Use GL info from INVOICES_MAT if not already set
                if match_info:
                    if not gl_account_number and match_info.get("gl_account"):
                        gl_account_number = match_info["gl_account"]
                    if not gl_account_name and match_info.get("gl_name"):
                        gl_account_name = match_info["gl_name"]

        # T-12: Limit historical data to the 12 months immediately before the target period
        if historical and period:
            try:
                p_month, p_year = period.split("/")
                # Target period as YYYY-MM for comparison
                target_yyyymm = f"{p_year}-{p_month.zfill(2)}"
                # Calculate 12 months before the target period
                t_year, t_month = int(p_year), int(p_month)
                # Go back 12 months
                cutoff_month = t_month - 12
                cutoff_year = t_year
                while cutoff_month <= 0:
                    cutoff_month += 12
                    cutoff_year -= 1
                cutoff_yyyymm = f"{cutoff_year}-{cutoff_month:02d}"
                # Filter: keep only months where cutoff < period_month < target
                filtered = []
                for h in historical:
                    h_period = h.get("period", "")  # "MM/YYYY"
                    try:
                        hm, hy = h_period.split("/")
                        h_yyyymm = f"{hy}-{hm.zfill(2)}"
                    except:
                        continue
                    if cutoff_yyyymm < h_yyyymm < target_yyyymm:
                        filtered.append(h)
                historical = filtered
            except Exception:
                pass  # If period parsing fails, use all data

        accrual = _calculate_accrual(historical)

        result = {
            "property_id": property_id,
            "account_number": account_number,
            "vendor_name": vendor_name,
            "vendor_id": vendor_id,
            "charge_code": charge_code,
            "utility_name": utility_name,
            "gl_account_number": gl_account_number,
            "gl_account_name": gl_account_name,
            "period": period,
            "source": source,
            "calculated_amount": accrual["calculated_amount"],
            "historical_months": accrual["historical_months"],
            "avg_amount": accrual["avg_amount"],
            "inflation_amount": accrual["inflation_amount"],
            "monthly_amounts": accrual["monthly_amounts"],
        }

        # Add INVOICES_MAT match info if available
        if match_info:
            result["matched_vendor"] = match_info.get("matched_vendor", "")
            result["match_type"] = match_info.get("match_type", "")
            result["match_confidence"] = match_info.get("confidence", 0)

        return result

    except Exception as e:
        print(f"[ACCRUAL CALC] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/accrual/cache-stats")
def api_accrual_cache_stats(user: str = Depends(require_user)):
    """Return current INVOICES_MAT cache status (for debugging)."""
    cache = _INVOICE_HISTORY_CACHE
    prop_count = len(cache.get("data", {}))
    vendor_count = sum(len(v) for v in cache.get("vendor_index", {}).values())
    record_count = sum(
        len(records)
        for vendors in cache.get("data", {}).values()
        for records in vendors.values()
    )
    unique_accounts = sum(len(a) for a in cache.get("account_data", {}).values())
    last_refresh = cache.get("last_refresh")
    age_seconds = round(time.time() - last_refresh, 1) if last_refresh else None

    return {
        "loaded": last_refresh is not None,
        "loading": cache.get("loading", False),
        "last_refresh_utc": datetime.utcfromtimestamp(last_refresh).isoformat() if last_refresh else None,
        "age_seconds": age_seconds,
        "ttl_seconds": cache.get("ttl_seconds", 7200),
        "property_count": prop_count,
        "vendor_count": vendor_count,
        "unique_accounts": unique_accounts,
        "record_count": record_count,
        "prop_code_map_size": len(cache.get("prop_code_map", {})),
    }


@app.get("/api/accrual/refresh-cache")
def api_accrual_refresh_cache(user: str = Depends(require_user)):
    """Force-refresh the INVOICES_MAT cache (admin only)."""
    if user not in ADMIN_USERS:
        return JSONResponse({"error": "Admin access required"}, status_code=403)

    import threading
    def _do_refresh():
        _load_invoice_history_cache()
    threading.Thread(target=_do_refresh, daemon=True).start()

    return {"ok": True, "message": "Cache refresh started in background"}


@app.post("/api/accrual/create")
async def api_accrual_create(request: Request, user: str = Depends(require_user)):
    """Create a manual/accrual/true-up entry in DynamoDB."""
    try:
        payload = await request.json()

        entry_type = str(payload.get("entry_type", "")).strip().upper()
        if entry_type not in ("ACCRUAL", "MANUAL", "TRUE-UP"):
            return JSONResponse({"error": "entry_type must be ACCRUAL, MANUAL, or TRUE-UP"}, status_code=400)

        property_id = str(payload.get("property_id", "")).strip()
        account_number = str(payload.get("account_number", "")).strip()
        period = str(payload.get("period", "")).strip()
        amount = payload.get("amount")

        if not property_id or not account_number or not period:
            return JSONResponse({"error": "property_id, account_number, and period required"}, status_code=400)
        if amount is None:
            return JSONResponse({"error": "amount required"}, status_code=400)

        import uuid
        entry_id = str(uuid.uuid4())

        item = {
            "entry_id": {"S": entry_id},
            "property_id": {"S": property_id},
            "property_name": {"S": str(payload.get("property_name", "")).strip()},
            "account_number": {"S": account_number},
            "vendor_name": {"S": str(payload.get("vendor_name", "")).strip()},
            "vendor_id": {"S": str(payload.get("vendor_id", "")).strip()},
            "charge_code": {"S": str(payload.get("charge_code", "")).strip()},
            "utility_name": {"S": str(payload.get("utility_name", "")).strip()},
            "gl_account_number": {"S": str(payload.get("gl_account_number", "")).strip()},
            "gl_account_name": {"S": str(payload.get("gl_account_name", "")).strip()},
            "amount": {"N": str(float(amount))},
            "entry_type": {"S": entry_type},
            "reason_code": {"S": str(payload.get("reason_code", "")).strip()},
            "note": {"S": str(payload.get("note", "")).strip()},
            "period": {"S": period},
            "historical_months": {"N": str(int(payload.get("historical_months", 0)))},
            "historical_avg": {"N": str(float(payload.get("historical_avg", 0)))},
            "created_by": {"S": user},
            "created_utc": {"S": datetime.utcnow().isoformat()}
        }

        ddb.put_item(TableName=MANUAL_ENTRIES_TABLE, Item=item)

        print(f"[ACCRUAL CREATE] Created {entry_type} entry {entry_id} for {property_id}/{account_number} period {period} amount={amount}")

        return {"ok": True, "entry_id": entry_id}

    except Exception as e:
        print(f"[ACCRUAL CREATE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)


@app.delete("/api/accrual/entry/{entry_id}")
def api_delete_accrual_entry(entry_id: str, user: str = Depends(require_user)):
    """Delete an accrual/manual/true-up entry from MANUAL_ENTRIES_TABLE and bust caches."""
    try:
        # Delete from accrual entries table
        ddb.delete_item(
            TableName=MANUAL_ENTRIES_TABLE,
            Key={'entry_id': {'S': entry_id}}
        )
        # Also try the CSV-upload manual billback table (covers both entry sources)
        try:
            ddb.delete_item(
                TableName=MANUAL_BILLBACK_TABLE,
                Key={'entry_id': {'S': entry_id}}
            )
        except Exception:
            pass

        # Bust completion tracker cache so it picks up the deletion
        keys_to_bust = [k for k in list(_CACHE.keys()) if isinstance(k, tuple) and len(k) > 0 and k[0] == "completion_tracker"]
        for k in keys_to_bust:
            _CACHE.pop(k, None)

        print(f"[ACCRUAL DELETE] Deleted entry {entry_id} by {user}")
        return {"success": True, "deleted": entry_id}
    except Exception as e:
        print(f"[ACCRUAL DELETE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/accrual/entries")
def api_accrual_entries(period: str = "", user: str = Depends(require_user)):
    """List manual/accrual entries for a period."""
    try:
        items = []
        if period:
            response = ddb.query(
                TableName=MANUAL_ENTRIES_TABLE,
                IndexName="period-index",
                KeyConditionExpression="period = :p",
                ExpressionAttributeValues={":p": {"S": period}}
            )
            items = response.get("Items", [])
            while response.get("LastEvaluatedKey"):
                response = ddb.query(
                    TableName=MANUAL_ENTRIES_TABLE,
                    IndexName="period-index",
                    KeyConditionExpression="period = :p",
                    ExpressionAttributeValues={":p": {"S": period}},
                    ExclusiveStartKey=response["LastEvaluatedKey"]
                )
                items.extend(response.get("Items", []))
        else:
            response = ddb.scan(TableName=MANUAL_ENTRIES_TABLE)
            items = response.get("Items", [])
            while response.get("LastEvaluatedKey"):
                response = ddb.scan(
                    TableName=MANUAL_ENTRIES_TABLE,
                    ExclusiveStartKey=response["LastEvaluatedKey"]
                )
                items.extend(response.get("Items", []))

        results = []
        for item in items:
            results.append({
                "entry_id": item.get("entry_id", {}).get("S", ""),
                "property_id": item.get("property_id", {}).get("S", ""),
                "property_name": item.get("property_name", {}).get("S", ""),
                "account_number": item.get("account_number", {}).get("S", ""),
                "vendor_name": item.get("vendor_name", {}).get("S", ""),
                "vendor_id": item.get("vendor_id", {}).get("S", ""),
                "charge_code": item.get("charge_code", {}).get("S", ""),
                "utility_name": item.get("utility_name", {}).get("S", ""),
                "gl_account_number": item.get("gl_account_number", {}).get("S", ""),
                "gl_account_name": item.get("gl_account_name", {}).get("S", ""),
                "amount": float(item.get("amount", {}).get("N", "0")),
                "entry_type": item.get("entry_type", {}).get("S", ""),
                "reason_code": item.get("reason_code", {}).get("S", ""),
                "note": item.get("note", {}).get("S", ""),
                "period": item.get("period", {}).get("S", ""),
                "historical_months": int(item.get("historical_months", {}).get("N", "0")),
                "historical_avg": float(item.get("historical_avg", {}).get("N", "0")),
                "created_by": item.get("created_by", {}).get("S", ""),
                "created_utc": item.get("created_utc", {}).get("S", "")
            })

        return {"items": results, "count": len(results)}

    except Exception as e:
        print(f"[ACCRUAL ENTRIES] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)


@app.delete("/api/accrual/entry")
def api_accrual_delete(entry_id: str = "", user: str = Depends(require_user)):
    """Delete a manual/accrual entry by entry_id."""
    try:
        if not entry_id:
            return JSONResponse({"error": "entry_id required"}, status_code=400)

        ddb.delete_item(
            TableName=MANUAL_ENTRIES_TABLE,
            Key={"entry_id": {"S": entry_id}}
        )

        print(f"[ACCRUAL DELETE] Deleted entry {entry_id} by {user}")
        return {"ok": True}

    except Exception as e:
        print(f"[ACCRUAL DELETE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)


# -------- UBI Batches --------
@app.post("/api/ubi-batch/create")
async def api_create_ubi_batch(request: Request, user: str = Depends(require_user)):
    """Create a new UBI billback batch"""
    try:
        form = await request.form()
        batch_name = form.get("batch_name", "").strip()
        period_start = form.get("period_start", "").strip()
        period_end = form.get("period_end", "").strip()
        memo = form.get("memo", "").strip()

        if not batch_name or not period_start or not period_end:
            return JSONResponse({"error": "batch_name, period_start, and period_end required"}, status_code=400)

        # Load master bills
        master_bills = _s3_get_master_bills()
        if not isinstance(master_bills, list):
            master_bills = []

        # Filter master bills by date range
        # Convert period dates from YYYY-MM-DD to datetime for comparison
        from datetime import datetime as dt
        try:
            period_start_dt = dt.strptime(period_start, "%Y-%m-%d")
            period_end_dt = dt.strptime(period_end, "%Y-%m-%d")
        except Exception:
            return JSONResponse({"error": "Invalid date format"}, status_code=400)

        selected_master_bills = []
        total_amount = 0
        properties = set()

        for mb in master_bills:
            mb_start = mb.get("billback_month_start", "")  # Format: MM/DD/YYYY
            try:
                mb_start_dt = dt.strptime(mb_start, "%m/%d/%Y")
                if period_start_dt <= mb_start_dt <= period_end_dt:
                    selected_master_bills.append(mb.get("master_bill_id"))
                    total_amount += mb.get("utility_amount", 0)
                    properties.add(mb.get("property_id"))
            except Exception:
                continue

        # Create batch ID (include batch_name to make it unique)
        import uuid
        batch_id = f"{batch_name}-{period_start}-{period_end}-{str(uuid.uuid4())[:8]}"

        # Create batch object
        batch = {
            "batch_id": batch_id,
            "batch_name": batch_name,
            "period_start": period_start,
            "period_end": period_end,
            "memo": memo,
            "master_bill_ids": selected_master_bills,
            "status": "draft",
            "created_utc": datetime.utcnow().isoformat(),
            "created_by": user,
            "reviewed_utc": None,
            "reviewed_by": None,
            "exported_utc": None,
            "exported_by": None,
            "run_date": None,
            "total_master_bills": len(selected_master_bills),
            "total_amount": total_amount,
            "properties_count": len(properties)
        }

        # Save batch
        batches = _ddb_get_config("ubi-batches")
        if not isinstance(batches, list):
            batches = []

        # Check if batch already exists
        exists = any(b.get("batch_id") == batch_id for b in batches)
        if exists:
            return JSONResponse({"error": "batch already exists"}, status_code=400)

        batches.append(batch)
        _ddb_put_config("ubi-batches", batches)

        return {"ok": True, "batch": batch}

    except Exception as e:
        print(f"[CREATE BATCH] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/ubi-batch/finalize")
async def api_finalize_ubi_batch(request: Request, user: str = Depends(require_user)):
    """Finalize a batch (mark as reviewed and set run_date)"""
    try:
        form = await request.form()
        batch_id = form.get("batch_id", "").strip()

        if not batch_id:
            return JSONResponse({"error": "batch_id required"}, status_code=400)

        # Load batches
        batches = _ddb_get_config("ubi-batches")
        if not isinstance(batches, list):
            return JSONResponse({"error": "batch not found"}, status_code=404)

        # Find and update batch
        found = False
        for batch in batches:
            if batch.get("batch_id") == batch_id:
                batch["status"] = "finalized"
                batch["reviewed_utc"] = datetime.utcnow().isoformat()
                batch["reviewed_by"] = user
                batch["run_date"] = datetime.utcnow().isoformat()
                found = True
                break

        if not found:
            return JSONResponse({"error": "batch not found"}, status_code=404)

        # Save
        _ddb_put_config("ubi-batches", batches)

        return {"ok": True}

    except Exception as e:
        print(f"[FINALIZE BATCH] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/ubi-batch/delete")
async def api_delete_ubi_batch(request: Request, user: str = Depends(require_user)):
    """Delete a batch (only if status is draft)"""
    try:
        form = await request.form()
        batch_id = form.get("batch_id", "").strip()

        if not batch_id:
            return JSONResponse({"error": "batch_id required"}, status_code=400)

        # Load batches
        batches = _ddb_get_config("ubi-batches")
        if not isinstance(batches, list):
            return JSONResponse({"error": "batch not found"}, status_code=404)

        # Find batch and check if it's deletable
        batch_to_delete = None
        for batch in batches:
            if batch.get("batch_id") == batch_id:
                batch_to_delete = batch
                break

        if not batch_to_delete:
            return JSONResponse({"error": "batch not found"}, status_code=404)

        # Only allow deletion of draft batches
        if batch_to_delete.get("status") != "draft":
            return JSONResponse({"error": "can only delete draft batches"}, status_code=400)

        # Remove batch from list
        batches = [b for b in batches if b.get("batch_id") != batch_id]

        # Save
        _ddb_put_config("ubi-batches", batches)

        return {"ok": True}

    except Exception as e:
        print(f"[DELETE BATCH] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/ubi-batch/list")
def api_list_ubi_batches(user: str = Depends(require_user)):
    """List all UBI batches"""
    batches = _ddb_get_config("ubi-batches")
    if not isinstance(batches, list):
        batches = []

    return {"items": batches, "count": len(batches)}


@app.get("/api/ubi-batch/detail/{batch_id}")
def api_ubi_batch_detail(batch_id: str, user: str = Depends(require_user)):
    """Get detail of a specific batch"""
    batches = _ddb_get_config("ubi-batches")
    if not isinstance(batches, list):
        return JSONResponse({"error": "not found"}, status_code=404)

    # URL decode
    import urllib.parse
    decoded_id = urllib.parse.unquote(batch_id)

    for batch in batches:
        if batch.get("batch_id") == decoded_id:
            # Load master bills for this batch
            master_bills = _s3_get_master_bills()
            if isinstance(master_bills, list):
                batch_mb_ids = set(batch.get("master_bill_ids", []))
                batch["master_bills"] = [mb for mb in master_bills if mb.get("master_bill_id") in batch_mb_ids]

            return batch

    return JSONResponse({"error": "not found"}, status_code=404)


@app.post("/api/ubi-batch/export-snowflake")
async def api_export_batch_to_snowflake(request: Request, user: str = Depends(require_user)):
    """Export a batch to Snowflake _Master_Bills_Prod table"""
    try:
        form = await request.form()
        batch_id = form.get("batch_id", "").strip()

        if not batch_id:
            return JSONResponse({"error": "batch_id required"}, status_code=400)

        # Load batch
        batches = _ddb_get_config("ubi-batches")
        if not isinstance(batches, list):
            return JSONResponse({"error": "batch not found"}, status_code=404)

        batch = None
        for b in batches:
            if b.get("batch_id") == batch_id:
                batch = b
                break

        if not batch:
            return JSONResponse({"error": "batch not found"}, status_code=404)

        # Check if batch is finalized
        if batch.get("status") != "finalized":
            return JSONResponse({"error": "batch must be finalized before export"}, status_code=400)

        # Load master bills
        master_bills = _s3_get_master_bills()
        if not isinstance(master_bills, list):
            master_bills = []

        # Filter to batch master bills
        batch_mb_ids = set(batch.get("master_bill_ids", []))
        batch_master_bills = [mb for mb in master_bills if mb.get("master_bill_id") in batch_mb_ids]

        if not batch_master_bills:
            return JSONResponse({"error": "no master bills to export"}, status_code=400)

        memo = batch.get("memo", "")
        run_date = batch.get("run_date", datetime.utcnow().isoformat())

        # Write to Snowflake
        print(f"[EXPORT SNOWFLAKE] Writing {len(batch_master_bills)} rows to Snowflake for batch {batch_id}")
        success, message, rows_inserted = _write_to_snowflake(
            batch_id=batch_id,
            master_bills=batch_master_bills,
            memo=memo,
            run_date=run_date
        )

        if not success:
            return JSONResponse({"error": f"Snowflake export failed: {message}"}, status_code=500)

        # Generate SQL for preview/audit purposes (matching NEW schema with Batch_ID)
        sql_rows = []
        for mb in batch_master_bills:
            escaped_memo = memo.replace("'", "''")
            row = (
                f"('{mb.get('property_id')}', "
                f"'{mb.get('ar_code_mapping')}', "
                f"'{mb.get('utility_name')}', "
                f"'{mb.get('utility_amount')}', "
                f"'{mb.get('billback_month_start')}', "
                f"'{mb.get('billback_month_end')}', "
                f"'{run_date}', "
                f"'{escaped_memo}', "
                f"'{batch_id}'"
                f")"
            )
            sql_rows.append(row)

        values_str = ',\n'.join(sql_rows)
        sql = f"""-- UBI Billback Export (EXECUTED)
-- Batch: {batch.get('batch_name')} (ID: {batch_id})
-- Period: {batch.get('period_start')} to {batch.get('period_end')}
-- Run Date: {run_date}
-- Total Rows: {rows_inserted}
-- Status: Successfully exported to Snowflake table "_Master_Bills_Prod"

INSERT INTO LAITMAN.UBI."_Master_Bills_Prod"
("Property_ID", "AR_Code_Mapping", "Utility_Name", "Utility_Amount",
 "Billback_Month_Start", "Billback_Month_End", "RunDate", "Memo", "Batch_ID")
VALUES
{values_str};
"""

        # Mark batch as exported
        batch["status"] = "exported"
        batch["exported_utc"] = datetime.utcnow().isoformat()
        batch["exported_by"] = user

        # Update batch in config
        for i, b in enumerate(batches):
            if b.get("batch_id") == batch_id:
                batches[i] = batch
                break

        _ddb_put_config("ubi-batches", batches)

        return {
            "ok": True,
            "sql": sql,
            "rows_exported": rows_inserted,
            "batch": batch,
            "snowflake_message": message
        }

    except Exception as e:
        print(f"[EXPORT SNOWFLAKE] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# -------- AP Team Members (single column: name) --------
@app.get("/api/config/ap-team")
def api_get_ap_team(user: str = Depends(require_user)):
    arr = _ddb_get_config("ap-team")
    if not isinstance(arr, list):
        arr = []
    # normalize to {name}
    out = []
    for r in arr:
        if isinstance(r, dict):
            out.append({"name": str(r.get("name") or r.get("Name") or "").strip()})
        elif isinstance(r, str):
            out.append({"name": r})
    return {"items": out}


@app.post("/api/config/ap-team")
async def api_save_ap_team(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    norm = []
    for r in items:
        if not isinstance(r, dict):
            continue
        name = str(r.get("name") or "").strip()
        if name:
            norm.append({"name": name})
    ok = _ddb_put_config("ap-team", norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True, "saved": len(norm)}


@app.get("/api/config/ubi-mapping")
def api_get_ubi_mapping(user: str = Depends(require_user)):
    base = _get_accounts_to_track()
    overlay = _ddb_get_config("ubi-mapping") or []
    by_key: dict[str, dict] = {}
    for r in overlay:
        if not isinstance(r, dict):
            continue
        k = "|".join([
            str(r.get("vendorId") or "").strip(),
            str(r.get("accountNumber") or "").strip(),
            str(r.get("propertyId") or "").strip(),
            str(r.get("glAccountNumber") or "").strip(),
        ])
        by_key[k] = {
            "isUbi": bool(r.get("isUbi") is True or str(r.get("isUbi")).lower() == "true"),
            "chargeCode": str(r.get("chargeCode") or "").strip(),
            "notes": str(r.get("notes") or "").strip(),
        }
    out = []
    for r in base:
        if not isinstance(r, dict):
            continue
        vendorId = str(r.get("vendorId") or "").strip()
        accountNumber = str(r.get("accountNumber") or "").strip()
        propertyId = str(r.get("propertyId") or "").strip()
        glAccountNumber = str(r.get("glAccountNumber") or "").strip()
        k = "|".join([vendorId, accountNumber, propertyId, glAccountNumber])
        ov = by_key.get(k) or {"isUbi": False, "chargeCode": "", "notes": ""}
        out.append({
            "vendorId": vendorId,
            "vendorName": str(r.get("vendorName") or "").strip(),
            "accountNumber": accountNumber,
            "propertyId": propertyId,
            "propertyName": str(r.get("propertyName") or "").strip(),
            "glAccountNumber": glAccountNumber,
            "glAccountName": str(r.get("glAccountName") or "").strip(),
            "daysBetweenBills": int(str(r.get("daysBetweenBills") or "0").strip() or 0),
            "isUbi": bool(ov.get("isUbi")),
            "chargeCode": str(ov.get("chargeCode") or "").strip(),
            "notes": str(ov.get("notes") or "").strip(),
        })
    return {"items": out}


@app.post("/api/config/ubi-mapping")
async def api_save_ubi_mapping(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    to_save = []
    for r in items:
        if not isinstance(r, dict):
            continue
        isUbi = (r.get("isUbi") is True) or (str(r.get("isUbi")).lower() == "true")
        chargeCode = str(r.get("chargeCode") or "").strip()
        notes = str(r.get("notes") or "").strip()
        if not isUbi and not chargeCode and not notes:
            continue
        to_save.append({
            "vendorId": str(r.get("vendorId") or "").strip(),
            "accountNumber": str(r.get("accountNumber") or "").strip(),
            "propertyId": str(r.get("propertyId") or "").strip(),
            "glAccountNumber": str(r.get("glAccountNumber") or "").strip(),
            "isUbi": bool(isUbi),
            "chargeCode": chargeCode,
            "notes": notes,
        })
    ok = _ddb_put_config("ubi-mapping", to_save)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True, "saved": len(to_save)}


# -------- UOM (Unit of Measure) Mapping --------
@app.get("/api/config/uom-mapping")
def api_get_uom_mapping(user: str = Depends(require_user)):
    """Get UOM conversion mappings. Returns list of mappings with structure:
    [{"original_uom": "CCF", "utility_type": "water", "conversion_factor": 748, "target_uom": "Gallons"}, ...]
    """
    try:
        # Try to load from S3
        standard_key = f"{DIM_UOM_PREFIX}latest.json.gz"
        try:
            txt = _read_s3_text(BUCKET, standard_key)
            data = json.loads(txt)
            if isinstance(data, list):
                return {"items": data}
        except Exception:
            pass
        # Try to find any file in the prefix
        key = _find_latest_data(DIM_UOM_PREFIX)
        if key:
            txt = _read_s3_text(BUCKET, key)
            data = json.loads(txt)
            if isinstance(data, list):
                return {"items": data}
    except Exception as e:
        print(f"[UOM MAPPING GET] Error: {e}")
    # Return empty list as default
    return {"items": []}


@app.post("/api/config/uom-mapping")
async def api_save_uom_mapping(request: Request, user: str = Depends(require_user)):
    """Save UOM conversion mappings to S3 for use by enricher Lambda."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)

    # Validate and normalize items
    to_save = []
    for r in items:
        if not isinstance(r, dict):
            continue
        original_uom = str(r.get("original_uom") or "").strip()
        utility_type = str(r.get("utility_type") or "").strip()
        target_uom = str(r.get("target_uom") or "").strip()
        try:
            conversion_factor = float(r.get("conversion_factor", 1.0))
        except Exception:
            conversion_factor = 1.0

        if not original_uom or not target_uom:
            continue  # Skip invalid entries

        to_save.append({
            "original_uom": original_uom,
            "utility_type": utility_type,  # Can be empty for universal conversions
            "conversion_factor": conversion_factor,
            "target_uom": target_uom,
        })

    # Save to S3 as gzipped JSON
    try:
        dt_str = dt.datetime.utcnow().strftime("%Y%m%d")
        key = f"{DIM_UOM_PREFIX}dt={dt_str}/data.json.gz"
        json_str = json.dumps(to_save, ensure_ascii=False, indent=2)
        compressed = gzip.compress(json_str.encode("utf-8"))
        s3.put_object(Bucket=BUCKET, Key=key, Body=compressed, ContentType="application/json", ContentEncoding="gzip")

        # Also save to standardized filename for fast loading
        standard_key = f"{DIM_UOM_PREFIX}latest.json.gz"
        s3.put_object(Bucket=BUCKET, Key=standard_key, Body=compressed, ContentType="application/json", ContentEncoding="gzip")

        print(f"[UOM MAPPING SAVE] Saved {len(to_save)} mappings to {key} and {standard_key}")
        return {"ok": True, "saved": len(to_save)}
    except Exception as e:
        print(f"[UOM MAPPING SAVE] Error: {e}")
        return JSONResponse({"error": f"save_failed: {str(e)}"}, status_code=500)


# -------- AP to Property Mapping --------
@app.get("/api/config/ap-mapping")
def api_get_ap_mapping(user: str = Depends(require_user)):
    arr = _ddb_get_config("ap-mapping")
    if not isinstance(arr, list):
        arr = []
    # normalize fields
    out = []
    for r in arr:
        if not isinstance(r, dict):
            continue
        out.append({
            "name": str(r.get("name") or "").strip(),
            "propertyId": str(r.get("propertyId") or "").strip(),
            "propertyName": str(r.get("propertyName") or "").strip(),
        })
    return {"items": out}


@app.post("/api/config/ap-mapping")
async def api_save_ap_mapping(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    norm = []
    for r in items:
        if not isinstance(r, dict):
            continue
        norm.append({
            "name": str(r.get("name") or "").strip(),
            "propertyId": str(r.get("propertyId") or "").strip(),
            "propertyName": str(r.get("propertyName") or "").strip(),
        })
    ok = _ddb_put_config("ap-mapping", norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    return {"ok": True, "saved": len(norm)}


# -------- Vendor-Property / Vendor-GL Override Config --------
@app.get("/api/config/vendor-property-overrides")
def api_get_vendor_property_overrides(user: str = Depends(require_user)):
    data = _ddb_get_config("vendor-property-overrides") or []
    return {"items": data}

@app.post("/api/config/vendor-property-overrides")
async def api_save_vendor_property_overrides(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    norm = []
    for r in items:
        if not isinstance(r, dict):
            continue
        vid = str(r.get("vendor_id") or "").strip()
        pid = str(r.get("property_id") or "").strip()
        if not vid or not pid:
            continue  # Skip entries with empty vendor/property IDs
        action = str(r.get("action") or "allow").strip().lower()
        if action not in ("allow", "block"):
            action = "allow"
        norm.append({
            "vendor_id": vid,
            "vendor_name": str(r.get("vendor_name") or "").strip(),
            "property_id": pid,
            "property_name": str(r.get("property_name") or "").strip(),
            "action": action,
            "added_by": user,  # Always use authenticated user, not client-supplied value
            "added_at": dt.datetime.utcnow().isoformat() + "Z",  # Always server-generated
        })
    # Deduplicate: keep last occurrence of each (vendor_id, property_id) pair
    seen_keys = {}
    for i, item in enumerate(norm):
        seen_keys[(item["vendor_id"], item["property_id"])] = i
    norm = [norm[i] for i in sorted(seen_keys.values())]
    ok = _ddb_put_config("vendor-property-overrides", norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    # Invalidate cached pairs so next validation uses updated overrides
    _VENDOR_PAIR_CACHE["last_refresh"] = None
    return {"ok": True, "saved": len(norm)}

@app.get("/api/config/vendor-gl-overrides")
def api_get_vendor_gl_overrides(user: str = Depends(require_user)):
    data = _ddb_get_config("vendor-gl-overrides") or []
    return {"items": data}

@app.post("/api/config/vendor-gl-overrides")
async def api_save_vendor_gl_overrides(request: Request, user: str = Depends(require_user)):
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)
    items = payload.get("items") if isinstance(payload, dict) else None
    if not isinstance(items, list):
        return JSONResponse({"error": "items must be a list"}, status_code=400)
    norm = []
    for r in items:
        if not isinstance(r, dict):
            continue
        vid = str(r.get("vendor_id") or "").strip()
        gl = str(r.get("gl_code") or "").strip()
        if not vid or not gl:
            continue  # Skip entries with empty vendor/GL code
        action = str(r.get("action") or "allow").strip().lower()
        if action not in ("allow", "block"):
            action = "allow"
        norm.append({
            "vendor_id": vid,
            "vendor_name": str(r.get("vendor_name") or "").strip(),
            "gl_code": gl,
            "gl_code_name": str(r.get("gl_code_name") or "").strip(),
            "action": action,
            "added_by": user,  # Always use authenticated user, not client-supplied value
            "added_at": dt.datetime.utcnow().isoformat() + "Z",  # Always server-generated
        })
    # Deduplicate: keep last occurrence of each (vendor_id, gl_code) pair
    seen_keys = {}
    for i, item in enumerate(norm):
        seen_keys[(item["vendor_id"], item["gl_code"])] = i
    norm = [norm[i] for i in sorted(seen_keys.values())]
    ok = _ddb_put_config("vendor-gl-overrides", norm)
    if not ok:
        return JSONResponse({"error": "save_failed"}, status_code=500)
    # Invalidate cached pairs so next validation uses updated overrides
    _VENDOR_PAIR_CACHE["last_refresh"] = None
    return {"ok": True, "saved": len(norm)}


# -------- DEBUG / IMPROVE Reporting APIs --------
@app.get("/api/debug/exclusion-hashes")
def api_debug_exclusion_hashes(user: str = Depends(require_user)):
    """Debug endpoint: show exclusion hash cache stats."""
    from datetime import datetime
    try:
        cache = _EXCLUSION_HASH_CACHE
        cache_age = None
        if cache["last_refresh"]:
            cache_age = (datetime.now() - cache["last_refresh"]).total_seconds()

        # Force a fresh build and capture details
        old_hashes = len(cache.get("hashes", set()))

        # Temporarily invalidate to force rebuild
        invalidate_exclusion_cache()
        fresh_hashes = _get_cached_exclusion_hashes(90)

        # Also count Stage 8 files
        paginator = s3.get_paginator('list_objects_v2')
        s8_file_count = 0
        months_scanned = []
        from datetime import timedelta
        cutoff = datetime.now() - timedelta(days=90)
        current = cutoff
        while current <= datetime.now():
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={current.year}/mm={current.month:02d}/"
            months_scanned.append(prefix)
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    if obj['Key'].endswith('.jsonl'):
                        s8_file_count += 1
            if current.month == 12:
                current = current.replace(year=current.year + 1, month=1, day=1)
            else:
                current = current.replace(month=current.month + 1, day=1)

        # Sample a few hashes
        sample = list(fresh_hashes)[:5]

        return {
            "cache_age_seconds": cache_age,
            "old_hash_count": old_hashes,
            "fresh_hash_count": len(fresh_hashes),
            "s8_file_count": s8_file_count,
            "ubi_assigned_prefix": UBI_ASSIGNED_PREFIX,
            "months_scanned": months_scanned,
            "sample_hashes": sample,
        }
    except Exception as e:
        import traceback
        return {"error": str(e), "trace": traceback.format_exc()}


@app.get("/api/debug/orphaned-stage7")
def api_debug_orphaned_stage7(user: str = Depends(require_user), days_back: int = 60):
    """Find Stage 7 files that contain lines already assigned in Stage 8 (should have been deleted).

    This diagnostic scans Stage 7, computes line hashes, and checks against
    the DDB exclusion table. Any matches are 'orphaned' - they should have been
    removed from Stage 7 during assignment.
    """
    from datetime import datetime, timedelta
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time as _time

    t0 = _time.time()
    excluded_hashes = _get_cached_exclusion_hashes(days_back)
    print(f"[ORPHAN CHECK] Using {len(excluded_hashes)} exclusion hashes")

    # Scan Stage 7 files
    today = datetime.now()
    all_keys = []
    for i in range(days_back):
        d = today - timedelta(days=i)
        prefix = f"{POST_ENTRATA_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
        try:
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    if obj['Key'].endswith('.jsonl'):
                        all_keys.append(obj['Key'])
        except Exception:
            continue

    print(f"[ORPHAN CHECK] Scanning {len(all_keys)} Stage 7 files")

    orphaned_files = []
    clean_files = 0

    def check_file(key):
        try:
            body = _read_s3_text(BUCKET, key)
            lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
            total_lines = 0
            excluded_lines = 0
            for line in lines:
                try:
                    rec = json.loads(line)
                    lh = _compute_stable_line_hash(rec)
                    total_lines += 1
                    if lh in excluded_hashes:
                        excluded_lines += 1
                except Exception:
                    continue
            if excluded_lines > 0:
                first = json.loads(lines[0])
                return {
                    "s3_key": key,
                    "total_lines": total_lines,
                    "excluded_lines": excluded_lines,
                    "all_excluded": excluded_lines == total_lines,
                    "account": first.get("Account Number", ""),
                    "vendor": first.get("EnrichedVendorName", ""),
                    "property": first.get("EnrichedPropertyName", ""),
                }
            return None
        except Exception:
            return None

    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = {executor.submit(check_file, k): k for k in all_keys}
        for future in as_completed(futures):
            result = future.result()
            if result:
                orphaned_files.append(result)
            else:
                clean_files += 1

    elapsed = _time.time() - t0
    fully_orphaned = [f for f in orphaned_files if f["all_excluded"]]
    partially_orphaned = [f for f in orphaned_files if not f["all_excluded"]]

    return {
        "total_stage7_files": len(all_keys),
        "clean_files": clean_files,
        "fully_orphaned": len(fully_orphaned),
        "partially_orphaned": len(partially_orphaned),
        "orphaned_details": orphaned_files[:50],
        "exclusion_hash_count": len(excluded_hashes),
        "scan_seconds": round(elapsed, 1),
    }


@app.post("/api/debug/cleanup-orphaned-stage7")
async def api_debug_cleanup_orphaned_stage7(request: Request, user: str = Depends(require_user)):
    """Delete Stage 7 files where ALL lines are already assigned (fully orphaned).

    Only deletes files where every single line hash exists in the exclusion table.
    Files with a mix of assigned/unassigned lines are rewritten to keep only unassigned lines.
    """
    from datetime import datetime
    import time as _time

    t0 = _time.time()
    payload = await request.json()
    s3_keys = payload.get("keys", [])
    if not s3_keys:
        return JSONResponse({"error": "No keys provided"}, status_code=400)

    excluded_hashes = _get_cached_exclusion_hashes(90)
    deleted = 0
    rewritten = 0
    errors = []

    for key in s3_keys:
        try:
            body = _read_s3_text(BUCKET, key)
            lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
            remaining = []
            for line in lines:
                try:
                    rec = json.loads(line)
                    lh = _compute_stable_line_hash(rec)
                    if lh not in excluded_hashes:
                        remaining.append(rec)
                except Exception:
                    remaining.append(json.loads(line))

            if not remaining:
                # All lines assigned - safe to delete
                s3.delete_object(Bucket=BUCKET, Key=key)
                deleted += 1
                print(f"[ORPHAN CLEANUP] Deleted fully orphaned: {key}")
            elif len(remaining) < len(lines):
                # Some lines assigned - rewrite with only unassigned lines
                y, m, d = _extract_ymd_from_key(key)
                base = _basename_from_key(key)
                new_key = _write_jsonl(POST_ENTRATA_PREFIX, y, m, d, base.replace('.jsonl', ''), remaining)
                if new_key != key:
                    s3.delete_object(Bucket=BUCKET, Key=key)
                rewritten += 1
                print(f"[ORPHAN CLEANUP] Rewritten (removed {len(lines) - len(remaining)} assigned lines): {key}")
        except Exception as e:
            errors.append({"key": key, "error": str(e)})

    elapsed = _time.time() - t0
    return {
        "deleted": deleted,
        "rewritten": rewritten,
        "errors": errors,
        "seconds": round(elapsed, 1),
    }


@app.get("/api/debug/reports")
def api_get_debug_reports(user: str = Depends(require_user)):
    """Get all debug reports for triage page."""
    try:
        response = ddb.scan(TableName=DEBUG_TABLE)
        items = response.get("Items", [])

        reports = []
        for item in items:
            reports.append({
                "report_id": item.get("report_id", {}).get("S", ""),
                "title": item.get("title", {}).get("S", ""),
                "description": item.get("description", {}).get("S", ""),
                "page_url": item.get("page_url", {}).get("S", ""),
                "requestor": item.get("requestor", {}).get("S", ""),
                "status": item.get("status", {}).get("S", "Open"),
                "priority": item.get("priority", {}).get("S", "Medium"),
                "type": item.get("type", {}).get("S", "bug"),
                "created_utc": item.get("created_utc", {}).get("S", ""),
                "updated_utc": item.get("updated_utc", {}).get("S", ""),
                "completed_utc": item.get("completed_utc", {}).get("S", ""),
                "completed_by": item.get("completed_by", {}).get("S", ""),
                "resolution_notes": item.get("resolution_notes", {}).get("S", ""),
                "release_tag": item.get("release_tag", {}).get("S", ""),
            })

        # Sort by created_utc descending
        reports.sort(key=lambda x: x.get("created_utc", ""), reverse=True)
        return {"reports": reports}
    except Exception as e:
        print(f"[DEBUG API] Error loading reports: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/debug/stats")
def api_debug_stats(user: str = Depends(require_user)):
    """Get dashboard stats for debug reports."""
    try:
        response = ddb.scan(TableName=DEBUG_TABLE)
        items = response.get("Items", [])

        stats = {"open": 0, "in_progress": 0, "completed": 0, "rejected": 0, "deferred": 0,
                 "bugs": 0, "features": 0, "enhancements": 0,
                 "completed_this_week": 0, "completed_this_month": 0,
                 "by_requestor": {}, "by_priority": {"Critical": 0, "High": 0, "Medium": 0, "Low": 0}}

        today = dt.date.today()
        week_start = today - dt.timedelta(days=today.weekday())
        month_start = today.replace(day=1)

        for item in items:
            status = item.get("status", {}).get("S", "Open")
            priority = item.get("priority", {}).get("S", "Medium")
            item_type = item.get("type", {}).get("S", "bug")
            requestor = item.get("requestor", {}).get("S", "unknown")
            completed_utc = item.get("completed_utc", {}).get("S", "")

            # Count by status
            if status == "Open":
                stats["open"] += 1
            elif status == "In Progress":
                stats["in_progress"] += 1
            elif status == "Completed":
                stats["completed"] += 1
            elif status == "Rejected":
                stats["rejected"] += 1
            elif status == "Deferred":
                stats["deferred"] += 1

            # Count by type
            if item_type == "bug":
                stats["bugs"] += 1
            elif item_type == "feature":
                stats["features"] += 1
            elif item_type == "enhancement":
                stats["enhancements"] += 1

            # Count by requestor
            stats["by_requestor"][requestor] = stats["by_requestor"].get(requestor, 0) + 1

            # Count by priority
            if priority in stats["by_priority"]:
                stats["by_priority"][priority] += 1

            # Count completions this week/month
            if completed_utc:
                try:
                    comp_date = dt.datetime.fromisoformat(completed_utc.replace("Z", "+00:00")).date()
                    if comp_date >= week_start:
                        stats["completed_this_week"] += 1
                    if comp_date >= month_start:
                        stats["completed_this_month"] += 1
                except Exception:
                    pass

        return stats
    except Exception as e:
        print(f"[DEBUG API] Error getting stats: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/debug/weekly-report")
def api_debug_weekly_report(week: str = "", user: str = Depends(require_user)):
    """Generate weekly report of completed items. Week format: YYYY-Wnn (e.g., 2024-W50)"""
    try:
        # Default to current week
        today = dt.date.today()
        if not week:
            week_num = today.isocalendar()[1]
            week = f"{today.year}-W{week_num:02d}"

        # Parse week to get date range
        try:
            year, w = week.split("-W")
            year = int(year)
            week_num = int(w)
            # Monday of that week
            week_start = dt.datetime.strptime(f"{year}-W{week_num:02d}-1", "%G-W%V-%u").date()
            week_end = week_start + dt.timedelta(days=6)
        except Exception:
            return JSONResponse({"error": "Invalid week format. Use YYYY-Wnn"}, status_code=400)

        response = ddb.scan(TableName=DEBUG_TABLE)
        items = response.get("Items", [])

        completed_items = {"bugs": [], "features": [], "enhancements": []}

        for item in items:
            status = item.get("status", {}).get("S", "")
            completed_utc = item.get("completed_utc", {}).get("S", "")

            if status != "Completed" or not completed_utc:
                continue

            try:
                comp_date = dt.datetime.fromisoformat(completed_utc.replace("Z", "+00:00")).date()
                if week_start <= comp_date <= week_end:
                    item_type = item.get("type", {}).get("S", "bug")
                    entry = {
                        "title": item.get("title", {}).get("S", ""),
                        "description": item.get("description", {}).get("S", ""),
                        "requestor": item.get("requestor", {}).get("S", ""),
                        "completed_by": item.get("completed_by", {}).get("S", ""),
                        "resolution_notes": item.get("resolution_notes", {}).get("S", ""),
                        "completed_utc": completed_utc,
                    }
                    if item_type == "bug":
                        completed_items["bugs"].append(entry)
                    elif item_type == "feature":
                        completed_items["features"].append(entry)
                    else:
                        completed_items["enhancements"].append(entry)
            except Exception:
                pass

        # Generate markdown report
        report_md = f"# Weekly Report: {week}\n"
        report_md += f"**Period:** {week_start.strftime('%b %d')} - {week_end.strftime('%b %d, %Y')}\n\n"

        total = len(completed_items["bugs"]) + len(completed_items["features"]) + len(completed_items["enhancements"])
        report_md += f"**Total Completed:** {total} items\n\n"

        if completed_items["bugs"]:
            report_md += f"## Bug Fixes ({len(completed_items['bugs'])})\n"
            for b in completed_items["bugs"]:
                report_md += f"- **{b['title']}**"
                if b["resolution_notes"]:
                    report_md += f": {b['resolution_notes']}"
                if b["requestor"]:
                    report_md += f" _(requested by {b['requestor']})_"
                report_md += "\n"
            report_md += "\n"

        if completed_items["features"]:
            report_md += f"## New Features ({len(completed_items['features'])})\n"
            for f in completed_items["features"]:
                report_md += f"- **{f['title']}**"
                if f["resolution_notes"]:
                    report_md += f": {f['resolution_notes']}"
                if f["requestor"]:
                    report_md += f" _(requested by {f['requestor']})_"
                report_md += "\n"
            report_md += "\n"

        if completed_items["enhancements"]:
            report_md += f"## Enhancements ({len(completed_items['enhancements'])})\n"
            for e in completed_items["enhancements"]:
                report_md += f"- **{e['title']}**"
                if e["resolution_notes"]:
                    report_md += f": {e['resolution_notes']}"
                if e["requestor"]:
                    report_md += f" _(requested by {e['requestor']})_"
                report_md += "\n"

        return {
            "week": week,
            "week_start": week_start.isoformat(),
            "week_end": week_end.isoformat(),
            "items": completed_items,
            "report_markdown": report_md,
            "total": total
        }
    except Exception as e:
        print(f"[DEBUG API] Error generating weekly report: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/debug/release-notes")
def api_debug_release_notes(tag: str = "", user: str = Depends(require_user)):
    """Generate release notes for a specific release tag."""
    try:
        response = ddb.scan(TableName=DEBUG_TABLE)
        items = response.get("Items", [])

        # Get available release tags
        all_tags = set()
        for item in items:
            rtag = item.get("release_tag", {}).get("S", "")
            if rtag:
                all_tags.add(rtag)

        if not tag:
            return {"available_tags": sorted(all_tags, reverse=True), "notes": None}

        # Filter items by tag
        release_items = {"bugs": [], "features": [], "enhancements": []}
        for item in items:
            rtag = item.get("release_tag", {}).get("S", "")
            if rtag != tag:
                continue
            item_type = item.get("type", {}).get("S", "bug")
            entry = {
                "title": item.get("title", {}).get("S", ""),
                "resolution_notes": item.get("resolution_notes", {}).get("S", ""),
                "requestor": item.get("requestor", {}).get("S", ""),
            }
            if item_type == "bug":
                release_items["bugs"].append(entry)
            elif item_type == "feature":
                release_items["features"].append(entry)
            else:
                release_items["enhancements"].append(entry)

        # Generate markdown
        notes_md = f"# Release Notes: {tag}\n\n"
        if release_items["features"]:
            notes_md += "## New Features\n"
            for f in release_items["features"]:
                notes_md += f"- {f['title']}"
                if f["resolution_notes"]:
                    notes_md += f" - {f['resolution_notes']}"
                notes_md += "\n"
            notes_md += "\n"
        if release_items["enhancements"]:
            notes_md += "## Improvements\n"
            for e in release_items["enhancements"]:
                notes_md += f"- {e['title']}"
                if e["resolution_notes"]:
                    notes_md += f" - {e['resolution_notes']}"
                notes_md += "\n"
            notes_md += "\n"
        if release_items["bugs"]:
            notes_md += "## Bug Fixes\n"
            for b in release_items["bugs"]:
                notes_md += f"- {b['title']}"
                if b["resolution_notes"]:
                    notes_md += f" - {b['resolution_notes']}"
                notes_md += "\n"

        return {
            "tag": tag,
            "available_tags": sorted(all_tags, reverse=True),
            "items": release_items,
            "notes_markdown": notes_md,
            "total": len(release_items["bugs"]) + len(release_items["features"]) + len(release_items["enhancements"])
        }
    except Exception as e:
        print(f"[DEBUG API] Error generating release notes: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/debug/report")
async def api_create_debug_report(request: Request, user: str = Depends(require_user)):
    """Create a new debug/improvement report."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)

    title = str(payload.get("title") or "").strip()
    description = str(payload.get("description") or "").strip()
    page_url = str(payload.get("page_url") or "").strip()
    report_type = str(payload.get("type") or "bug").strip()
    priority = str(payload.get("priority") or "Medium").strip()

    if not title or not description:
        return JSONResponse({"error": "title and description required"}, status_code=400)

    import uuid
    report_id = str(uuid.uuid4())
    now_utc = dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z")

    try:
        ddb.put_item(
            TableName=DEBUG_TABLE,
            Item={
                "report_id": {"S": report_id},
                "title": {"S": title},
                "description": {"S": description},
                "page_url": {"S": page_url},
                "requestor": {"S": user},
                "status": {"S": "Open"},
                "priority": {"S": priority},
                "type": {"S": report_type},
                "created_utc": {"S": now_utc},
                "updated_utc": {"S": now_utc},
            }
        )
        return {"ok": True, "report_id": report_id}
    except Exception as e:
        print(f"[DEBUG API] Error creating report: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/debug/report/{report_id}/update")
async def api_update_debug_report(report_id: str, request: Request, user: str = Depends(require_user)):
    """Update a debug report - status, priority, type, resolution, release tag."""
    try:
        payload = await request.json()
    except Exception:
        return JSONResponse({"error": "invalid json"}, status_code=400)

    status = payload.get("status")
    priority = payload.get("priority")
    report_type = payload.get("type")
    resolution_notes = payload.get("resolution_notes")
    release_tag = payload.get("release_tag")

    now_utc = dt.datetime.now(dt.timezone.utc).isoformat().replace("+00:00", "Z")

    try:
        update_expr = "SET updated_utc = :updated"
        expr_values = {":updated": {"S": now_utc}}
        expr_names = {}

        if status:
            update_expr += ", #status = :status"
            expr_values[":status"] = {"S": str(status)}
            expr_names["#status"] = "status"
            # If status is Completed, set completed_utc and completed_by
            if status == "Completed":
                update_expr += ", completed_utc = :completed_utc, completed_by = :completed_by"
                expr_values[":completed_utc"] = {"S": now_utc}
                expr_values[":completed_by"] = {"S": user}

        if priority:
            update_expr += ", priority = :priority"
            expr_values[":priority"] = {"S": str(priority)}

        if report_type:
            update_expr += ", #type = :type"
            expr_values[":type"] = {"S": str(report_type)}
            expr_names["#type"] = "type"

        if resolution_notes is not None:
            update_expr += ", resolution_notes = :resolution_notes"
            expr_values[":resolution_notes"] = {"S": str(resolution_notes)}

        if release_tag is not None:
            update_expr += ", release_tag = :release_tag"
            expr_values[":release_tag"] = {"S": str(release_tag)}

        ddb.update_item(
            TableName=DEBUG_TABLE,
            Key={"report_id": {"S": report_id}},
            UpdateExpression=update_expr,
            ExpressionAttributeValues=expr_values,
            **({"ExpressionAttributeNames": expr_names} if expr_names else {})
        )
        return {"ok": True}
    except Exception as e:
        print(f"[DEBUG API] Error updating report: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.delete("/api/debug/report/{report_id}")
def api_delete_debug_report(report_id: str, user: str = Depends(require_user)):
    """Delete a debug report."""
    try:
        ddb.delete_item(
            TableName=DEBUG_TABLE,
            Key={"report_id": {"S": report_id}}
        )
        return {"ok": True}
    except Exception as e:
        print(f"[DEBUG API] Error deleting report: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


# simple in-memory cache for track API
# Increased TTL to reduce expensive S3 operations (26+ list calls + downloads per uncached request)
_TRACK_CACHE: Dict[Tuple[str, str], Dict[str, Any]] = {}
_TRACK_CACHE_TS: Dict[Tuple[str, str], float] = {}
_TRACK_TTL_SECONDS = 3600  # 1 hour - track data doesn't change frequently

@app.get("/api/track")
def api_track(request: Request, user: str = Depends(require_user)):
    """Return TRACK data: months headers and joined rows with per-month status and tooltip.
    Status rules:
      - POSTED if a Stage 7 record exists for (prop,vendor,acct,month)
      - Else PENDING if a Stage 6 (or Stage 4) record exists for that key
      - Else UPCOMING if today < expected_cutoff (1st of month + daysBetweenBills)
      - Else MISSING
    """
    today = dt.date.today()
    # Build months: prev 9 + current + next 3 (total 13)
    months: list[dt.date] = []
    ref = dt.date(today.year, today.month, 1)
    for i in range(9, 0, -1):
        y = (ref.year * 12 + ref.month - i - 1) // 12
        m = (ref.month - i - 1) % 12 + 1
        months.append(dt.date(y, m, 1))
    for i in range(0, 4):
        y = (ref.year * 12 + ref.month + i - 1) // 12
        m = (ref.month + i - 1) % 12 + 1
        months.append(dt.date(y, m, 1))
    month_labels = [d.strftime("%b").upper() for d in months]
    month_keys = [f"{d.year:04d}-{d.month:02d}" for d in months]

    # Load configs (before cache so we can key on accounts length)
    accounts = _get_accounts_to_track()
    mapping = _ddb_get_config("ap-mapping") or []
    map_by_pid = {str(r.get("propertyId") or "").strip(): r for r in mapping if isinstance(r, dict)}

    # Build vendor ID -> vendor code lookup from vendor cache
    vendor_code_map: dict[str, str] = {}
    try:
        vend_cache_obj = s3.get_object(Bucket="api-vendor", Key="vendors/latest.json")
        vend_cache_data = json.loads(vend_cache_obj["Body"].read().decode("utf-8"))
        for v in vend_cache_data.get("vendors", []):
            vid = str(v.get("vendorId", "")).strip()
            vcode = str(v.get("vendorCode", "")).strip()
            if vid and vcode:
                vendor_code_map[vid] = vcode
    except Exception as e:
        print(f"[api_track] Failed to load vendor cache for codes: {e}")
    # cache key by month range and accounts length; allow manual bypass
    cache_key = (month_keys[0], month_keys[-1], len(accounts))
    now_ts = dt.datetime.utcnow().timestamp()
    do_refresh = request.query_params.get("refresh") in ("1", "true", "yes")
    if (not do_refresh) and (cache_key in _TRACK_CACHE) and (now_ts - _TRACK_CACHE_TS.get(cache_key, 0) < _TRACK_TTL_SECONDS):
        cache_age = now_ts - _TRACK_CACHE_TS.get(cache_key, 0)
        print(f"[TRACK CACHE HIT] age={cache_age:.1f}s, accounts={len(accounts)}, months={month_keys[0]} to {month_keys[-1]}")
        return _TRACK_CACHE[cache_key]

    print(f"[TRACK CACHE MISS] Fetching from S3: accounts={len(accounts)}, months={month_keys[0]} to {month_keys[-1]}, refresh={do_refresh}")

    # Determine day range for S3 scans
    start_day = months[0]
    # to last day of last month cell
    end_month = months[-1]
    if end_month.month == 12:
        next_month = dt.date(end_month.year + 1, 1, 1)
    else:
        next_month = dt.date(end_month.year, end_month.month + 1, 1)
    end_day = next_month - dt.timedelta(days=1)

    # Collect keys and read records (month-level listing to reduce API calls)
    stage4_keys = list(_iter_stage_objects_by_month(STAGE4_PREFIX, months))
    stage6_keys = list(_iter_stage_objects_by_month(STAGE6_PREFIX, months))
    stage4 = _read_json_records_from_s3(stage4_keys)
    stage6 = _read_json_records_from_s3(stage6_keys)
    stage7_keys = list(_iter_stage_objects_by_month(POST_ENTRATA_PREFIX, months))
    stage7 = _read_json_records_from_s3(stage7_keys)

    # Also check Historical Archive for archived bills
    archive_keys = list(_iter_stage_objects_by_month(HIST_ARCHIVE_PREFIX, months))
    archive = _read_json_records_from_s3(archive_keys)

    # Merge archive into stage7 so archived bills show as POSTED
    stage7.extend(archive)

    def norm_rec(rec: dict) -> dict:
        pid = rec.get("EnrichedPropertyID") or rec.get("propertyId") or rec.get("PropertyID")
        vid = rec.get("EnrichedVendorID") or rec.get("vendorId") or rec.get("VendorID")
        acct = rec.get("Account Number") or rec.get("accountNumber") or rec.get("AccountNumber")
        bill_date = rec.get("Bill Date") or rec.get("billDate")
        pstart = rec.get("Bill Period Start") or rec.get("billPeriodStart")
        pend = rec.get("Bill Period End") or rec.get("billPeriodEnd")
        due = rec.get("Due Date") or rec.get("dueDate")
        bd = _parse_date_any(str(bill_date or ""))
        mk = f"{bd.year:04d}-{bd.month:02d}" if bd else None
        return {
            "propertyId": str(pid or "").strip(),
            "vendorId": str(vid or "").strip(),
            "accountNumber": str(acct or "").strip(),
            "billDate": bd.isoformat() if bd else "",
            "billPeriodStart": pstart,
            "billPeriodEnd": pend,
            "dueDate": due,
            "monthKey": mk,
        }

    idx4 = {}
    for r in stage4:
        n = norm_rec(r)
        if n["monthKey"]:
            idx4[(n["propertyId"], n["vendorId"], n["accountNumber"], n["monthKey"])] = n
    idx6 = {}
    for r in stage6:
        n = norm_rec(r)
        if n["monthKey"]:
            idx6[(n["propertyId"], n["vendorId"], n["accountNumber"], n["monthKey"])] = n

    idx7 = {}
    for r in stage7:
        n = norm_rec(r)
        if n["monthKey"]:
            idx7[(n["propertyId"], n["vendorId"], n["accountNumber"], n["monthKey"])] = n

    def status_for(key_tuple: tuple, cutoff: dt.date) -> tuple[str, dict|None]:
        r7 = idx7.get(key_tuple)
        r6 = idx6.get(key_tuple)
        r4 = idx4.get(key_tuple)
        if r7:
            return ("POSTED", r7)
        if r6:
            return ("PENDING", r6)
        if r4:
            return ("PENDING", r4)
        if dt.date.today() < cutoff:
            return ("UPCOMING", None)
        return ("MISSING", None)

    rows = []
    for a in accounts:
        pid = str(a.get("propertyId") or "").strip()
        vid = str(a.get("vendorId") or "").strip()
        acct = str(a.get("accountNumber") or "").strip()
        days = int(a.get("daysBetweenBills") or 0)
        mrow = map_by_pid.get(pid) or {}
        ap = str(mrow.get("name") or "").strip()
        # Determine latest bill date across the window for this (pid,vid,acct)
        latest_bill_glob: dt.date | None = None
        for mk in month_keys:
            rec0 = (
                idx7.get((pid, vid, acct, mk))
                or idx6.get((pid, vid, acct, mk))
                or idx4.get((pid, vid, acct, mk))
            )
            if rec0:
                b = rec0.get("billDate")
                if b:
                    try:
                        d = dt.datetime.strptime(b, "%Y-%m-%d").date() if "-" in b else dt.datetime.strptime(b, "%m/%d/%Y").date()
                        if (latest_bill_glob is None) or (d > latest_bill_glob):
                            latest_bill_glob = d
                    except Exception:
                        pass
        next_expected: dt.date | None = None
        if latest_bill_glob and days:
            next_expected = latest_bill_glob + dt.timedelta(days=int(days))

        cells = []
        for mk, md in zip(month_keys, months):
            cutoff = md + dt.timedelta(days=max(days, 0))
            if next_expected and next_expected.year == md.year and next_expected.month == md.month:
                cutoff = next_expected
            st, rec = status_for((pid, vid, acct, mk), cutoff)
            tip = None
            label = ""
            if rec:
                label = rec.get("billDate") or ""
                tip = {
                    "billDate": rec.get("billDate") or "",
                    "billPeriodStart": rec.get("billPeriodStart") or "",
                    "billPeriodEnd": rec.get("billPeriodEnd") or "",
                    "dueDate": rec.get("dueDate") or "",
                }
            cells.append({"key": mk, "status": st, "label": label, "tooltip": tip})
        exp_str = ""
        if next_expected:
            exp = next_expected
            exp_str = exp.strftime("%Y-%m-%d")

        rows.append({
            "apName": ap,
            "vendorId": vid,
            "vendorCode": vendor_code_map.get(vid, ""),
            "vendorName": a.get("vendorName") or "",
            "accountNumber": acct,
            "propertyId": pid,
            "propertyName": a.get("propertyName") or "",
            "glAccountName": a.get("glAccountName") or "",
            "expectedBillDate": exp_str,
            "daysBetweenBills": days,
            "months": cells,
        })

    out = {"months": month_labels, "rows": rows}
    _TRACK_CACHE[cache_key] = out
    _TRACK_CACHE_TS[cache_key] = now_ts
    return out
@app.post("/api/delete_preentrata")
def api_delete_preentrata(key: str = Form(...), user: str = Depends(require_user)):
    """Delete a single Pre-Entrata merged file by its full S3 key.
    Returns {ok:true, deleted:1} on success. Missing files are treated as success with deleted:0.
    Also clears the 'Submitted' status so PARSE page shows correct status.
    """
    try:
        if not key or not key.startswith(PRE_ENTRATA_PREFIX):
            return JSONResponse({"error": "bad key"}, status_code=400)
        # If it doesn't exist, treat as success (idempotent)
        try:
            s3.head_object(Bucket=BUCKET, Key=key)
        except Exception:
            return {"ok": True, "deleted": 0}

        # Read the file BEFORE deleting to get line IDs for status reset
        cleared_count = 0
        y, m, d = None, None, None
        try:
            txt = _read_s3_text(BUCKET, key)
            lines = [json.loads(l) for l in txt.strip().split('\n') if l.strip()]
            for rec in lines:
                # Get the original S3 key and row index to build the review table PK
                orig_key = rec.get("__s3_key__", "")
                row_idx = rec.get("__row_idx__", 0)
                if orig_key:
                    line_id = line_id_from(orig_key, row_idx)
                    # Delete from review table to clear Submitted status
                    review_pk = f"review#{line_id}"
                    try:
                        ddb.delete_item(TableName=REVIEW_TABLE, Key={"pk": {"S": review_pk}})
                        cleared_count += 1
                    except Exception:
                        pass
                    # Extract date from key for cache invalidation
                    if not y:
                        y, m, d = _extract_ymd_from_key(orig_key)
        except Exception as e:
            print(f"[DELETE_PREENTRATA] Warning: Failed to clear Submitted status: {e}")

        s3.delete_object(Bucket=BUCKET, Key=key)
        # Invalidate post_view cache so deleted items don't reappear
        _CACHE.pop("post_view_items", None)
        # Also invalidate the day cache so PARSE page shows updated status
        if y and m and d:
            invalidate_day_cache(y, m, d)
        print(f"[DELETE_PREENTRATA] Deleted {key}, cleared {cleared_count} line statuses")
        return {"ok": True, "deleted": 1, "cleared": cleared_count}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/day", response_class=HTMLResponse)
def day_view(request: Request, date: str, user: str = Depends(require_user)):
    try:
        y, m, d = date.split("-")
    except ValueError:
        return RedirectResponse("/", status_code=302)
    rows = load_day(y, m, d)
    # show a limited set of columns
    cols = [
        "Invoice Number","Account Number","Line Item Account Number","Service Address","Utility Type",
        "Bill Period Start","Bill Period End","EnrichedGLAccountNumber","EnrichedGLAccountName","GL DESC_NEW",
        "ENRICHED CONSUMPTION","ENRICHED UOM","PDF_LINK","__id__","__s3_key__","__row_idx__"
    ]
    # filter
    view = [{k: r.get(k) for k in cols} for r in rows]
    return templates.TemplateResponse("day.html", {"request": request, "date": date, "rows": view, "user": user})


@app.get("/invoices", response_class=HTMLResponse)
def invoices_view(request: Request, date: str, user: str = Depends(require_user)):
    try:
        y, m, d = date.split("-")
    except ValueError:
        return RedirectResponse("/", status_code=302)
    rows = load_day(y, m, d)

    # Pre-collect all unique pdf_ids to batch fetch header drafts in parallel
    unique_pdf_ids = set()
    for r in rows:
        s3_key = r.get("__s3_key__", "")
        if s3_key:
            unique_pdf_ids.add(pdf_id_from_key(s3_key))
    unique_pdf_ids.discard("(unknown)")

    # Batch fetch all header drafts in parallel (30 concurrent DynamoDB calls)
    header_drafts = get_header_drafts_batch(list(unique_pdf_ids), user)

    # group by (vendor, account, pdf_id) and collect ids; prefer header draft vendor override
    inv: Dict[tuple, Dict[str, Any]] = {}
    group_ids: Dict[tuple, List[str]] = {}
    header_vendor_cache: Dict[str, str] = {}
    # Cache the first valid vendor per pdf_id to ensure consistency across all rows from same PDF
    pdf_vendor_cache: Dict[str, str] = {}
    # Track accounts per pdf_id to detect multi-account PDFs
    pdf_accounts: Dict[str, set] = {}
    for r in rows:
        # Fall back from Account Number to Line Item Account Number for consistency
        account_no = str(r.get("Account Number", "") or r.get("Line Item Account Number", "") or "") or "(unknown)"
        # Use a stable unique id for the bill: hash of the source s3 key (pdf_id)
        pdf_id = pdf_id_from_key(r.get("__s3_key__", "")) if r.get("__s3_key__") else "(unknown)"
        invoice_no = str(r.get("Invoice Number", "")) or "(unknown)"  # retained only for display if needed
        # Try to load a header draft for this pdf to get the latest vendor name selected by the user
        vend_override = None
        try:
            if pdf_id and pdf_id != "(unknown)":
                if pdf_id in header_vendor_cache:
                    vend_override = header_vendor_cache[pdf_id]
                else:
                    # Use pre-fetched header draft instead of sequential DynamoDB call
                    dft = header_drafts.get(pdf_id)
                    if dft and isinstance(dft.get("fields"), dict):
                        vo = str(dft["fields"].get("EnrichedVendorName") or "").strip()
                        if vo:
                            vend_override = vo
                    header_vendor_cache[pdf_id] = vend_override or ""
        except Exception:
            pass
        # Check if we already have a vendor cached for this pdf_id (ensures all rows from same PDF group together)
        if pdf_id in pdf_vendor_cache and pdf_vendor_cache[pdf_id]:
            vendor = pdf_vendor_cache[pdf_id]
        else:
            vendor = (
                vend_override
                or str(r.get("EnrichedVendorName", ""))
                or str(r.get("Vendor Name", ""))
                or str(r.get("Vendor", ""))
                or str(r.get("Utility Type", ""))
                or "(unknown)"
            )
            # Cache the vendor for subsequent rows from the same PDF
            if pdf_id != "(unknown)" and vendor and vendor != "(unknown)":
                pdf_vendor_cache[pdf_id] = vendor
        key = (vendor, account_no, pdf_id)
        # Track accounts per pdf_id for multi-account detection
        pdf_accounts.setdefault(pdf_id, set()).add(account_no)
        # Extract property name for display in invoice list
        property_name = (
            str(r.get("EnrichedPropertyName", ""))
            or str(r.get("Property Name", ""))
            or str(r.get("PropertyName", ""))
            or ""
        )
        g = inv.setdefault(key, {
            "vendor": vendor,
            "account": account_no,
            "invoice": invoice_no,
            "parsed_date": date,
            "parsed_dt": None,
            "parsed_dt_fmt": None,
            "submitted_at": None,
            "submitted_at_fmt": None,
            "count": 0,
            "status": "REVIEW",
            "pdf_id": pdf_id,
            "total_amount": 0.0,
            "s3_key": r.get("__s3_key__", ""),  # Keep original S3 key for splitting
            "property": property_name
        })
        # Update property if not set (first row may not have it)
        if not g.get("property") and property_name:
            g["property"] = property_name
        g["count"] += 1
        # Sum up line item charges (exclude summary/aggregate rows like subtotals, taxes, totals)
        try:
            desc = str(r.get("Line Item Description", "")).upper().strip()
            # Skip rows with descriptions indicating they're summary/aggregate lines
            # Use word boundaries (\b) to avoid false positives like "FEES" matching "FEE"
            # Also check for exact matches of common summary-only terms
            summary_patterns = [
                r'\bSUBTOTAL\b', r'\bGRAND TOTAL\b', r'\bBALANCE DUE\b', r'\bAMOUNT DUE\b',
                r'\bTOTAL DUE\b', r'\bTOTAL CHARGES?\b', r'\bTOTAL AMOUNT\b'
            ]
            # Exact match for very short descriptions that are clearly summaries
            exact_summary = desc in ['TOTAL', 'SUBTOTAL', 'TAX', 'TAXES', 'BALANCE', 'AMOUNT DUE', 'TOTAL DUE']
            is_summary_row = exact_summary or any(re.search(p, desc) for p in summary_patterns)

            if not is_summary_row:
                charge_str = str(r.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                g["total_amount"] += float(charge_str) if charge_str else 0.0
        except (ValueError, TypeError):
            pass
        # pdf_id already set above; keep earliest parsed time below
        group_ids.setdefault(key, []).append(str(r.get("__id__")))
        # capture earliest parsed_at_utc for this group if present
        pat = r.get("parsed_at_utc") or r.get("ParsedAtUtc")
        if pat:
            try:
                if (g["parsed_dt"] is None) or (pat < g["parsed_dt"]):
                    # normalize to milliseconds without timezone for display
                    ts = pat.replace('Z', '+00:00')
                    try:
                        dtv = dt.datetime.fromisoformat(ts)
                        g["parsed_dt_fmt"] = dtv.strftime('%Y-%m-%d %H:%M:%S.%f')[:23]
                    except Exception:
                        g["parsed_dt_fmt"] = pat
                    g["parsed_dt"] = pat
            except Exception:
                pass

    # batch lookup statuses once
    all_ids: List[str] = [i for ids in group_ids.values() for i in ids if i]
    stmap = get_status_map(all_ids)

    def has_account_artifact(acct_val: str) -> bool:
        """Return True if an artifact exists for this account on the day in either Stage 6 (Pre-Entrata) OR Stage 7 (Post-Entrata).
        We match by sanitized account token anywhere in the basename.
        """
        acct = (acct_val or "").strip()
        if not acct:
            return False
        token_dash = "".join(ch if (ch.isalnum() or ch in "-") else "-" for ch in acct)
        token_us = "".join(ch if ch.isalnum() else "_" for ch in acct)
        prefixes = [
            f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/",
            f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/",
        ]
        for pfx in prefixes:
            try:
                resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=pfx)
                for obj in resp.get("Contents", []) or []:
                    k = obj.get("Key", "")
                    if token_dash in k or token_us in k:
                        return True
            except Exception:
                continue
        return False

    # compute status per group consistent with dashboard rule
    for key, meta in inv.items():
        _, acct, pid = key
        ids_all = group_ids.get(key, [])
        # Exclude Deleted lines from status calculation (matches API logic)
        ids_active = [i for i in ids_all if stmap.get(i, {}).get("status") != "Deleted"]
        submitted = sum(1 for i in ids_active if stmap.get(i, {}).get("status") == "Submitted")
        # Get latest submitted_at time for this group
        submitted_times = [stmap.get(i, {}).get("submitted_at") for i in ids_active if stmap.get(i, {}).get("submitted_at")]
        if submitted_times:
            latest_submit = max(submitted_times)
            meta["submitted_at"] = latest_submit
            try:
                ts = latest_submit.replace('Z', '+00:00')
                dtv = dt.datetime.fromisoformat(ts)
                meta["submitted_at_fmt"] = dtv.strftime('%Y-%m-%d %H:%M:%S')[:19]
            except Exception:
                meta["submitted_at_fmt"] = latest_submit[:19] if latest_submit else None
        if submitted == 0:
            meta["status"] = "REVIEW"
        elif submitted == len(ids_active):
            # All non-deleted lines submitted = COMPLETE
            meta["status"] = "COMPLETE"
        else:
            meta["status"] = "PARTIAL"
        # Flag multi-account PDFs
        accounts_in_pdf = pdf_accounts.get(pid, set())
        meta["multi_account"] = len(accounts_in_pdf) > 1
        meta["account_count"] = len(accounts_in_pdf)
    invoices = list(inv.values())
    # sort: REVIEW/PARTIAL first (oldest dates first), COMPLETE at bottom
    status_rank = {"REVIEW": 0, "PARTIAL": 1, "COMPLETE": 2}
    invoices.sort(key=lambda x: (status_rank.get(x["status"], 9), x.get("parsed_dt") or x["parsed_date"], x["vendor"], x["account"], x["invoice"]))
    return templates.TemplateResponse("invoices.html", {"request": request, "date": date, "invoices": invoices, "user": user})


@app.post("/api/delete_parsed")
def api_delete_parsed(date: str = Form(...), pdf_ids: str = Form(...), user: str = Depends(require_user)):
    """Delete parsed invoices by pdf_id by removing their underlying enriched jsonl files for the given day.
    This deletes all S3 objects for the day that contain lines associated with the provided pdf_ids.
    """
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)
    try:
        wanted = set([p for p in (pdf_ids or '').split(',') if p])
        if not wanted:
            return JSONResponse({"error": "no pdf_ids"}, status_code=400)
        rows = load_day(y, m, d)
        # find all source keys that have any row with matching pdf_id
        keys = set()
        accounts = set()
        for r in rows:
            key = r.get("__s3_key__")
            if not key:
                continue
            pid = pdf_id_from_key(key)
            if pid in wanted:
                keys.add(key)
                acct = str(r.get("Account Number", "")).strip()
                if acct:
                    accounts.add(acct)
        if not keys:
            return {"ok": True, "deleted": 0}
        # delete the objects
        deleted = 0
        for k in keys:
            s3.delete_object(Bucket=BUCKET, Key=k)
            deleted += 1
        # Also delete any Pre-Entrata (Stage 6) files that match the deleted invoices
        # Match on business fields: Account Number + Bill Period Start + Bill Period End + Bill Date
        # This ensures we only delete the exact invoice, not unrelated ones with same account
        # Also clear DynamoDB status so counts stay consistent across all views
        if keys:
            # Build set of (account, bill_period_start, bill_period_end, bill_date) tuples to match
            invoice_signatures = set()
            for r in rows:
                key = r.get("__s3_key__")
                if key and key in keys:
                    sig = (
                        str(r.get("Account Number", "")).strip(),
                        str(r.get("Bill Period Start", "")).strip(),
                        str(r.get("Bill Period End", "")).strip(),
                        str(r.get("Bill Date", "")).strip(),
                    )
                    invoice_signatures.add(sig)

            day_prefix = f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"
            resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=day_prefix)
            s6_keys_to_check = [obj.get("Key", "") for obj in (resp.get("Contents", []) or []) if obj.get("Key", "").endswith(".jsonl")]

            # Parallel S3 reads to check Stage 6 file signatures
            # IMPORTANT: Also verify source_input_key matches one of the deleted S3 keys
            # This prevents deleting Stage 6 files for TRUE DUPLICATES where user submitted
            # one copy and is deleting the other copy from PARSE
            def _check_s6_file(s6_key):
                try:
                    s6_content = _read_s3_text(BUCKET, s6_key)
                    lines = [l for l in s6_content.strip().split('\n') if l.strip()] if s6_content else []
                    if lines:
                        first_rec = json.loads(lines[0])
                        # Check if the source_input_key matches one of the deleted keys
                        # This is the S3 key of the original Stage 4 file that was submitted
                        s6_source_key = first_rec.get("source_input_key", "") or first_rec.get("__s3_key__", "")
                        if s6_source_key not in keys:
                            # This Stage 6 file came from a DIFFERENT source file (true duplicate)
                            # Don't delete it - the user only wants to delete one copy
                            return None
                        s6_sig = (
                            str(first_rec.get("Account Number", "")).strip(),
                            str(first_rec.get("Bill Period Start", "")).strip(),
                            str(first_rec.get("Bill Period End", "")).strip(),
                            str(first_rec.get("Bill Date", "")).strip(),
                        )
                        if s6_sig in invoice_signatures:
                            return (s6_key, lines)
                except Exception:
                    pass
                return None

            to_delete = []
            if s6_keys_to_check:
                futures = [_GLOBAL_EXECUTOR.submit(_check_s6_file, k) for k in s6_keys_to_check]
                try:
                    for f in as_completed(futures, timeout=60):
                        try:
                            result = f.result()
                            if result:
                                to_delete.append(result)
                        except Exception:
                            pass
                except TimeoutError:
                    print(f"[delete_parsed] Timeout checking Stage 6 files  proceeding with {len(to_delete)} matches found so far")

            # Parallel DDB deletes + S3 file deletes
            def _delete_s6_file(s6_key, lines):
                try:
                    for line in lines:
                        try:
                            rec = json.loads(line)
                            orig_key = rec.get("__s3_key__", "")
                            row_idx = rec.get("__row_idx__", 0)
                            if orig_key:
                                line_id = f"{orig_key}#{row_idx}"
                                ddb.delete_item(TableName=REVIEW_TABLE, Key={"pk": {"S": line_id}})
                        except Exception:
                            pass
                    s3.delete_object(Bucket=BUCKET, Key=s6_key)
                    return 1
                except Exception:
                    return 0

            if to_delete:
                del_futures = [_GLOBAL_EXECUTOR.submit(_delete_s6_file, s6k, lns) for s6k, lns in to_delete]
                try:
                    for f in as_completed(del_futures, timeout=60):
                        try:
                            deleted += f.result()
                        except Exception:
                            pass
                except TimeoutError:
                    print(f"[delete_parsed] Timeout deleting Stage 6 files  some may remain")
        # Invalidate caches so deleted files don't show up
        invalidate_day_cache(y, m, d)
        _CACHE.pop(("day_status_counts", y, m, d), None)
        _CACHE.pop(("parse_dashboard",), None)
        return {"ok": True, "deleted": deleted}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


def _find_invoices_all_stages(y: str, m: str, d: str, wanted_pdf_ids: set) -> dict:
    """
    Search for invoices across Stage 4 and 6 by pdf_id.
    Returns dict of s3_key -> list of rows for matching invoices.
    This is needed because submitted invoices move from Stage 4 to Stage 6.
    Note: Stage 7 (posted) is intentionally excluded - once posted, invoices shouldn't be modified.
    """
    by_key: dict[str, list] = {}
    found_pdf_ids = set()

    # Stage 4: Enriched (pending review)
    stage4_prefix = f"{ENRICH_PREFIX}yyyy={y}/mm={m}/dd={d}/"
    # Stage 6: Pre-Entrata (submitted, pending POST)
    stage6_prefix = f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"

    for prefix in [stage4_prefix, stage6_prefix]:
        # Skip if we already found all wanted pdf_ids
        remaining = wanted_pdf_ids - found_pdf_ids
        if not remaining:
            break

        try:
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    key = obj['Key']
                    if not (key.endswith('.jsonl') or key.endswith('.jsonl.gz')):
                        continue
                    pid = pdf_id_from_key(key)
                    if pid in remaining and pid not in found_pdf_ids:
                        # Found a matching invoice - read it
                        try:
                            txt = _read_s3_text(BUCKET, key)
                            rows = [json.loads(l) for l in txt.strip().split('\n') if l.strip()]
                            for r in rows:
                                r["__s3_key__"] = key
                            by_key[key] = rows
                            found_pdf_ids.add(pid)
                        except Exception as e:
                            print(f"[_find_invoices_all_stages] Error reading {key}: {e}")
        except Exception as e:
            print(f"[_find_invoices_all_stages] Error listing {prefix}: {e}")

    return by_key


@app.post("/api/bulk_assign_property")
def api_bulk_assign_property(
    date: str = Form(...),
    pdf_ids: str = Form(...),
    property_id: str = Form(""),
    property_name: str = Form(""),
    user: str = Depends(require_user)
):
    """Bulk assign property to all lines in the selected invoices."""
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)

    wanted = set([p for p in (pdf_ids or '').split(',') if p])
    if not wanted:
        return JSONResponse({"error": "no pdf_ids"}, status_code=400)
    if not property_id and not property_name:
        return JSONResponse({"error": "property_id or property_name required"}, status_code=400)

    try:
        # Search across all stages (4, 6, 7) for the invoices
        by_key = _find_invoices_all_stages(y, m, d, wanted)

        if not by_key:
            return {"ok": True, "updated": 0, "message": "No matching invoices found"}

        # Update each file (parallel S3 writes)
        def _write_property_update(s3_key, key_rows):
            for rec in key_rows:
                if property_id:
                    rec["EnrichedPropertyID"] = property_id
                if property_name:
                    rec["EnrichedPropertyName"] = property_name
                rec.pop("__s3_key__", None)
            new_content = '\n'.join(json.dumps(rec, ensure_ascii=False) for rec in key_rows)
            if s3_key.endswith('.gz'):
                body = gzip.compress(new_content.encode('utf-8'))
                s3.put_object(Bucket=BUCKET, Key=s3_key, Body=body, ContentType='application/json', ContentEncoding='gzip')
            else:
                s3.put_object(Bucket=BUCKET, Key=s3_key, Body=new_content.encode('utf-8'), ContentType='application/json')
            return 1

        updated_count = 0
        s3_futures = [_GLOBAL_EXECUTOR.submit(_write_property_update, k, v) for k, v in by_key.items()]
        try:
            for f in as_completed(s3_futures, timeout=60):
                try:
                    updated_count += f.result()
                except Exception:
                    pass
        except TimeoutError:
            print(f"[bulk_assign_property] Timeout writing S3 files  {updated_count} of {len(by_key)} completed")

        # Update or create header drafts in DynamoDB so property override matches S3
        # Update BOTH user draft AND __final__ draft to ensure change sticks (parallel)
        def _update_property_draft(pdf_id_val, draft_user):
            try:
                pk = f"draft#{pdf_id_val}#__header__#{draft_user}"
                resp = ddb.get_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
                item = resp.get("Item")
                if item:
                    fields_raw = item.get("fields", {}).get("S", "{}")
                    fields = json.loads(fields_raw) if fields_raw else {}
                else:
                    fields = {}
                if property_id:
                    fields["EnrichedPropertyID"] = property_id
                if property_name:
                    fields["EnrichedPropertyName"] = property_name
                ddb.put_item(
                    TableName=DRAFTS_TABLE,
                    Item={
                        "pk": {"S": pk},
                        "pdf_id": {"S": pdf_id_val},
                        "line_id": {"S": "__header__"},
                        "user": {"S": draft_user},
                        "fields": {"S": json.dumps(fields, ensure_ascii=False)},
                        "updated_utc": {"S": dt.datetime.utcnow().isoformat()}
                    }
                )
            except Exception as e:
                print(f"[bulk_assign_property] Warning: Failed to update header draft for {pdf_id_val}/{draft_user}: {e}")

        ddb_futures = [_GLOBAL_EXECUTOR.submit(_update_property_draft, pid, du) for pid in wanted for du in [user, "__final__"]]
        try:
            for f in as_completed(ddb_futures, timeout=60):
                try:
                    f.result()
                except Exception:
                    pass
        except TimeoutError:
            print("[bulk_assign_property] Timeout updating DDB drafts  some may not be updated")

        # Invalidate cache
        invalidate_day_cache(y, m, d)
        return {"ok": True, "updated": updated_count}
    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/bulk_assign_vendor")
def api_bulk_assign_vendor(
    date: str = Form(...),
    pdf_ids: str = Form(...),
    vendor_id: str = Form(""),
    vendor_name: str = Form(""),
    user: str = Depends(require_user)
):
    """Bulk assign vendor to all lines in the selected invoices."""
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)

    wanted = set([p for p in (pdf_ids or '').split(',') if p])
    if not wanted:
        return JSONResponse({"error": "no pdf_ids"}, status_code=400)
    if not vendor_id and not vendor_name:
        return JSONResponse({"error": "vendor_id or vendor_name required"}, status_code=400)

    try:
        # Search across all stages (4, 6, 7) for the invoices
        by_key = _find_invoices_all_stages(y, m, d, wanted)

        if not by_key:
            return {"ok": True, "updated": 0, "message": "No matching invoices found"}

        # Update each file (parallel S3 writes)
        def _write_vendor_update(s3_key, key_rows):
            for rec in key_rows:
                if vendor_id:
                    rec["EnrichedVendorID"] = vendor_id
                if vendor_name:
                    rec["EnrichedVendorName"] = vendor_name
                rec.pop("__s3_key__", None)
            new_content = '\n'.join(json.dumps(rec, ensure_ascii=False) for rec in key_rows)
            if s3_key.endswith('.gz'):
                body = gzip.compress(new_content.encode('utf-8'))
                s3.put_object(Bucket=BUCKET, Key=s3_key, Body=body, ContentType='application/json', ContentEncoding='gzip')
            else:
                s3.put_object(Bucket=BUCKET, Key=s3_key, Body=new_content.encode('utf-8'), ContentType='application/json')
            return 1

        updated_count = 0
        s3_futures = [_GLOBAL_EXECUTOR.submit(_write_vendor_update, k, v) for k, v in by_key.items()]
        try:
            for f in as_completed(s3_futures, timeout=60):
                try:
                    updated_count += f.result()
                except Exception:
                    pass
        except TimeoutError:
            print(f"[bulk_assign_vendor] Timeout writing S3 files  {updated_count} of {len(by_key)} completed")

        # Update or create header drafts in DynamoDB so vendor override matches S3
        # Update BOTH user draft AND __final__ draft to ensure change sticks (parallel)
        def _update_vendor_draft(pdf_id_val, draft_user):
            try:
                pk = f"draft#{pdf_id_val}#__header__#{draft_user}"
                resp = ddb.get_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
                item = resp.get("Item")
                if item:
                    fields_raw = item.get("fields", {}).get("S", "{}")
                    try:
                        fields = json.loads(fields_raw) if isinstance(fields_raw, str) else {}
                    except Exception:
                        fields = {}
                else:
                    fields = {}
                if vendor_name:
                    fields["EnrichedVendorName"] = vendor_name
                if vendor_id:
                    fields["EnrichedVendorID"] = vendor_id
                ddb.put_item(
                    TableName=DRAFTS_TABLE,
                    Item={
                        "pk": {"S": pk},
                        "pdf_id": {"S": pdf_id_val},
                        "line_id": {"S": "__header__"},
                        "user": {"S": draft_user},
                        "fields": {"S": json.dumps(fields, ensure_ascii=False)},
                        "updated_utc": {"S": dt.datetime.utcnow().isoformat()}
                    }
                )
            except Exception as e:
                print(f"[bulk_assign_vendor] Warning: Failed to update header draft for {pdf_id_val}/{draft_user}: {e}")

        ddb_futures = [_GLOBAL_EXECUTOR.submit(_update_vendor_draft, pid, du) for pid in wanted for du in [user, "__final__"]]
        try:
            for f in as_completed(ddb_futures, timeout=60):
                try:
                    f.result()
                except Exception:
                    pass
        except TimeoutError:
            print("[bulk_assign_vendor] Timeout updating DDB drafts  some may not be updated")

        # Invalidate cache
        invalidate_day_cache(y, m, d)
        return {"ok": True, "updated": updated_count}
    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/bulk_rework")
def api_bulk_rework(
    date: str = Form(...),
    pdf_ids: str = Form(...),
    notes: str = Form(""),
    user: str = Depends(require_user)
):
    """Bulk send selected invoices back for rework with shared notes."""
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)

    wanted = list(set([p for p in (pdf_ids or '').split(',') if p]))
    if not wanted:
        return JSONResponse({"error": "no pdf_ids"}, status_code=400)

    try:
        rows = load_day(y, m, d)

        # Group rows by pdf_id
        by_pdf: dict[str, list] = {}
        for r in rows:
            key = r.get("__s3_key__")
            if not key:
                continue
            pid = pdf_id_from_key(key)
            if pid in wanted:
                by_pdf.setdefault(pid, []).append(r)

        sent_count = 0
        errors = []

        for pid in wanted:
            pid_rows = by_pdf.get(pid, [])
            if not pid_rows:
                errors.append(f"No data found for {pid}")
                continue

            first = pid_rows[0]

            # Find PDF link
            pdf_link = first.get("PDF_LINK", "")
            if not pdf_link:
                # Try to infer from key
                pdf_link = _infer_pdf_key_for_doc(y, m, d, pid_rows, pid)

            if not pdf_link:
                errors.append(f"No PDF link for {pid}")
                continue

            # Parse the S3 key from PDF link
            final_url = pdf_link
            if '?' in final_url:
                final_url = final_url.split('?')[0]

            parsed = None
            if final_url.startswith('s3://'):
                parts = final_url[5:].split('/', 1)
                if len(parts) == 2:
                    parsed = (parts[0], parts[1])
            elif '.s3.' in final_url or 's3.amazonaws.com' in final_url:
                try:
                    from urllib.parse import urlparse
                    pu = urlparse(final_url)
                    if '.s3.' in pu.netloc:
                        bucket_part = pu.netloc.split('.s3.')[0]
                        key_part = pu.path.lstrip('/')
                        parsed = (bucket_part, key_part)
                    elif 's3.amazonaws.com' in pu.netloc:
                        path_parts = pu.path.lstrip('/').split('/', 1)
                        if len(path_parts) == 2:
                            parsed = (path_parts[0], path_parts[1])
                except Exception:
                    pass

            if not parsed:
                errors.append(f"Cannot parse PDF location for {pid}")
                continue

            src_bucket, src_key = parsed

            # Copy to rework prefix
            ts = dt.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
            base = os.path.basename(src_key).replace("\\", "_").replace("/", "_")
            dest_key = f"{REWORK_PREFIX}yyyy={y}/mm={m}/dd={d}/{ts}_{pid}_{base}"

            try:
                s3.copy_object(Bucket=BUCKET, CopySource={"Bucket": src_bucket, "Key": src_key}, Key=dest_key)
            except Exception as e:
                errors.append(f"Copy failed for {pid}: {str(e)}")
                continue

            # Write sidecar rework.json with notes
            meta = {
                "notes": notes,
                "user": user,
                "rework_ts": ts,
                "original_pdf_id": pid,
                "bill_from": first.get("Bill From", ""),
                "account_number": first.get("Account Number", ""),
                "bill_date": first.get("Bill Date", ""),
            }
            meta_key = dest_key.rsplit('.', 1)[0] + ".rework.json"
            s3.put_object(Bucket=BUCKET, Key=meta_key, Body=json.dumps(meta, ensure_ascii=False).encode('utf-8'), ContentType='application/json')

            # Delete enriched artifacts for this pdf_id
            keys_to_delete = set()
            accounts = set()
            for r in pid_rows:
                k = r.get("__s3_key__")
                if k:
                    keys_to_delete.add(k)
                acct = str(r.get("Account Number", "")).strip()
                if acct:
                    accounts.add(acct)

            for k in keys_to_delete:
                try:
                    s3.delete_object(Bucket=BUCKET, Key=k)
                except Exception:
                    pass

            # Delete Pre-Entrata (Stage 6) files that match the reworked invoices
            # Match on business fields: Account Number + Bill Period Start + Bill Period End + Bill Date
            # This ensures we only delete the exact invoice, not unrelated ones with same account
            # Also clear DynamoDB status so counts stay consistent across all views
            if keys_to_delete:
                # Build set of (account, bill_period_start, bill_period_end, bill_date) tuples to match
                invoice_signatures = set()
                for r in pid_rows:
                    sig = (
                        str(r.get("Account Number", "")).strip(),
                        str(r.get("Bill Period Start", "")).strip(),
                        str(r.get("Bill Period End", "")).strip(),
                        str(r.get("Bill Date", "")).strip(),
                    )
                    invoice_signatures.add(sig)

                day_prefix = f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"
                resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=day_prefix)
                for obj in resp.get("Contents", []) or []:
                    s6_key = obj.get("Key", "")
                    if not s6_key.endswith(".jsonl"):
                        continue
                    try:
                        # Read Stage 6 file to check if it matches any reworked invoice
                        s6_content = _read_s3_text(BUCKET, s6_key)
                        lines = [l for l in s6_content.strip().split('\n') if l.strip()] if s6_content else []
                        if lines:
                            first_rec = json.loads(lines[0])
                            # IMPORTANT: Also verify source_input_key matches one of the deleted keys
                            # This prevents deleting Stage 6 files for TRUE DUPLICATES where user
                            # submitted one copy and is reworking the other copy
                            s6_source_key = first_rec.get("source_input_key", "") or first_rec.get("__s3_key__", "")
                            if s6_source_key not in keys_to_delete:
                                # This Stage 6 file came from a DIFFERENT source file (true duplicate)
                                # Don't delete it - the user only wants to rework one copy
                                continue
                            s6_sig = (
                                str(first_rec.get("Account Number", "")).strip(),
                                str(first_rec.get("Bill Period Start", "")).strip(),
                                str(first_rec.get("Bill Period End", "")).strip(),
                                str(first_rec.get("Bill Date", "")).strip(),
                            )
                            # Only delete if signature matches one of the reworked invoices
                            if s6_sig in invoice_signatures:
                                # Clear DynamoDB status for each line in the Stage 6 file
                                for line in lines:
                                    try:
                                        rec = json.loads(line)
                                        orig_key = rec.get("__s3_key__", "")
                                        row_idx = rec.get("__row_idx__", 0)
                                        if orig_key:
                                            line_id = f"{orig_key}#{row_idx}"
                                            ddb.delete_item(TableName=REVIEW_TABLE, Key={"pk": {"S": line_id}})
                                    except Exception:
                                        pass
                                # Delete the Stage 6 file
                                s3.delete_object(Bucket=BUCKET, Key=s6_key)
                    except Exception:
                        pass

            sent_count += 1

        # Invalidate caches
        invalidate_day_cache(y, m, d)
        _CACHE.pop(("day_status_counts", y, m, d), None)
        _CACHE.pop(("parse_dashboard",), None)

        return {"ok": True, "sent": sent_count, "errors": errors if errors else None}
    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/split_bill")
def api_split_bill(date: str = Form(...), pdf_id: str = Form(...), user: str = Depends(require_user)):
    """Split a multi-account PDF into separate bill files by account number.

    This takes an enriched JSONL file that contains multiple account numbers and splits it
    into separate files, one per account. Each new file gets a unique key based on
    original_key + account_number suffix.
    """
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)

    try:
        rows = load_day(y, m, d)

        # Find all rows belonging to this pdf_id
        target_key = None
        target_rows = []
        for r in rows:
            key = r.get("__s3_key__", "")
            if not key:
                continue
            pid = pdf_id_from_key(key)
            if pid == pdf_id:
                target_key = key
                target_rows.append(r)

        if not target_rows:
            return JSONResponse({"error": "No rows found for pdf_id"}, status_code=404)

        # Group rows by account number
        from collections import defaultdict
        rows_by_account: Dict[str, List[Dict]] = defaultdict(list)
        for r in target_rows:
            acct = str(r.get("Account Number", "")) or "(unknown)"
            rows_by_account[acct].append(r)

        if len(rows_by_account) < 2:
            return JSONResponse({"error": "Only one account found - nothing to split"}, status_code=400)

        # Create separate files for each account
        created_files = []
        base_key = target_key.rsplit('.', 1)[0]  # Remove .jsonl extension

        for acct, acct_rows in rows_by_account.items():
            # Create new key with account suffix
            safe_acct = "".join(ch if ch.isalnum() else "_" for ch in acct)
            new_key = f"{base_key}_ACCT_{safe_acct}.jsonl"

            # Prepare rows for new file (remove internal fields)
            output_lines = []
            for r in acct_rows:
                # Create clean copy without internal fields
                clean_row = {k: v for k, v in r.items() if not k.startswith("__")}
                output_lines.append(json.dumps(clean_row, ensure_ascii=False))

            # Write new file
            content = "\n".join(output_lines)
            s3.put_object(
                Bucket=BUCKET,
                Key=new_key,
                Body=content.encode('utf-8'),
                ContentType='application/json'
            )
            created_files.append({"key": new_key, "account": acct, "rows": len(acct_rows)})

        # Delete original combined file
        s3.delete_object(Bucket=BUCKET, Key=target_key)

        # Also delete any header drafts for the old pdf_id (they're no longer valid)
        try:
            for suffix in ["admin", "__final__"]:
                pk = f"draft#{pdf_id}#__header__#{suffix}"
                ddb.delete_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
        except Exception:
            pass  # Ignore errors cleaning up drafts

        # Invalidate cache so the page shows the new split files
        invalidate_day_cache(y, m, d)

        return {
            "ok": True,
            "original_file": target_key,
            "split_into": created_files,
            "message": f"Split into {len(created_files)} separate bills by account"
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/rework")
def api_rework(
    date: str = Form(...),
    pdf_id: str = Form(...),
    notes: str = Form(""),
    bill_from: str = Form(""),
    account_number: str = Form(""),
    bill_date: str = Form(""),
    pdf_key: str = Form(""),
    expected_lines: str | None = Form(None),
    user: str = Depends(require_user)
):
    """Send a bill back to a REWORK pipeline and delete current parsed artifacts for this pdf_id on the given day.
    Steps:
      - Identify rows for (date,pdf_id)
      - Derive source PDF s3 key from PDF_LINK
      - Copy PDF to REWORK_PREFIX with timestamped name
      - Write a sidecar rework.json with notes and metadata
      - Delete enriched artifacts for the pdf_id and related Pre-Entrata files for same account on that day
    """
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)
    try:
        rows = load_day(y, m, d)
        # filter rows belonging to this pdf_id
        hit = [r for r in rows if r.get("__s3_key__") and pdf_id_from_key(r.get("__s3_key__")) == pdf_id]
        if not hit:
            # Fallback: try to locate by Account Number (and Bill Date if provided)
            acc = (account_number or str(first.get("Account Number", "") if False else "")).strip()  # safeguard; "first" not defined yet
            if not acc:
                # derive from header draft if present
                try:
                    hd = get_draft(pdf_id, "__header__", user) or {"fields": {}}
                    acc = str(hd.get("fields", {}).get("Account Number", "")).strip()
                except Exception:
                    acc = ""
            bd = str(bill_date or "").strip()
            if acc:
                cand = [r for r in rows if str(r.get("Account Number", "")).strip() == acc]
                if bd:
                    exact = [r for r in cand if str(r.get("Bill Date", "")).strip() == bd]
                    hit = exact or cand
                else:
                    hit = cand
        if not hit:
            return JSONResponse({"error": "no rows for pdf_id"}, status_code=404)
        first = hit[0]
        # Allow explicit key from client to avoid parsing fragile links
        if pdf_key and isinstance(pdf_key, str):
            key_in = pdf_key.strip()
            try:
                if key_in.startswith(('http://','https://','s3://')):
                    parsed = _parse_s3_from_url(key_in)
                    if parsed:
                        src_bucket, src_key = parsed
                    else:
                        from urllib.parse import urlparse as _up
                        _p = _up(key_in)
                        src_bucket, src_key = BUCKET, (_p.path or '').lstrip('/')
                else:
                    key2 = key_in.lstrip('/')
                    if key2.startswith(f"{BUCKET}/"):
                        key2 = key2[len(BUCKET)+1:]
                    src_bucket, src_key = BUCKET, key2
                # proceed to copy
                pdf_url = key_in
                parsed = (src_bucket, src_key)
            except Exception:
                parsed = None
        else:
            parsed = None
        pdf_url = str(first.get("PDF_LINK", "") or first.get("source_input_key", "") or (pdf_key or "")) if not parsed else pdf_url
        if not pdf_url:
            # Try to infer PDF key as a fallback
            cand = _infer_pdf_key_for_doc(y, m, d, hit, pdf_id)
            if cand:
                src_bucket, src_key = BUCKET, cand
                pdf_url = cand
            else:
                return JSONResponse({"error": "missing PDF link"}, status_code=400)
        # Step 1: expand legacy short links
        pdf_url = _maybe_expand_short(pdf_url)
        # Step 2: try parse original and resolved final (unless parsed from pdf_key already)
        parsed_orig = parsed or _parse_s3_from_url(pdf_url)
        final_url = _resolve_final_url(pdf_url)
        parsed_final = _parse_s3_from_url(final_url)
        # Step 3: if looks like lambda-url and still not parsed, aggressively resolve via GET
        if not parsed_final and pdf_url:
            try:
                host = (urlparse(pdf_url).netloc or "")
                if ".lambda-url." in host:
                    r = requests.get(pdf_url, allow_redirects=True, timeout=12, stream=False)
                    final_url = r.url or final_url or pdf_url
                    parsed_final = _parse_s3_from_url(final_url)
            except Exception:
                pass
        parsed = parsed_final or parsed_orig
        # Step 4: last-ditch explicit virtual-hosted parse
        if not parsed and (final_url or pdf_url):
            cand = final_url or pdf_url
            try:
                p = urlparse(cand)
                host = p.netloc or ""; path = (p.path or "").lstrip('/')
                if ".s3.amazonaws.com" in host and path:
                    b = host.split('.s3.amazonaws.com', 1)[0]
                    if b:
                        parsed = (b, unquote(path))
            except Exception:
                pass
        # Step 5: accept bare key
        if not parsed and pdf_url and ('/' in pdf_url) and not pdf_url.startswith(('http://','https://','s3://')):
            parsed = (BUCKET, pdf_url.lstrip('/'))
        if not parsed:
            # Fallback: try to infer the PDF key like /pdf does
            try:
                key_guess = _infer_pdf_key_for_doc(y, m, d, hit, pdf_id)
                if not key_guess:
                    # global scan by pdf_id across known prefixes
                    cands = []
                    for pfx in (REWORK_PREFIX, INPUT_PREFIX, PARSED_INPUTS_PREFIX, HIST_ARCHIVE_PREFIX):
                        pag = s3.get_paginator("list_objects_v2")
                        for page in pag.paginate(Bucket=BUCKET, Prefix=pfx):
                            for obj in page.get("Contents", []) or []:
                                k = obj.get("Key", ""); base = os.path.basename(k).lower()
                                if base.endswith('.pdf') and pdf_id.lower() in base:
                                    cands.append((k, obj.get("LastModified")))
                    if cands:
                        cands.sort(key=lambda t: (t[1] or 0), reverse=True)
                        key_guess = cands[0][0]
                if key_guess:
                    src_bucket, src_key = BUCKET, key_guess.lstrip('/')
                    parsed = (src_bucket, src_key)
            except Exception:
                parsed = None
        if not parsed:
            print(f"/api/rework parse failed: pdf_url={pdf_url} final_url={final_url}")
            return JSONResponse({"error": "cannot parse s3 from PDF link"}, status_code=400)
        src_bucket, src_key = parsed
        # copy to rework prefix
        ts = dt.datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')
        base = os.path.basename(src_key).replace("\\", "_").replace("/", "_")
        dest_key = f"{REWORK_PREFIX}yyyy={y}/mm={m}/dd={d}/{ts}_{pdf_id}_{base}"
        s3.copy_object(Bucket=BUCKET, CopySource={"Bucket": src_bucket, "Key": src_key}, Key=dest_key)
        # write sidecar json with notes (and optional header context like Bill From)
        # Attempt to read header draft so we can include 'Bill From' back to the parser
        header_pid = pdf_id  # header draft is keyed by pdf_id + '__header__'
        header_draft = get_draft(header_pid, "__header__", user) or {"fields": {}}
        bill_from_val = (str(bill_from).strip() if bill_from is not None else "")
        if not bill_from_val:
            bill_from_val = str(header_draft.get("fields", {}).get("Bill From", "")).strip()
        if not bill_from_val:
            # fallback to source row fields if draft not present
            r0 = first
            bill_from_val = (
                str(r0.get("Bill From", ""))
                or str(r0.get("Bill From Name", ""))
                or str(r0.get("Bill From Name First Line", ""))
                or str(r0.get("Vendor Name", ""))
                or str(r0.get("EnrichedVendorName", ""))
            )
        # Normalize expected_lines to int if provided
        exp_lines_val = None
        try:
            if expected_lines is not None and str(expected_lines).strip():
                exp_lines_val = int(str(expected_lines).strip())
        except Exception:
            exp_lines_val = None

        meta = {
            "requested_by": user,
            "requested_utc": dt.datetime.utcnow().isoformat(),
            "date": date,
            "pdf_id": pdf_id,
            "src_bucket": src_bucket,
            "src_key": src_key,
            "dest_bucket": BUCKET,
            "dest_key": dest_key,
            "notes": notes or "",
            "Bill From": bill_from_val,
        }
        if exp_lines_val is not None:
            # include multiple synonymous keys for downstream compatibility
            meta["expected_line_count"] = exp_lines_val
            meta["expectedLines"] = exp_lines_val
            meta["expected_lines"] = exp_lines_val
            meta["line_count"] = exp_lines_val
            meta["min_lines"] = exp_lines_val
        meta_key = dest_key.rsplit('.', 1)[0] + ".rework.json"
        s3.put_object(Bucket=BUCKET, Key=meta_key, Body=json.dumps(meta, ensure_ascii=False).encode('utf-8'), ContentType='application/json')

        # delete enriched artifacts for this pdf_id (reuse logic from api_delete_parsed)
        # find all enriched jsonl objects tied to this pdf_id for the day
        keys = set()
        accounts = set()
        for r in hit:
            k = r.get("__s3_key__")
            if k:
                keys.add(k)
            acct = str(r.get("Account Number", "")).strip()
            if acct:
                accounts.add(acct)
        deleted = 0
        for k in keys:
            try:
                s3.delete_object(Bucket=BUCKET, Key=k); deleted += 1
            except Exception:
                pass
        # Also delete Pre-Entrata (Stage 6) files that match the reworked invoices
        # Match on business fields: Account Number + Bill Period Start + Bill Period End + Bill Date
        # This ensures we only delete the exact invoice, not unrelated ones with same account
        # Also clear DynamoDB status so counts stay consistent across all views
        if keys:
            # Build set of (account, bill_period_start, bill_period_end, bill_date) tuples to match
            invoice_signatures = set()
            for r in hit:
                sig = (
                    str(r.get("Account Number", "")).strip(),
                    str(r.get("Bill Period Start", "")).strip(),
                    str(r.get("Bill Period End", "")).strip(),
                    str(r.get("Bill Date", "")).strip(),
                )
                invoice_signatures.add(sig)

            day_prefix = f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"
            resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=day_prefix)
            for obj in resp.get("Contents", []) or []:
                s6_key = obj.get("Key", "")
                if not s6_key.endswith(".jsonl"):
                    continue
                try:
                    # Read Stage 6 file to check if it matches any reworked invoice
                    s6_content = _read_s3_text(BUCKET, s6_key)
                    lines = [l for l in s6_content.strip().split('\n') if l.strip()] if s6_content else []
                    if lines:
                        first_rec = json.loads(lines[0])
                        # IMPORTANT: Also verify source_input_key matches one of the deleted keys
                        # This prevents deleting Stage 6 files for TRUE DUPLICATES where user
                        # submitted one copy and is reworking the other copy
                        s6_source_key = first_rec.get("source_input_key", "") or first_rec.get("__s3_key__", "")
                        if s6_source_key not in keys:
                            # This Stage 6 file came from a DIFFERENT source file (true duplicate)
                            # Don't delete it - the user only wants to rework one copy
                            continue
                        s6_sig = (
                            str(first_rec.get("Account Number", "")).strip(),
                            str(first_rec.get("Bill Period Start", "")).strip(),
                            str(first_rec.get("Bill Period End", "")).strip(),
                            str(first_rec.get("Bill Date", "")).strip(),
                        )
                        # Only delete if signature matches one of the reworked invoices
                        if s6_sig in invoice_signatures:
                            # Clear DynamoDB status for each line in the Stage 6 file
                            for line in lines:
                                try:
                                    rec = json.loads(line)
                                    orig_key = rec.get("__s3_key__", "")
                                    row_idx = rec.get("__row_idx__", 0)
                                    if orig_key:
                                        line_id = f"{orig_key}#{row_idx}"
                                        ddb.delete_item(TableName=REVIEW_TABLE, Key={"pk": {"S": line_id}})
                                except Exception:
                                    pass
                            # Delete the Stage 6 file
                            s3.delete_object(Bucket=BUCKET, Key=s6_key)
                            deleted += 1
                except Exception:
                    pass
        # Invalidate cache for this day so UI reflects deletions immediately
        try:
            invalidate_day_cache(y, m, d)
        except Exception:
            pass

        return {"ok": True, "copied_key": dest_key, "meta_key": meta_key, "deleted": deleted}
    except Exception as e:
        import traceback
        return JSONResponse({"error": _sanitize_error(e, "API request")}, status_code=500)
def get_status_map(ids: List[str]) -> Dict[str, Dict[str, str]]:
    """Returns a dict mapping id -> {"status": str, "submitted_at": str}"""
    if not ids:
        return {}
    # DynamoDB batch_get in chunks of 100
    out: Dict[str, Dict[str, str]] = {}
    it = iter(ids)
    while True:
        chunk = list(islice(it, 100))
        if not chunk:
            break
        keys = [{"pk": {"S": i}} for i in chunk]
        resp = ddb.batch_get_item(RequestItems={REVIEW_TABLE: {"Keys": keys}})
        for item in resp.get("Responses", {}).get(REVIEW_TABLE, []):
            pk = item.get("pk", {}).get("S")
            status = item.get("status", {}).get("S", "")
            submitted_at = item.get("submitted_at", {}).get("S", "")
            if pk:
                out[pk] = {"status": status, "submitted_at": submitted_at}
    return out


def day_status_counts(y: str, m: str, d: str) -> Dict[str, int]:
    """Compute per-day status PER INVOICE GROUP (vendor, account, pdf_id) for the dashboard.

    Rules (aligned with invoices list logic except artifact check):
    - Exclude lines marked 'Deleted'.
    - COMPLETE when all non-deleted lines are Submitted (no artifact check here).
    - PARTIAL if some submitted; REVIEW if none submitted.
    """
    # Cache day_status_counts to avoid expensive DynamoDB queries on every parse page load
    # Use longer TTL for past days since they change less frequently
    cache_key = ("day_status_counts", y, m, d)
    now = time.time()
    ent = _CACHE.get(cache_key)
    ttl = _get_cache_ttl(y, m, d)
    if ent and (now - ent.get("ts", 0) < ttl):
        return ent.get("data", {})

    rows = load_day(y, m, d)

    # Collect all unique pdf_ids first for batch fetching
    unique_pdf_ids = set()
    for r in rows:
        key = r.get("__s3_key__", "")
        if key:
            pid = pdf_id_from_key(key)
            if pid:
                unique_pdf_ids.add(pid)

    # Batch fetch all header drafts in parallel (uses __final__ user for shared headers)
    header_by_pdf: Dict[str, Dict[str, Any]] = {}
    if unique_pdf_ids:
        # Fetch headers in parallel using existing batch function with __final__ user
        batch_drafts = get_header_drafts_batch(list(unique_pdf_ids), "__final__")
        for pid, draft in batch_drafts.items():
            if draft and isinstance(draft.get("fields"), dict):
                header_by_pdf[pid] = draft["fields"]
            else:
                header_by_pdf[pid] = {}

    # Build mapping: (vendor, account, pdf_id) -> list of row ids
    groups: Dict[tuple, List[str]] = {}
    all_ids: List[str] = []
    for r in rows:
        key = r.get("__s3_key__")
        if not key:
            continue
        pid = pdf_id_from_key(key)
        rid = str(r.get("__id__"))
        if not (rid and pid):
            continue

        hdr = header_by_pdf.get(pid, {})
        # Use row's Account Number directly (NOT header override) to preserve multi-account grouping
        # Use Line Item Account Number as fallback when Account Number is blank (for subtotals/taxes)
        account_no = str(r.get("Account Number", "") or r.get("Line Item Account Number", "") or "") or "(unknown)"
        vendor = (
            str(hdr.get("EnrichedVendorName", ""))
            or str(r.get("EnrichedVendorName", ""))
            or str(r.get("Vendor Name", ""))
            or str(r.get("Vendor", ""))
            or str(r.get("Utility Type", ""))
            or "(unknown)"
        )

        group_key = (vendor, account_no, pid)
        groups.setdefault(group_key, []).append(rid)
        all_ids.append(rid)

    stmap = get_status_map(all_ids)

    review = partial = complete = 0
    for group_key, ids_all in groups.items():
        ids_active = [i for i in ids_all if stmap.get(i, {}).get("status") != "Deleted"]
        if not ids_active:
            # No active lines; treat as REVIEW bucket until files are fully removed
            review += 1
            continue
        submitted = sum(1 for i in ids_active if stmap.get(i, {}).get("status") == "Submitted")
        if submitted == 0:
            review += 1
        elif submitted == len(ids_active):
            complete += 1
        else:
            partial += 1

    counts = {"REVIEW": review, "PARTIAL": partial, "COMPLETE": complete}
    _CACHE[cache_key] = {"ts": now, "data": counts}
    return counts


@app.get("/review", response_class=HTMLResponse)
def review_view(request: Request, date: str, pdf_id: str, user: str = Depends(require_user)):
    try:
        y, m, d = date.split("-")
    except ValueError:
        return RedirectResponse("/", status_code=302)
    all_rows = load_day(y, m, d)
    # Filter strictly by pdf_id (unique hash from __s3_key__), not by invoice number
    rows = [r for r in all_rows if r.get("__s3_key__") and pdf_id_from_key(r.get("__s3_key__")) == pdf_id]
    if not rows:
        # Bill may have moved past Stage 4  search other stages for same date partition
        for _alt_prefix in (STAGE6_PREFIX, POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX, HIST_ARCHIVE_PREFIX):
            _alt_pfx = f"{_alt_prefix}yyyy={y}/mm={m}/dd={d}/"
            try:
                _alt_keys = []
                for _pg in s3.get_paginator("list_objects_v2").paginate(Bucket=BUCKET, Prefix=_alt_pfx):
                    for _obj in _pg.get("Contents", []):
                        if _obj["Key"].lower().endswith(".jsonl"):
                            _alt_keys.append(_obj["Key"])
                for _ak in _alt_keys:
                    if pdf_id_from_key(_ak) == pdf_id:
                        rows = _fetch_s3_file(_ak)
                        break
                if rows:
                    break
            except Exception:
                continue
    if not rows:
        return templates.TemplateResponse("error.html", {"request": request, "message": "No rows found for that document."}, status_code=404)
    # build header defaults from first row (invoice-level)
    header_fields = [
        "EnrichedPropertyName","EnrichedPropertyID",
        "EnrichedVendorName","EnrichedVendorID","Account Number",
        "Bill Period Start","Bill Period End","Bill Date","Due Date",
        "Service Address","Service City","Service State","Service Zipcode",
        "Special Instructions",
        "Bill From",
    ]
    header: Dict[str, Any] = {k: (rows[0].get(k, "") if rows else "") for k in header_fields}

    # Merge saved header draft (user edits) over JSONL defaults for initial render
    # This prevents flash of empty fields while JS loads the draft
    header_draft = get_draft(pdf_id, "__header__", user)
    if header_draft and header_draft.get("fields"):
        draft_fields = header_draft["fields"]
        if isinstance(draft_fields, str):
            try:
                draft_fields = json.loads(draft_fields)
            except Exception:
                draft_fields = {}
        if isinstance(draft_fields, dict):
            for k, v in draft_fields.items():
                if k in header_fields and v:  # Only overwrite if draft has non-empty value
                    header[k] = v

    # Backfill 'Bill From' if missing using common source fields (vendor/billing name lines)
    if not (header.get("Bill From") or ""):
        r0 = rows[0] if rows else {}
        bf = (
            str(r0.get("Bill From", ""))
            or str(r0.get("Bill From Name", ""))
            or str(r0.get("Bill From Name First Line", ""))
            or str(r0.get("Vendor Name", ""))
            or str(r0.get("EnrichedVendorName", ""))
        )
        header["Bill From"] = bf
    # Build a simple vendor name -> id map from all rows for the day
    vend_map: Dict[str, str] = {}
    for r in all_rows:
        vn = str(r.get("EnrichedVendorName", "") or r.get("Vendor Name", "") or r.get("Vendor", "")).strip()
        vid = str(r.get("EnrichedVendorID", "")).strip()
        if vn:
            key = vn.upper()
            if key not in vend_map or not vend_map[key]:
                vend_map[key] = vid
    # Do not override EnrichedVendorName from Bill From in the UI. Let enrichment derive the canonical vendor.
    # We still backfill EnrichedVendorID when a vendor name is present.
    # If vendor ID is missing but we have a vendor name (from data or edits), try to backfill ID
    if (not str(header.get("EnrichedVendorID", "")).strip()) and str(header.get("EnrichedVendorName", "")).strip():
        vkey = str(header.get("EnrichedVendorName", "")).strip().upper()
        if vkey in vend_map and vend_map[vkey]:
            header["EnrichedVendorID"] = vend_map[vkey]
    header_account = header.get("Account Number", "")

    # We already filtered by the exact pdf_id; no further narrowing needed

    pdf_link_any = _infer_pdf_key_for_doc(y, m, d, rows, pdf_id)
    # build line models with sequential numbering independent of __row_idx__
    lines = []
    for i, r in enumerate(rows, start=1):
        key = r.get("__s3_key__"); idx = r.get("__row_idx__", 0)
        pid = pdf_id_from_key(key) if key else ""
        lid = line_id_from(key or "", idx)
        # Page metadata for chunk-to-line mapping (from large file processor or standard parser)
        source_page_start = r.get("source_page_start", 0)
        source_page_end = r.get("source_page_end", 0)
        source_chunk = r.get("source_chunk", 0)
        lines.append({
            "pdf_id": pid,
            "line_id": lid,
            "line_number": i,
            "orig_id": r.get("__id__"),
            "pdf_link": (r.get("PDF_LINK", "") or pdf_link_any),
            "source_page_start": source_page_start,
            "source_page_end": source_page_end,
            "source_chunk": source_chunk,
            "original": {
                "EnrichedGLAccountNumber": r.get("EnrichedGLAccountNumber",""),
                "EnrichedGLAccountName": r.get("EnrichedGLAccountName",""),
                "GL DESC_NEW": r.get("GL DESC_NEW",""),
                "ENRICHED CONSUMPTION": r.get("ENRICHED CONSUMPTION",""),
                "ENRICHED UOM": r.get("ENRICHED UOM",""),
                "Meter Number": r.get("Meter Number",""),
                "Meter Size": r.get("Meter Size",""),
                "House Or Vacant": r.get("House Or Vacant",""),
                "Utility Type": r.get("Utility Type",""),
                "Line Item Description": r.get("Line Item Description",""),
                "Line Item Charge": r.get("Line Item Charge",""),
                "Consumption Amount": r.get("Consumption Amount",""),
                "Unit of Measure": r.get("Unit of Measure",""),
                "Previous Reading": r.get("Previous Reading",""),
                "Previous Reading Date": r.get("Previous Reading Date",""),
                "Current Reading": r.get("Current Reading",""),
                "Current Reading Date": r.get("Current Reading Date",""),
                "Rate": r.get("Rate",""),
            }
        })
    return templates.TemplateResponse(
        "review.html",
        {
            "request": request,
            "date": date,
            "invoice": str(rows[0].get("Invoice Number", "")) or "(unknown)",
            "account": header_account,
            "header": header,
            "lines": lines,
            "user": user,
        },
    )


# -------- APIs (JSON) --------
def _read_recent_exports(prefix: str, max_parts: int = 5) -> list[dict]:
    """Read up to N most-recent dt=YYYY-MM-DD/data.json.gz files under prefix."""
    _k = ("recent_exports", prefix, max_parts)
    now = time.time()
    ent = _CACHE.get(_k)
    if ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        return ent.get("data", [])
    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix)
    parts = []
    for obj in resp.get("Contents", []):
        k = obj["Key"]
        if "/dt=" in k and k.endswith("data.json.gz"):
            try:
                dt_str = k.split("/dt=")[-1].split("/")[0]
                parts.append((dt_str, k))
            except Exception:
                pass
    if not parts:
        return []
    parts.sort(key=lambda x: x[0], reverse=True)
    items: list[dict] = []
    for _, key in parts[:max_parts]:
        obj = s3.get_object(Bucket=BUCKET, Key=key)
        gz = obj["Body"].read()
        data = gzip.decompress(gz)
        text = data.decode("utf-8", errors="ignore")
        text_stripped = text.lstrip()
        if text_stripped.startswith("["):
            import json as _json
            try:
                arr = _json.loads(text)
                if isinstance(arr, list):
                    items.extend([x for x in arr if isinstance(x, dict)])
            except Exception:
                pass
        else:
            for line in text.splitlines():
                line = line.strip()
                if not line:
                    continue
                try:
                    import json as _json
                    rec = _json.loads(line)
                    if isinstance(rec, dict):
                        items.append(rec)
                except Exception:
                    continue
    _CACHE[_k] = {"ts": now, "data": items}
    return items

def _get_first_key(d: dict, candidates: list[str]) -> str:
    for k in candidates:
        if k in d:
            return k
    # try case-insensitive
    lower = {k.lower(): k for k in d.keys()}
    for k in candidates:
        if k.lower() in lower:
            return lower[k.lower()]
    return ""

# -------- URL/PDF helpers --------
def _infer_pdf_key_for_doc(y: str, m: str, d: str, rows: List[Dict[str, Any]], pdf_id: str) -> str:
    """Best-effort inference of original PDF S3 key for a document when rows lack PDF_LINK.
    Strategy:
      1) Use source_input_key if present (direct S3 key, doesn't expire)
      2) Use PDF_LINK if it's an S3 URL (not a Lambda short link)
      3) Derive <timestamp>_<orig_base> from enriched key basename and search:
         - INPUT_PREFIX (flat)
         - REWORK day folder
         - Global REWORK (latest match)
         - Global INPUT (latest match)
         - Variant without trailing " (1)"
         - Any PDF in day REWORK that includes pdf_id
    Returns a key relative to the bucket, or empty string.
    """
    # 1) Check source_input_key FIRST - it's a direct S3 key that doesn't expire
    for r in rows:
        src_key = str(r.get("source_input_key", "")).strip()
        if src_key:
            # Accept bare S3 keys (relative paths with /)
            if "/" in src_key and not src_key.startswith(("http://", "https://", "s3://")):
                return src_key
            # Also accept full S3 URLs
            lv = src_key.lower()
            if lv.startswith("s3://") or ".s3.amazonaws.com" in lv or ".s3." in lv:
                return src_key

    # 2) Check PDF_LINK if it's an S3 URL (skip Lambda short links - they expire)
    for r in rows:
        val = str(r.get("PDF_LINK", "")).strip()
        if val:
            lv = val.lower()
            if lv.startswith("s3://") or ".s3.amazonaws.com" in lv or ".s3." in lv:
                return val
    try:
        # 3) derive from enriched key basename
        ek = str(rows[0].get("__s3_key__", "")) if rows else ""
        tail = os.path.basename(ek)
        base_no_ext = tail.rsplit('.', 1)[0]
        parts = base_no_ext.split('_', 1)
        ts_part = parts[0] if parts else ""
        orig_base = parts[1] if len(parts) > 1 else ""
        # Try INPUT flat
        if ts_part and orig_base:
            cand1 = f"{INPUT_PREFIX}{ts_part}_{orig_base}.pdf"
            try:
                s3.head_object(Bucket=BUCKET, Key=cand1)
                return cand1
            except Exception:
                pass
        # If we have a timestamp part, search known prefixes for any PDF starting with that timestamp
        if ts_part:
            try:
                best = (None, None)
                for pfx in (INPUT_PREFIX, PARSED_INPUTS_PREFIX, REWORK_PREFIX):
                    pag = s3.get_paginator("list_objects_v2")
                    for page in pag.paginate(Bucket=BUCKET, Prefix=pfx):
                        for obj in page.get("Contents", []) or []:
                            k = obj.get("Key", ""); lm = obj.get("LastModified")
                            bn = os.path.basename(k)
                            if bn.lower().endswith('.pdf') and bn.startswith(f"{ts_part}_"):
                                # Prefer exact orig_base if present, else pick freshest
                                if orig_base and bn.lower().startswith(f"{ts_part}_{orig_base}".lower()):
                                    return k
                                if not best[1] or (lm and lm > best[1]):
                                    best = (k, lm)
                if best[0]:
                    return best[0]
            except Exception:
                pass
        # REWORK day
        if orig_base:
            day_prefix = f"{REWORK_PREFIX}yyyy={y}/mm={m}/dd={d}/"
            resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=day_prefix)
            for obj in resp.get("Contents", []) or []:
                k = obj.get("Key", "")
                if k.lower().endswith(("_" + orig_base + ".pdf").lower()):
                    return k
        # Global REWORK (exact filename match)
        if orig_base:
            best = (None, None)
            rp = s3.get_paginator("list_objects_v2")
            for page in rp.paginate(Bucket=BUCKET, Prefix=REWORK_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    if k.lower().endswith(("_" + orig_base + ".pdf").lower()):
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
        # Global PARSED_INPUTS (exact filename match)
        if orig_base:
            best = (None, None)
            pi = s3.get_paginator("list_objects_v2")
            for page in pi.paginate(Bucket=BUCKET, Prefix=PARSED_INPUTS_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    if k.lower().endswith(("_" + orig_base + ".pdf").lower()):
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
            # Fuzzy: any PARSED_INPUTS PDF containing orig_base in filename
            best = (None, None)
            for page in pi.paginate(Bucket=BUCKET, Prefix=PARSED_INPUTS_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    base = os.path.basename(k).lower()
                    if base.endswith('.pdf') and orig_base.lower() in base:
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
            # Fuzzy: any REWORK PDF containing orig_base in filename
            best = (None, None)
            for page in rp.paginate(Bucket=BUCKET, Prefix=REWORK_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    base = os.path.basename(k).lower()
                    if base.endswith('.pdf') and orig_base.lower() in base:
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
        # Global INPUT (exact filename match)
        if orig_base:
            best = (None, None)
            ip = s3.get_paginator("list_objects_v2")
            for page in ip.paginate(Bucket=BUCKET, Prefix=INPUT_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    if k.lower().endswith(("_" + orig_base + ".pdf").lower()):
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
            # Fuzzy: any INPUT PDF containing orig_base in filename
            best = (None, None)
            for page in ip.paginate(Bucket=BUCKET, Prefix=INPUT_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    base = os.path.basename(k).lower()
                    if base.endswith('.pdf') and orig_base.lower() in base:
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
        # Variant without (1)
        if orig_base and orig_base.endswith(" (1)"):
            ob = orig_base[:-4]
            best = (None, None)
            rp = s3.get_paginator("list_objects_v2")
            for page in rp.paginate(Bucket=BUCKET, Prefix=REWORK_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    if k.lower().endswith(("_" + ob + ".pdf").lower()):
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
            best = (None, None)
            ip = s3.get_paginator("list_objects_v2")
            for page in ip.paginate(Bucket=BUCKET, Prefix=INPUT_PREFIX):
                for obj in page.get("Contents", []) or []:
                    k = obj.get("Key", ""); lm = obj.get("LastModified")
                    if k.lower().endswith(("_" + ob + ".pdf").lower()):
                        if not best[1] or (lm and lm > best[1]):
                            best = (k, lm)
            if best[0]:
                return best[0]
        # Day REWORK any pdf containing pdf_id
        day_prefix = f"{REWORK_PREFIX}yyyy={y}/mm={m}/dd={d}/"
        resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=day_prefix)
        pdf_keys = []
        for obj in resp.get("Contents", []) or []:
            k = obj.get("Key", "")
            kl = k.lower()
            if kl.endswith('.pdf') and (pdf_id.lower() in kl or True):
                pdf_keys.append(k)
        # Prefer pdfs containing pdf_id, else if exactly one PDF that day, use it
        for k in pdf_keys:
            if pdf_id.lower() in k.lower():
                return k
        if len(pdf_keys) == 1:
            return pdf_keys[0]
        # Global scan by pdf_id across REWORK/INPUT/PARSED_INPUTS as a last resort
        try:
            candidates = []
            for pfx in (REWORK_PREFIX, INPUT_PREFIX, PARSED_INPUTS_PREFIX):
                pag = s3.get_paginator("list_objects_v2")
                for page in pag.paginate(Bucket=BUCKET, Prefix=pfx):
                    for obj in page.get("Contents", []) or []:
                        k = obj.get("Key", ""); base = os.path.basename(k).lower()
                        if base.endswith('.pdf') and pdf_id.lower() in base:
                            candidates.append((k, obj.get("LastModified")))
            if candidates:
                candidates.sort(key=lambda t: (t[1] or 0), reverse=True)
                return candidates[0][0]
        except Exception:
            pass
    except Exception:
        pass
    return ""
def _resolve_final_url(u: str) -> str:
    """Follow redirects to resolve any short URL to a final URL.
    Falls back to the input on failure. Uses HEAD first, then GET.
    """
    if not u:
        return ""
    try:
        try:
            r = requests.head(u, allow_redirects=True, timeout=6)
            if r.url:
                return r.url
        except Exception:
            pass
        r2 = requests.get(u, allow_redirects=True, timeout=8, stream=True)
        return r2.url or u
    except Exception:
        return u

def _parse_s3_from_url(u: str):
    """Parse various S3 URL styles to (bucket, key).
    Supports:
      - s3://bucket/key
      - https://s3.amazonaws.com/bucket/key
      - https://s3.<region>.amazonaws.com/bucket/key
      - https://bucket.s3.amazonaws.com/key
      - https://bucket.s3.<region>.amazonaws.com/key
      - https://bucket.s3-<region>.amazonaws.com/key
    Returns tuple or None if unrecognized.
    """
    if not u:
        return None
    try:
        if u.startswith("s3://"):
            rest = u[5:]
            i = rest.find('/')
            if i > 0:
                return rest[:i], unquote(rest[i+1:])
            return None
        p = urlparse(u)
        host = p.netloc or ""
        path = (p.path or "").lstrip('/')
        if not host:
            return None
        # virtual-hosted-style
        # bucket.s3.amazonaws.com or bucket.s3.<region>.amazonaws.com or bucket.s3-<region>.amazonaws.com
        if host.endswith("amazonaws.com"):
            parts = host.split('.')
            if parts and parts[0] and not parts[0].startswith('s3'):
                # parts[0] is bucket
                bucket = parts[0]
                return bucket, unquote(path)
            # path-style: s3.amazonaws.com/bucket/key or s3.<region>.amazonaws.com/bucket/key
            segs = path.split('/', 1)
            if len(segs) == 2 and segs[0] and segs[1]:
                return segs[0], unquote(segs[1])
        # explicit virtual-hosted fallback: <bucket>.s3.amazonaws.com/<key>
        if ".s3.amazonaws.com" in host and path:
            try:
                bucket = host.split('.s3.amazonaws.com', 1)[0]
                if bucket:
                    return bucket, unquote(path)
            except Exception:
                pass
        # AWS Console object URL patterns
        # Examples:
        # https://s3.console.aws.amazon.com/s3/object/<bucket>?region=us-east-1&prefix=<key>
        # https://s3.console.aws.amazon.com/s3/buckets/<bucket>/object?prefix=<key>
        if host.startswith('s3.console.aws.amazon.com'):
            q = parse_qs(p.query or '')
            pref = (q.get('prefix') or q.get('key') or [''])[0]
            # try extract bucket from path segs
            segs = [s for s in path.split('/') if s]
            bucket = ''
            if 'object' in segs:
                try:
                    i = segs.index('object')
                    if i+1 < len(segs):
                        bucket = segs[i+1]
                except ValueError:
                    pass
            if not bucket and 'buckets' in segs:
                try:
                    i = segs.index('buckets')
                    if i+1 < len(segs):
                        bucket = segs[i+1]
                except ValueError:
                    pass
            if bucket and pref:
                return bucket, unquote(pref.lstrip('/'))
        # Generic query with bucket/key
        q = parse_qs(p.query or '')
        qb = (q.get('bucket') or q.get('Bucket') or [''])[0]
        qk = (q.get('key') or q.get('Key') or [''])[0]
        if qb and qk:
            return qb, unquote(qk.lstrip('/'))
        return None
    except Exception:
        return None

def _maybe_expand_short(u: str) -> str:
    """If URL is one of our legacy short links (Lambda Function URL), expand via DDB.
    Expected shape: https://<hash>.lambda-url.<region>.on.aws/<code>
    Returns the expanded original URL if found; otherwise returns input.
    """
    try:
        if not u:
            return u
        p = urlparse(u)
        host = p.netloc or ""
        path = (p.path or "/").strip("/")
        # Lambda Function URL host typically contains '.lambda-url.' and '.on.aws'
        if ".lambda-url." in host and host.endswith(".on.aws") and path and "/" not in path:
            code = path
            try:
                resp = ddb.get_item(TableName=SHORT_TABLE, Key={"code": {"S": code}})
                item = resp.get("Item")
                if item and "url" in item:
                    url = item["url"].get("S") or item["url"].get("s")
                    if url:
                        return url
            except Exception:
                return u
        return u
    except Exception:
        return u

@app.get("/api/catalogs")
def api_catalogs(user: str = Depends(require_user), response: Response = None):
    """Return full catalogs from S3 exports for properties, vendors, and GL accounts.
    Sources:
      - s3://{BUCKET}/{EXPORTS_ROOT}dim_property/dt=YYYY-MM-DD/data.json.gz
      - s3://{BUCKET}/{EXPORTS_ROOT}dim_vendor/dt=YYYY-MM-DD/data.json.gz
      - s3://{BUCKET}/{EXPORTS_ROOT}dim_gl_account/dt=YYYY-MM-DD/data.json.gz

    PERF: Caches processed result for 5 minutes to avoid repeated S3 reads and processing.
    """
    # Check endpoint-level cache first (processed result)
    cache_key = "api_catalogs_processed"
    now = time.time()
    ent = _CACHE.get(cache_key)
    if ent and (now - ent.get("ts", 0) < CACHE_TTL_SECONDS):
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        return ent.get("data", {})

    try:
        # PERF: Only load 1 most recent file per catalog (was 5)
        prop_items = _read_recent_exports(f"{EXPORTS_ROOT}dim_property/", max_parts=1)
        gl_items = _read_recent_exports(f"{EXPORTS_ROOT}dim_gl_account/", max_parts=1)

        props = []
        vends = []
        gls = []

        for r in prop_items:
            k_name = _get_first_key(r, ["PropertyName", "property_name", "Name", "name"])
            k_id = _get_first_key(r, ["PropertyID", "property_id", "Id", "id"])
            name = str(r.get(k_name, "")).strip()
            pid = str(r.get(k_id, "")).strip()
            if name:
                props.append({"name": name, "id": pid})

        # Load vendors from new vendor cache (has vendorCode) with fallback to dim_vendor
        try:
            print("[api_catalogs] Loading vendors from api-vendor/vendors/latest.json...")
            vend_cache_obj = s3.get_object(Bucket="api-vendor", Key="vendors/latest.json")
            vend_cache_data = json.loads(vend_cache_obj["Body"].read().decode("utf-8"))
            vendor_list = vend_cache_data.get("vendors", [])
            print(f"[api_catalogs] Loaded {len(vendor_list)} vendors from cache")
            codes_count = 0
            for v in vendor_list:
                name = str(v.get("name", "")).strip()
                vid = str(v.get("vendorId", "")).strip()
                vcode = str(v.get("vendorCode", "")).strip()
                if vcode:
                    codes_count += 1
                if name:
                    vends.append({"name": name, "id": vid, "code": vcode})
            print(f"[api_catalogs] Vendors with code: {codes_count}/{len(vendor_list)}")
        except Exception as e:
            print(f"[api_catalogs] ERROR loading vendor cache: {e}, falling back to dim_vendor")
            vend_items = _read_recent_exports(f"{EXPORTS_ROOT}dim_vendor/", max_parts=1)
            for r in vend_items:
                k_name = _get_first_key(r, ["VendorName", "vendor_name", "Name", "name"])
                k_id = _get_first_key(r, ["VendorID", "vendor_id", "Id", "id"])
                name = str(r.get(k_name, "")).strip()
                vid = str(r.get(k_id, "")).strip()
                if name:
                    vends.append({"name": name, "id": vid, "code": ""})

        for r in gl_items:
            # Fuzzy detection for GL keys if straightforward keys missing
            # Prefer formatted account number if available
            k_num = _get_first_key(r, [
                "FORMATTED_ACCOUNT_NUMBER",
                "FormattedAccountNumber",
                "formatted_account_number",
                "Formatted Account Number",
                "GLAccountNumber",
                "gl_account_number",
                "ACCOUNT_NUMBER",
                "Number",
                "number",
            ])
            k_nm = _get_first_key(r, ["NAME", "GLAccountName", "gl_account_name", "Name", "name"]) 
            if not k_num:
                for k in r.keys():
                    kl = k.lower()
                    if ("formatted" in kl and "account" in kl and ("num" in kl or "number" in kl)) or (("gl" in kl or "g/l" in kl or "general ledger" in kl) and ("acct" in kl or "account" in kl) and ("num" in kl or "number" in kl or "code" in kl)):
                        k_num = k; break
            if not k_nm:
                for k in r.keys():
                    kl = k.lower()
                    if ("gl" in kl or "general ledger" in kl) and ("name" in kl or "descr" in kl or "description" in kl):
                        k_nm = k; break
            num = str(r.get(k_num, "")).strip() if k_num else ""
            nm = str(r.get(k_nm, "")).strip() if k_nm else ""
            if num:
                gls.append({"number": num, "name": nm})

        # unique & sorted
        def _uniq(items, key):
            seen = set(); out = []
            for it in items:
                k = it.get(key, "")
                if k not in seen:
                    out.append(it); seen.add(k)
            return out

        props = sorted(_uniq(props, "name"), key=lambda x: x["name"].upper())
        # Dedupe vendors by ID (not name) - multiple vendors can have same name (e.g. City of Tacoma)
        vends = sorted(_uniq(vends, "id"), key=lambda x: x["name"].upper())
        gls = sorted(_uniq(gls, "number"), key=lambda x: x["name"].upper()) 

        # utilities: return Title Case consistently
        _base_utils = [
            "WATER","SEWER","TRASH","ELECTRICITY","GAS","STORMWATER","RECYCLE","RECYCLING","INTERNET","CABLE","TELECOM","WASTEWATER","HOA"
        ]
        utils = sorted({u.title() for u in _base_utils})
        try:
            if response is not None:
                response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
        except Exception:
            pass
        result = {"properties": props, "vendors": vends, "gl_accounts": gls, "utilities": utils}
        # Cache the processed result
        _CACHE[cache_key] = {"ts": time.time(), "data": result}
        return result
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)
@app.get("/api/options")
def api_options(date: str, user: str = Depends(require_user), response: Response = None):
    """Return dropdown options for properties, vendors, GL accounts, and utilities for a given day.
    Positioned after app initialization so the route registers properly.
    """
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)
    rows = load_day(y, m, d)
    prop = {}
    vend = {}
    gl = {}
    # Start with a Title Case base set
    utils = set(u.title() for u in [
        "WATER","SEWER","TRASH","ELECTRICITY","GAS","STORMWATER","RECYCLE","RECYCLING","INTERNET","CABLE","TELECOM","WASTEWATER","HOA"
    ])
    for r in rows:
        pn = str(r.get("EnrichedPropertyName", "") or r.get("Property Name", "")).strip()
        pid = str(r.get("EnrichedPropertyID", "")).strip()
        if pn:
            if pn not in prop:
                prop[pn] = pid
            elif not prop[pn] and pid:
                prop[pn] = pid

        vn = str(r.get("EnrichedVendorName", "") or r.get("Vendor Name", "") or r.get("Vendor", "")).strip()
        vid = str(r.get("EnrichedVendorID", "")).strip()
        if vn:
            if vn not in vend:
                vend[vn] = vid
            elif not vend[vn] and vid:
                vend[vn] = vid

        gnum = str(r.get("EnrichedGLAccountNumber", "")).strip()
        gname = str(r.get("EnrichedGLAccountName", "")).strip()
        gid = str(r.get("EnrichedGLAccountID", "")).strip()
        if gnum:
            if gnum not in gl:
                gl[gnum] = {"name": gname, "id": gid}
            elif not gl[gnum].get("name") and gname:
                gl[gnum]["name"] = gname
            elif not gl[gnum].get("id") and gid:
                gl[gnum]["id"] = gid
        ut = str(r.get("Utility Type", "")).strip()
        if ut:
            utils.add(ut.title())
    props = [{"name": k, "id": v} for k, v in sorted(prop.items())]
    vends = [{"name": k, "id": v} for k, v in sorted(vend.items())]
    gls = [{"number": k, "name": v.get("name", ""), "id": v.get("id", "")} for k, v in sorted(gl.items())]
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"properties": props, "vendors": vends, "gl_accounts": gls, "utilities": sorted(utils)}
@app.get("/api/dates")
def api_dates(user: str = Depends(require_user)):
    return {"dates": list_dates()}


@app.get("/api/day")
def api_day(date: str, user: str = Depends(require_user), response: Response = None):
    y, m, d = date.split("-")
    rows = load_day(y, m, d)
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"rows": rows}


@app.get("/api/invoices")
def api_invoices(date: str, user: str = Depends(require_user), response: Response = None):
    y, m, d = date.split("-")
    rows = load_day(y, m, d)
    # Build id list to fetch statuses so we can exclude Deleted lines from counts
    id_list: List[str] = [str(r.get("__id__")) for r in rows if r.get("__id__")]
    stmap = get_status_map(id_list)
    # Cache per-pdf header overrides for vendor/account, prefer shared '__final__', then current user
    header_by_pdf: Dict[str, Dict[str, Any]] = {}
    for r in rows:
        key = r.get("__s3_key__", "")
        if not key: continue
        pid = pdf_id_from_key(key)
        if pid and pid not in header_by_pdf:
            d = get_draft(pid, "__header__", "__final__") or get_draft(pid, "__header__", user)
            header_by_pdf[pid] = (d or {}).get("fields", {})
    inv: Dict[str, Dict[str, Any]] = {}
    for r in rows:
        if stmap.get(str(r.get("__id__")), {}).get("status") == "Deleted":
            continue
        inv_no = str(r.get("Invoice Number", "")) or "(unknown)"
        key = r.get("__s3_key__", "")
        pid = pdf_id_from_key(key) if key else ""
        hdr = header_by_pdf.get(pid, {})
        # If header changed account/vendor, reflect in list display (grouping stays by invoice number here)
        g = inv.setdefault(inv_no, {"invoice": inv_no, "count": 0, "status": "REVIEW"})
        g["count"] += 1
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"invoices": sorted(inv.values(), key=lambda x: x["invoice"]) }


@app.get("/api/invoices_status")
def api_invoices_status(date: str, user: str = Depends(require_user), response: Response = None):
    """Return status per (vendor, account, pdf_id) group for the given date, matching the logic used by /invoices."""
    y, m, d = date.split("-")
    rows = load_day(y, m, d)
    # Preload header overrides per pdf, prefer shared '__final__', then current user
    header_by_pdf: Dict[str, Dict[str, Any]] = {}
    for r in rows:
        key = r.get("__s3_key__", "")
        if not key: continue
        pid = pdf_id_from_key(key)
        if pid and pid not in header_by_pdf:
            d = get_draft(pid, "__header__", "__final__") or get_draft(pid, "__header__", user)
            header_by_pdf[pid] = (d or {}).get("fields", {})

    inv: Dict[tuple, Dict[str, Any]] = {}
    group_ids: Dict[tuple, List[str]] = {}
    for r in rows:
        pdf_id = pdf_id_from_key(r.get("__s3_key__", "")) if r.get("__s3_key__") else "(unknown)"
        hdr = header_by_pdf.get(pdf_id, {})
        # Use row's Account Number directly (NOT header override) to preserve multi-account grouping
        # Use Line Item Account Number as fallback when Account Number is blank (for subtotals/taxes)
        account_no = str(r.get("Account Number", "") or r.get("Line Item Account Number", "") or "") or "(unknown)"
        invoice_no = str(r.get("Invoice Number", "")) or "(unknown)"
        vendor = (
            str(hdr.get("EnrichedVendorName", ""))
            or str(r.get("EnrichedVendorName", ""))
            or str(r.get("Vendor Name", ""))
            or str(r.get("Vendor", ""))
            or str(r.get("Utility Type", ""))
            or "(unknown)"
        )
        property_name = (
            str(r.get("EnrichedPropertyName", ""))
            or str(r.get("Property Name", ""))
            or str(r.get("PropertyName", ""))
            or ""
        )
        key = (vendor, account_no, pdf_id)
        g = inv.setdefault(key, {
            "vendor": vendor,
            "account": account_no,
            "pdf_id": pdf_id,
            "invoice": invoice_no,
            "count": 0,
            "status": "REVIEW",
            "property": property_name
        })
        # Update property if not set (first row may not have it)
        if not g.get("property") and property_name:
            g["property"] = property_name
        rid = str(r.get("__id__"))
        group_ids.setdefault(key, []).append(rid)
        # We will compute count after we know which ids are Deleted

    all_ids: List[str] = [i for ids in group_ids.values() for i in ids if i]
    stmap = get_status_map(all_ids)

    def has_account_artifact(acct_val: str) -> bool:
        # Consider both Pre-Entrata and Post-Entrata artifacts for completion
        acct = (acct_val or "").strip()
        if not acct:
            return False
        token_dash = "".join(ch if (ch.isalnum() or ch in "-") else "-" for ch in acct)
        token_us = "".join(ch if ch.isalnum() else "_" for ch in acct)
        prefixes = [
            f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/",
            f"{POST_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/",
        ]
        for pfx in prefixes:
            try:
                resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=pfx)
                for obj in resp.get("Contents", []) or []:
                    k = obj.get("Key", "")
                    if token_dash in k or token_us in k:
                        return True
            except Exception:
                continue
        return False

    for key, meta in inv.items():
        _, acct, _ = key
        ids_all = group_ids.get(key, [])
        # Exclude Deleted from counts and completion criteria
        ids_active = [i for i in ids_all if stmap.get(i, {}).get("status") != "Deleted"]
        meta["count"] = len(ids_active)
        submitted = sum(1 for i in ids_active if stmap.get(i, {}).get("status") == "Submitted")
        if submitted == 0:
            meta["status"] = "REVIEW"
        elif submitted == len(ids_active):
            meta["status"] = "COMPLETE"
        else:
            meta["status"] = "PARTIAL"

    out = [
        {
            "vendor": v["vendor"],
            "account": v["account"],
            "pdf_id": v["pdf_id"],
            "invoice": v["invoice"],
            "count": v["count"],
            "status": v["status"],
            "property": v.get("property", ""),
        }
        for v in inv.values()
    ]
    try:
        if response is not None:
            response.headers["Cache-Control"] = f"private, max-age={CACHE_TTL_SECONDS}"
    except Exception:
        pass
    return {"invoices": out}


@app.get("/api/drafts")
def api_get_draft(pdf_id: str, line_id: str, user: str = Depends(require_user)):
    # Choose the freshest between user and '__final__' by updated_utc; avoid stale user autosaves overriding submitted values
    final_d = get_draft(pdf_id, line_id, "__final__")
    user_d = get_draft(pdf_id, line_id, user)
    def ts(d):
        try:
            return dt.datetime.fromisoformat(d.get("updated_utc")) if d and d.get("updated_utc") else dt.datetime.min
        except Exception:
            return dt.datetime.min
    out = None
    if user_d and final_d:
        out = user_d if ts(user_d) >= ts(final_d) else final_d
    else:
        out = user_d or final_d or {"fields": {}}
    return {"draft": out or {"fields": {}} }


@app.get("/api/drafts/new-lines")
def api_get_new_line_drafts(pdf_id: str, user: str = Depends(require_user)):
    """Get all new line drafts for a pdf_id (lines added by user that don't exist in original S3 data).

    Scans drafts table for entries where line_id starts with 'new-' for this pdf_id.
    Returns list of {line_id, fields} for each new line found.
    """
    if not pdf_id:
        return {"new_lines": []}

    # Query both user's drafts and __final__ drafts for new lines
    # Use scan with filter since we can't query by partial pk
    new_lines = []
    seen_line_ids = set()

    for check_user in [user, "__final__"]:
        prefix = f"draft#{pdf_id}#new-"
        try:
            # Scan is expensive but new lines are rare and this is a targeted prefix
            paginator = ddb.get_paginator('scan')
            for page in paginator.paginate(
                TableName=DRAFTS_TABLE,
                FilterExpression="begins_with(pk, :prefix)",
                ExpressionAttributeValues={":prefix": {"S": f"draft#{pdf_id}#new-"}},
            ):
                for item in page.get("Items", []):
                    pk = item.get("pk", {}).get("S", "")
                    # Extract line_id from pk: draft#{pdf_id}#{line_id}#{user}
                    parts = pk.split("#")
                    if len(parts) >= 4:
                        line_id = parts[2]
                        item_user = parts[3]
                        if line_id.startswith("new-") and line_id not in seen_line_ids:
                            # Parse fields
                            fields_raw = item.get("fields", {}).get("S", "{}")
                            try:
                                fields = json.loads(fields_raw) if isinstance(fields_raw, str) else fields_raw
                            except Exception:
                                fields = {}
                            # Check if deleted
                            if fields.get("__deleted__"):
                                continue
                            new_lines.append({
                                "line_id": line_id,
                                "fields": fields
                            })
                            seen_line_ids.add(line_id)
        except Exception as e:
            print(f"[NEW LINES] Error scanning for new lines: {e}")

    return {"new_lines": new_lines}


@app.post("/api/drafts/batch")
def api_get_drafts_batch(payload: Dict[str, Any] = Body(...), user: str = Depends(require_user)):
    """Batch load multiple drafts in a single request - much faster than individual calls.

    Request body: {"items": [{"pdf_id": "...", "line_id": "..."}, ...]}
    Response: {"drafts": {"pdf_id#line_id": {...draft...}, ...}}
    """
    items = payload.get("items", [])
    if not items:
        return {"drafts": {}}

    # Build list of all keys to fetch (both user and __final__ for each item)
    keys_to_fetch = []
    for item in items:
        pdf_id = item.get("pdf_id")
        line_id = item.get("line_id")
        if pdf_id and line_id:
            # Fetch both user draft and final draft to compare
            keys_to_fetch.append({"pk": {"S": f"draft#{pdf_id}#{line_id}#{user}"}})
            keys_to_fetch.append({"pk": {"S": f"draft#{pdf_id}#{line_id}#__final__"}})

    if not keys_to_fetch:
        return {"drafts": {}}

    # DynamoDB BatchGetItem has a limit of 100 keys per request
    all_items = []
    for i in range(0, len(keys_to_fetch), 100):
        batch_keys = keys_to_fetch[i:i+100]
        try:
            resp = ddb.batch_get_item(
                RequestItems={
                    DRAFTS_TABLE: {"Keys": batch_keys}
                }
            )
            all_items.extend(resp.get("Responses", {}).get(DRAFTS_TABLE, []))
            # Handle unprocessed keys (retry once)
            unprocessed = resp.get("UnprocessedKeys", {}).get(DRAFTS_TABLE, {}).get("Keys", [])
            if unprocessed:
                retry_resp = ddb.batch_get_item(RequestItems={DRAFTS_TABLE: {"Keys": unprocessed}})
                all_items.extend(retry_resp.get("Responses", {}).get(DRAFTS_TABLE, []))
        except Exception as e:
            print(f"[BATCH DRAFTS] Error fetching batch: {e}")

    # Parse DynamoDB items into a lookup dict
    drafts_by_pk = {}
    for item in all_items:
        pk = item.get("pk", {}).get("S", "")
        parsed = {k: list(v.values())[0] for k, v in item.items()}
        if "fields" in parsed:
            try:
                parsed["fields"] = json.loads(parsed["fields"]) if isinstance(parsed["fields"], str) else parsed["fields"]
            except Exception:
                parsed["fields"] = {}
        drafts_by_pk[pk] = parsed

    # Helper to get timestamp for comparison
    def ts(d):
        try:
            return dt.datetime.fromisoformat(d.get("updated_utc")) if d and d.get("updated_utc") else dt.datetime.min
        except Exception:
            return dt.datetime.min

    # Build response: pick freshest between user and final for each item
    result = {}
    for item in items:
        pdf_id = item.get("pdf_id")
        line_id = item.get("line_id")
        if not pdf_id or not line_id:
            continue

        user_pk = f"draft#{pdf_id}#{line_id}#{user}"
        final_pk = f"draft#{pdf_id}#{line_id}#__final__"
        user_d = drafts_by_pk.get(user_pk)
        final_d = drafts_by_pk.get(final_pk)

        # Pick the freshest
        if user_d and final_d:
            out = user_d if ts(user_d) >= ts(final_d) else final_d
        else:
            out = user_d or final_d or {"fields": {}}

        key = f"{pdf_id}#{line_id}"
        result[key] = out or {"fields": {}}

    return {"drafts": result}


@app.put("/api/drafts")
def api_put_draft(payload: Dict[str, Any] = Body(...), user: str = Depends(require_user)):
    pdf_id = payload.get("pdf_id"); line_id = payload.get("line_id"); fields = payload.get("fields", {})
    date = payload.get("date", ""); invoice = str(payload.get("invoice", ""))
    if not pdf_id or not line_id:
        return JSONResponse({"error":"missing pdf_id/line_id"}, status_code=400)
    put_draft(pdf_id, line_id, user, fields, date, invoice)
    return {"ok": True}


# -------- Invoice Timing Tracker APIs --------
def _get_timing(invoice_id: str, user: str) -> dict:
    """Get timing record for an invoice/user combination."""
    pk = f"timing#{invoice_id}#{user}"
    try:
        resp = ddb.get_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
        item = resp.get("Item")
        if not item:
            return {"total_seconds": 0, "sessions": []}
        return {
            "total_seconds": int(item.get("total_seconds", {}).get("N", 0)),
            "sessions": json.loads(item.get("sessions", {}).get("S", "[]")),
            "last_heartbeat": item.get("last_heartbeat", {}).get("S", ""),
            "current_session_start": item.get("current_session_start", {}).get("S", ""),
        }
    except Exception as e:
        print(f"[TIMING] Error getting timing: {e}")
        return {"total_seconds": 0, "sessions": []}


def _put_timing(invoice_id: str, user: str, timing_data: dict):
    """Save timing record for an invoice/user combination."""
    pk = f"timing#{invoice_id}#{user}"
    try:
        item = {
            "pk": {"S": pk},
            "invoice_id": {"S": invoice_id},
            "user": {"S": user},
            "total_seconds": {"N": str(int(timing_data.get("total_seconds", 0)))},
            "sessions": {"S": json.dumps(timing_data.get("sessions", []))},
            "updated_utc": {"S": dt.datetime.utcnow().isoformat()},
        }
        if timing_data.get("last_heartbeat"):
            item["last_heartbeat"] = {"S": timing_data["last_heartbeat"]}
        if timing_data.get("current_session_start"):
            item["current_session_start"] = {"S": timing_data["current_session_start"]}
        ddb.put_item(TableName=DRAFTS_TABLE, Item=item)
        return True
    except Exception as e:
        print(f"[TIMING] Error saving timing: {e}")
        return False


@app.get("/api/timing/{invoice_id}")
def api_get_timing(invoice_id: str, user: str = Depends(require_user)):
    """Get timing data for an invoice."""
    timing = _get_timing(invoice_id, user)
    return {
        "invoice_id": invoice_id,
        "user": user,
        "total_seconds": timing.get("total_seconds", 0),
        "total_minutes": round(timing.get("total_seconds", 0) / 60, 1),
        "sessions": timing.get("sessions", []),
    }


@app.post("/api/timing/{invoice_id}/start")
def api_start_timing(invoice_id: str, user: str = Depends(require_user)):
    """Start a timing session for an invoice."""
    timing = _get_timing(invoice_id, user)
    now = dt.datetime.utcnow().isoformat()
    timing["current_session_start"] = now
    timing["last_heartbeat"] = now
    _put_timing(invoice_id, user, timing)
    return {"ok": True, "started_at": now}


@app.post("/api/timing/{invoice_id}/heartbeat")
def api_timing_heartbeat(invoice_id: str, user: str = Depends(require_user)):
    """Update heartbeat - called periodically to track active time."""
    timing = _get_timing(invoice_id, user)
    now = dt.datetime.utcnow()
    now_str = now.isoformat()

    # If no active session, start one
    if not timing.get("current_session_start"):
        timing["current_session_start"] = now_str
        timing["last_heartbeat"] = now_str
        _put_timing(invoice_id, user, timing)
        return {"ok": True, "action": "started_new_session"}

    # Check if session is stale (no heartbeat for > 2 minutes = session ended)
    last_hb = timing.get("last_heartbeat", "")
    if last_hb:
        try:
            last_hb_dt = dt.datetime.fromisoformat(last_hb.replace("Z", "+00:00").replace("+00:00", ""))
            diff = (now - last_hb_dt).total_seconds()
            if diff > 120:  # Session was stale, close it and start new
                # Add elapsed time from stale session (use last heartbeat as end)
                session_start = timing.get("current_session_start", "")
                if session_start:
                    start_dt = dt.datetime.fromisoformat(session_start.replace("Z", "+00:00").replace("+00:00", ""))
                    session_seconds = (last_hb_dt - start_dt).total_seconds()
                    if session_seconds > 0:
                        timing["total_seconds"] = timing.get("total_seconds", 0) + int(session_seconds)
                        timing.setdefault("sessions", []).append({
                            "start": session_start,
                            "end": last_hb,
                            "seconds": int(session_seconds),
                        })
                # Start new session
                timing["current_session_start"] = now_str
        except Exception:
            pass

    timing["last_heartbeat"] = now_str
    _put_timing(invoice_id, user, timing)
    return {"ok": True, "total_seconds": timing.get("total_seconds", 0)}


@app.post("/api/timing/{invoice_id}/stop")
def api_stop_timing(invoice_id: str, user: str = Depends(require_user)):
    """Stop timing session for an invoice."""
    timing = _get_timing(invoice_id, user)
    now = dt.datetime.utcnow()
    now_str = now.isoformat()

    session_start = timing.get("current_session_start", "")
    session_seconds = 0

    if session_start:
        try:
            start_dt = dt.datetime.fromisoformat(session_start.replace("Z", "+00:00").replace("+00:00", ""))
            session_seconds = int((now - start_dt).total_seconds())
            if session_seconds > 0:
                timing["total_seconds"] = timing.get("total_seconds", 0) + session_seconds
                timing.setdefault("sessions", []).append({
                    "start": session_start,
                    "end": now_str,
                    "seconds": session_seconds,
                })
        except Exception:
            pass

    timing["current_session_start"] = ""
    timing["last_heartbeat"] = ""
    _put_timing(invoice_id, user, timing)

    return {
        "ok": True,
        "session_seconds": session_seconds,
        "total_seconds": timing.get("total_seconds", 0),
        "total_minutes": round(timing.get("total_seconds", 0) / 60, 1),
    }


@app.get("/api/timing/summary")
def api_get_timing_summary(date: str = "", user: str = Depends(require_user)):
    """Get timing summary for all invoices worked on by user (optionally filtered by date)."""
    try:
        # Scan for all timing records for this user
        response = ddb.scan(
            TableName=DRAFTS_TABLE,
            FilterExpression="begins_with(pk, :prefix) AND #u = :user",
            ExpressionAttributeNames={"#u": "user"},
            ExpressionAttributeValues={
                ":prefix": {"S": "timing#"},
                ":user": {"S": user},
            },
        )
        items = []
        for item in response.get("Items", []):
            invoice_id = item.get("invoice_id", {}).get("S", "")
            total_secs = int(item.get("total_seconds", {}).get("N", 0))
            updated = item.get("updated_utc", {}).get("S", "")
            # Filter by date if provided
            if date and updated and not updated.startswith(date):
                continue
            items.append({
                "invoice_id": invoice_id,
                "total_seconds": total_secs,
                "total_minutes": round(total_secs / 60, 1),
                "updated": updated,
            })
        items.sort(key=lambda x: x.get("updated", ""), reverse=True)
        total_all = sum(i.get("total_seconds", 0) for i in items)
        return {
            "user": user,
            "date_filter": date,
            "invoices": items,
            "invoice_count": len(items),
            "total_seconds": total_all,
            "total_minutes": round(total_all / 60, 1),
            "total_hours": round(total_all / 3600, 2),
        }
    except Exception as e:
        print(f"[TIMING] Error getting summary: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/overrides")
def api_overrides(date: str = Form(...), payload: str = Form(...), user: str = Depends(require_user)):
    # payload is JSON array of override items with id/source_s3_key/row_index/changes
    try:
        items = json.loads(payload)
    except Exception as e:
        return JSONResponse({"error": f"invalid payload: {e}"}, status_code=400)
    y, m, d = date.split("-")
    out_key = write_overrides(y, m, d, items)
    if not out_key:
        return JSONResponse({"error": "no overrides"}, status_code=400)
    return {"ok": True, "s3_key": out_key}


@app.post("/api/status")
def api_status(id: str = Form(...), status: str = Form(...), user: str = Form("reviewer")):
    put_status(id, status, user)
    return {"ok": True}


@app.post("/api/submit")
def api_submit(date: str = Form(...), ids: str = Form(...), extras: str = Form(""), deleted_ids: str = Form(""), unit_overrides: str = Form(""), confirmed_warnings: str = Form(""), background_tasks: BackgroundTasks = None, user: str = Depends(require_user)):
    """Finalize an invoice: create override delta and merged outputs, update status, and optionally notify via SQS.

    Behavior:
    - Load originals for provided ids (all from the same invoice).
    - Load header draft (line_id='__header__') and each line draft.
    - Build per-line delta (only changed fields) and merged records (original + applied overrides).
    - Write two files to S3 under OVERRIDE_PREFIX: overrides_delta_*.jsonl and overrides_merged_*.jsonl.
    - Update DDB review status and optionally send SQS messages.

    If a line is coded as Vacant but has no unit/apartment number in Service Address,
    the endpoint returns 'missing_units' with line details for the frontend to prompt.
    Re-submit with unit_overrides JSON: {"line_id": "unit_number", ...}

    confirmed_warnings: JSON list of warning types that user confirmed (e.g., ["amount_spike", "mass_delete"]).
    When present, learning patterns are quarantined for admin review.
    """
    # parse date
    try:
        y, m, d = date.split('-')
    except ValueError:
        return JSONResponse({"error": "bad date"}, status_code=400)

    try:
        # Use ||| as delimiter to avoid breaking on filenames with commas
        id_list = [x for x in ids.split('|||') if x]
        deleted_set = set([x for x in (deleted_ids or "").split('|||') if x])
        if not id_list:
            return JSONResponse({"error": "no ids"}, status_code=400)

        # Parse unit overrides: {"line_id": "unit_number", ...}
        unit_override_map: Dict[str, str] = {}
        try:
            if unit_overrides:
                unit_override_map = json.loads(unit_overrides)
                if not isinstance(unit_override_map, dict):
                    unit_override_map = {}
        except Exception:
            unit_override_map = {}

        # Parse confirmed warnings (for quarantine tracking)
        confirmed_warning_types: list = []
        try:
            if confirmed_warnings:
                confirmed_warning_types = json.loads(confirmed_warnings)
                if not isinstance(confirmed_warning_types, list):
                    confirmed_warning_types = []
        except Exception:
            confirmed_warning_types = []

        # load all rows for the day, map by __id__
        # Try with cached data first (matches what page saw), then force refresh if no match
        rows = load_day(y, m, d)
        by_id = {str(r.get("__id__")): r for r in rows}
        originals: List[Dict[str, Any]] = []
        for id_ in id_list:
            if id_ in by_id:
                originals.append(by_id[id_])
        # If no matches found, try again with fresh data (cache may have expired since page load)
        if not originals:
            rows = load_day(y, m, d, force_refresh=True)
            by_id = {str(r.get("__id__")): r for r in rows}
            for id_ in id_list:
                if id_ in by_id:
                    originals.append(by_id[id_])
        if not originals:
            return JSONResponse({"error": "no matching originals", "submitted_ids": id_list, "available_ids": list(by_id.keys())[:5]}, status_code=404)

        # determine pdf_id for header draft key from first original
        first = originals[0]
        key0 = first.get("__s3_key__", "")
        pid0 = pdf_id_from_key(key0) if key0 else ""
        header_draft = get_draft(pid0, "__header__", user) or {"fields": {}}
        header_fields = header_draft.get("fields", {})

        # editable line-level fields (exclude GL DESC_NEW which is auto)
        line_edit_fields = [
            "EnrichedGLAccountNumber","EnrichedGLAccountName","ENRICHED CONSUMPTION","ENRICHED UOM",
            "Meter Number","Meter Size","House Or Vacant","Utility Type",
            "Line Item Description","Line Item Charge","Consumption Amount","Unit of Measure",
            "Previous Reading","Previous Reading Date","Current Reading","Current Reading Date","Rate",
        ]
        # invoice-level editable fields (applied to each line)
        # NOTE: Service Address, Service City, Service State, Service Zipcode are NOT header fields
        # They are per-line fields since an invoice can have multiple service addresses
        header_edit_fields = [
            "EnrichedVendorName","EnrichedVendorID",
            "EnrichedPropertyName","EnrichedPropertyID",
            "Account Number",
            "Bill Period Start","Bill Period End","Bill Date","Due Date",
            "Special Instructions",
            "Bill From",
        ]

        deltas: List[Dict[str, Any]] = []
        merged: List[Dict[str, Any]] = []

        def _norm(val: Any) -> str:
            return (str(val or "").strip())

        def _build_gl_desc(rec: Dict[str, Any]) -> str:
            """Recompute GL DESC_NEW to mirror enrichment formatting using the latest edited values.

            For VACANT GLs (5705-0000, 5715-0000, 5720-1000, 5721-1000):
              "(M/D/YY-M/D/YY V[E/G/W/S] Street#Letter@Unit[!])"
              Example: (7/24/25-8/21/25 VE 9436N@159)

            For HOUSE GLs (standard format):
              "{Line Item Description} | {BPS}-{BPE} | {Service Address} | {Account Number} | {Line Item Account Number} | {Meter Number} | {Consumption} | {UOM}"
            """
            # Check for VACANT GL - use special format
            vacant_desc = _build_vacant_gl_desc(rec)
            if vacant_desc:
                return vacant_desc

            # HOUSE format (standard)
            addr = _norm(rec.get("Service Address")).upper()
            acct = _norm(rec.get("Account Number"))
            li_acct = _norm(rec.get("Line Item Account Number"))
            meter = _norm(rec.get("Meter Number"))
            desc = _norm(rec.get("Line Item Description")).upper()
            # Prefer enriched consumption/uom, fallback to raw
            cons = _norm(rec.get("ENRICHED CONSUMPTION") or rec.get("Consumption Amount"))
            uom = _norm(rec.get("ENRICHED UOM") or rec.get("Unit of Measure")).upper()
            bps = _norm(rec.get("Bill Period Start"))
            bpe = _norm(rec.get("Bill Period End"))
            rng = f"{bps}-{bpe}" if (bps or bpe) else ""
            # Build in requested order
            parts = [
                desc,
                rng,
                addr,
                acct,
                li_acct,
                meter,
                cons,
                uom,
            ]
            return " | ".join(parts)

        UPPER_FIELDS = [
            "Service Address", "Service City", "Service State"
        ]

        # --- House/Vacant server-side guardrails ---
        VACANT_NAMES = {"VACANT ELECTRIC","VACANT GAS","VACANT WATER","VACANT SEWER","VACANT ACTIVATION"}
        HOUSE_BACKFILL = {
            "VACANT ELECTRIC": "HOUSE ELECTRIC",
            "VACANT GAS": "GAS",
            "VACANT WATER": "WATER",
            "VACANT SEWER": "SEWER",
            "VACANT ACTIVATION": "",
        }
        # GL Name -> GL Number mapping to keep them in sync when _ensure_hov changes GL Name
        GL_NAME_TO_NUMBER = {
            "VACANT ELECTRIC": "57050000",
            "VACANT GAS": "57150000",
            "VACANT WATER": "57201000",
            "VACANT SEWER": "57211000",
            "VACANT ACTIVATION": "57040000",
            "HOUSE ELECTRIC": "57060000",
            "GAS": "57100000",
            "WATER": "57200000",
            "SEWER": "57210000",
        }
        # Only these utility GL types can be swapped between House/Vacant.
        # Everything else (Penalties, Late Fee, Taxes, etc.) is left alone.
        _UTILITY_GL_KEYWORDS = {"ELECTRIC", "GAS", "WATER", "SEWER"}
        import re as _re
        # Match unit numbers: explicit keywords OR street suffix followed by unit number
        # NOTE: APARTMENT/SUITE must come before APT/AP/STE to avoid partial matches
        _unit_re = _re.compile(
            r"(?:\b(?:APARTMENT|APT|AP|SUITE|STE|UNIT|BLDG)|#)\s*\w+"  # APARTMENT 5, APT 5, AP 3022, # F305
            r"|\b(?:AVE|AVENUE|ST|STREET|DR|DRIVE|RD|ROAD|LN|LANE|BLVD|BOULEVARD|WAY|CT|COURT|PL|PLACE|CIR|CIRCLE|PKWY|PARKWAY)\s+(?:\d+[A-Z]?|[A-Z]\d*)\b",  # AVE 542, ST 101A, PKWY 100
            _re.I
        )
        def _ensure_hov(rec: Dict[str, Any], user_edited_fields: set = None) -> None:
            """Default to House unless there's a clear unit/apartment indicator. Keep GL Name and Number in sync.
            Vacant only when a clear unit/apartment indicator is present and no explicit parser choice exists.

            IMPORTANT: If user explicitly edited GL fields (in user_edited_fields), we respect their choice
            and do NOT auto-modify GL Name or Number.
            """
            user_edited_fields = user_edited_fields or set()
            user_edited_gl = bool({"EnrichedGLAccountName", "EnrichedGLAccountNumber"} & user_edited_fields)

            hov = str(rec.get("House Or Vacant") or "").strip()
            gln = str(rec.get("EnrichedGLAccountName") or "").strip()
            util = str(rec.get("Utility Type") or "").strip()
            addr = str(rec.get("Service Address") or "").strip()
            has_unit = bool(_unit_re.search(addr))
            gln_upper = gln.upper()
            is_vacant_gl = ("VACANT" in gln_upper)
            # decide desired HOV; if parser already set HOV, respect it
            desired = hov if hov else ("Vacant" if has_unit else "House")
            if hov != desired:
                rec["House Or Vacant"] = desired

            # SKIP GL auto-modification if user explicitly edited GL fields
            if user_edited_gl:
                return

            # Only modify GL for utility types (Electric, Gas, Water, Sewer).
            # Everything else (Penalties, Late Fee, Taxes, etc.) is never a House/Vacant GL.
            if not any(kw in gln_upper for kw in _UTILITY_GL_KEYWORDS):
                return

            # adjust GL Name if it conflicts with desired
            if desired == "Vacant":
                if not is_vacant_gl:
                    # try to map to a Vacant version for the utility
                    vac_try = f"Vacant {util}".strip()
                    if vac_try.upper() in VACANT_NAMES:
                        rec["EnrichedGLAccountName"] = vac_try
                        # Sync GL Number to match the new Name
                        new_num = GL_NAME_TO_NUMBER.get(vac_try.upper())
                        if new_num:
                            rec["EnrichedGLAccountNumber"] = new_num
                    elif gln and not gln_upper.startswith("VACANT "):
                        rec["EnrichedGLAccountName"] = "Vacant " + gln
                        # Try to sync GL Number for the new Vacant name
                        new_num = GL_NAME_TO_NUMBER.get(("VACANT " + gln).upper())
                        if new_num:
                            rec["EnrichedGLAccountNumber"] = new_num
            else:  # House
                if is_vacant_gl:
                    mapped = HOUSE_BACKFILL.get(gln_upper, gln.replace("Vacant ", "").replace("VACANT ", "").strip())
                    if mapped is not None:
                        rec["EnrichedGLAccountName"] = mapped
                        # Sync GL Number to match the new Name
                        if mapped:
                            new_num = GL_NAME_TO_NUMBER.get(mapped.upper())
                            if new_num:
                                rec["EnrichedGLAccountNumber"] = new_num

        # Prepare vendor name -> id map from the full day's rows to resolve IDs on submit
        vendor_map_submit: Dict[str, str] = {}
        for r in rows:
            n = str(r.get("EnrichedVendorName", "") or r.get("Vendor Name", "") or r.get("Vendor", "")).strip()
            i = str(r.get("EnrichedVendorID", "")).strip()
            if n:
                key = n.upper()
                if key not in vendor_map_submit or not vendor_map_submit[key]:
                    vendor_map_submit[key] = i

        for orig in originals:
            key = orig.get("__s3_key__", ""); idx = orig.get("__row_idx__", 0)
            pid = pdf_id_from_key(key) if key else ""
            lid = line_id_from(key or "", idx)
            line_draft = get_draft(pid, lid, user) or {"fields": {}}
            lf = line_draft.get("fields", {})

            # If this original line is flagged deleted, skip producing merged/delta output
            if str(orig.get("__id__")) in deleted_set:
                continue

            # apply header + line fields
            new_rec = dict(orig)
            for k in header_edit_fields:
                if k in header_fields and header_fields[k] != "":
                    new_rec[k] = header_fields[k]
            for k in line_edit_fields:
                if k in lf and lf[k] != "":
                    new_rec[k] = lf[k]
            # If we have a vendor name but missing ID, attempt to map by name
            if str(new_rec.get("EnrichedVendorName", "")).strip() and not str(new_rec.get("EnrichedVendorID", "")).strip():
                vkey = str(new_rec.get("EnrichedVendorName", "")).strip().upper()
                vid = vendor_map_submit.get(vkey, "")
                if vid:
                    new_rec["EnrichedVendorID"] = vid

            # Inherit Service Address from other lines if this line has "No Address" or blank.
            # All four address fields are inherited from the SAME sibling to avoid mixing
            # addresses from different service locations on a multi-meter invoice.
            _addr_fields = ("Service Address", "Service City", "Service State", "Service Zipcode")
            _primary_val = (new_rec.get("Service Address") or "").strip()
            if not _primary_val or _primary_val.upper() == "NO ADDRESS":
                # Find the first sibling with a valid Service Address
                for sibling in originals:
                    sib_addr = (sibling.get("Service Address") or "").strip()
                    if sib_addr and sib_addr.upper() != "NO ADDRESS":
                        # Inherit all four fields from this same sibling
                        for addr_field in _addr_fields:
                            sib_val = (sibling.get(addr_field) or "").strip()
                            if sib_val and sib_val.upper() != "NO ADDRESS":
                                new_rec[addr_field] = sib_val
                        break

            # Normalize select fields to ALL CAPS for downstream analytics consistency
            for fname in UPPER_FIELDS:
                if fname in new_rec and isinstance(new_rec[fname], str):
                    new_rec[fname] = new_rec[fname].upper()

            # Server-side enforce House/Vacant rule before recomputing desc
            # Pass user-edited fields so we don't overwrite explicit GL edits
            # IMPORTANT: Only include fields that had NON-EMPTY values actually applied
            # Empty draft values are NOT applied (see line_edit_fields loop), so don't mark as "edited"
            actually_applied_fields = {k for k in lf.keys() if k in line_edit_fields and lf.get(k, "") != ""}
            _ensure_hov(new_rec, user_edited_fields=actually_applied_fields)

            # Recompute GL DESC_NEW using latest values
            new_rec["GL DESC_NEW"] = _build_gl_desc(new_rec)

            # build delta only for changed fields (excluding GL DESC_NEW)
            delta = {k: new_rec.get(k) for k in (set(header_edit_fields) | set(line_edit_fields)) if str(new_rec.get(k, "")) != str(orig.get(k, ""))}
            if delta:
                deltas.append({
                    "__id__": orig.get("__id__"),
                    "__s3_key__": key,
                    "__row_idx__": idx,
                    "changes": delta,
                })
            merged.append(new_rec)

        # Handle any manual extra lines (added in UI)
        extra_lines: List[Dict[str, Any]] = []
        try:
            if extras:
                parsed = json.loads(extras)
                if isinstance(parsed, list):
                    extra_lines = parsed
        except Exception:
            # ignore malformed extras to avoid blocking submits
            extra_lines = []

        for e in extra_lines:
            # build a new record using header defaults overlaid with provided fields
            new_rec = dict(first)  # start from first to retain context columns
            # apply header fields first
            for k in header_edit_fields:
                if k in header_fields and header_fields[k] != "":
                    new_rec[k] = header_fields[k]
            # then overlay provided manual fields
            for k, v in (e or {}).items():
                if v != "":
                    new_rec[k] = v
            # Normalize select fields to ALL CAPS
            for fname in UPPER_FIELDS:
                if fname in new_rec and isinstance(new_rec[fname], str):
                    new_rec[fname] = new_rec[fname].upper()
            # Enforce House/Vacant on manual line too
            # For manual lines, only include fields with non-empty values that were actually applied
            manual_applied_fields = {k for k, v in (e or {}).items() if v != ""}
            _ensure_hov(new_rec, user_edited_fields=manual_applied_fields)
            # Recompute GL DESC_NEW for manual line
            new_rec["GL DESC_NEW"] = _build_gl_desc(new_rec)
            new_rec["__id__"] = None
            new_rec["__s3_key__"] = first.get("__s3_key__", "")
            new_rec["__row_idx__"] = -1
            new_rec["__manual__"] = True
            merged.append(new_rec)

            # represent the manual line as a delta of provided fields + any header-applied edits
            change_keys = set(header_edit_fields) | set(line_edit_fields)
            changes_obj = {k: new_rec.get(k) for k in change_keys if k in new_rec}
            deltas.append({
                "__id__": None,
                "__s3_key__": new_rec.get("__s3_key__", ""),
                "__row_idx__": -1,
                "changes": changes_obj,
                "__manual__": True,
            })

        # --- Validate: Vacant lines must have unit/apartment in Service Address ---
        # Check each merged record; if coded Vacant but no unit found, prompt user
        missing_units: List[Dict[str, Any]] = []
        for idx, rec in enumerate(merged):
            gln = str(rec.get("EnrichedGLAccountName") or "").strip().upper()
            hov = str(rec.get("House Or Vacant") or "").strip().upper()
            is_vacant = ("VACANT" in gln) or (hov == "VACANT")
            if not is_vacant:
                continue

            addr = str(rec.get("Service Address") or "").strip()
            has_unit = bool(_unit_re.search(addr))

            # Generate a unique line identifier
            line_id = str(rec.get("__id__") or f"manual_{idx}")

            # Check if user provided an override for this line
            # Empty string means "user confirmed no unit exists" - that's valid
            if line_id in unit_override_map:
                unit_num = str(unit_override_map[line_id]).strip()
                if unit_num:
                    # Apply the unit number to Service Address
                    if not has_unit:
                        rec["Service Address"] = f"{addr} #{unit_num}".strip()
                    has_unit = True
                    # Recompute GL DESC_NEW with updated address
                    rec["GL DESC_NEW"] = _build_gl_desc(rec)
                else:
                    # Empty string = user confirmed no unit exists, skip validation
                    has_unit = True  # Treat as "confirmed no unit"

            # If still no unit after applying overrides, flag it
            if not has_unit:
                missing_units.append({
                    "line_id": line_id,
                    "line_idx": idx,
                    "service_address": addr,
                    "gl_name": rec.get("EnrichedGLAccountName", ""),
                    "line_description": rec.get("Line Item Description", ""),
                    "line_charge": rec.get("Line Item Charge", ""),
                    "meter_number": rec.get("Meter Number", ""),
                    "account_number": rec.get("Account Number", ""),
                })

        # If there are Vacant lines missing units, return to frontend for prompt
        if missing_units:
            return JSONResponse({
                "error": "missing_units",
                "message": "Vacant lines are missing apartment/unit numbers. Please provide them.",
                "missing_units": missing_units,
            }, status_code=400)

        # Count lines for immediate response (before async work)
        total_lines = len([o for o in originals if str(o.get("__id__")) not in deleted_set]) + len(extra_lines)

        # Define background task for heavy I/O operations
        def do_submit_io():
            nonlocal deltas, merged, originals, first, header_fields, extra_lines, id_list, deleted_set, pid0, confirmed_warning_types
            try:
                # write outputs
                delta_key = _write_jsonl(OVERRIDE_PREFIX, y, m, d, "overrides_delta", deltas)
                merged_key = _write_jsonl(OVERRIDE_PREFIX, y, m, d, "overrides_merged", merged)

                # also write a per-invoice final (pre-Entrata) JSONL under Bill_Parser_6_PreEntrata_Submission
                def _safe(val: str) -> str:
                    val = (val or "").strip()
                    if not val:
                        return "(unknown)"
                    val = re.sub(r"\s+", " ", val)
                    keep = []
                    for ch in val:
                        keep.append(ch if (ch.isalnum() or ch in " -_()&,+.#") else "-")
                    return "".join(keep).strip()

                prop_name = _safe(str(header_fields.get("EnrichedPropertyName") or first.get("EnrichedPropertyName", "") or first.get("Property Name", "")))
                vendor_name = _safe(str(header_fields.get("EnrichedVendorName") or first.get("EnrichedVendorName", "") or first.get("Vendor Name", "") or first.get("Vendor", "") or first.get("Utility Type", "")))
                account_name = _safe(str(header_fields.get("Account Number") or first.get("Account Number", "") or first.get("Account", "")))
                svc_start = _safe(str(header_fields.get("Bill Period Start") or first.get("Bill Period Start", "")))
                svc_end = _safe(str(header_fields.get("Bill Period End") or first.get("Bill Period End", "")))
                due_date = _safe(str(header_fields.get("Due Date") or first.get("Due Date", "")))
                basename = f"{prop_name}-{vendor_name}-{account_name}-{svc_start}-{svc_end}-{due_date}"

                status_label = "Not Posted"
                title_str = f"{account_name} | {prop_name} | {vendor_name} | {y}-{m}-{d}"
                submit_timestamp = dt.datetime.utcnow().isoformat()
                merged_with_meta = [{**rec, "Title": title_str, "Status": status_label, "Submitter": user, "SubmittedAt": submit_timestamp} for rec in merged]

                if os.getenv("PRE_ENTRATA_KEEP_ONLY_LATEST", "1") == "1":
                    prefix = f"{PRE_ENTRATA_PREFIX}yyyy={y}/mm={m}/dd={d}/"
                    resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix)
                    for obj in resp.get("Contents", []):
                        k = obj["Key"]
                        if account_name in k and due_date and due_date in k:
                            s3.delete_object(Bucket=BUCKET, Key=k)

                _write_jsonl(PRE_ENTRATA_PREFIX, y, m, d, basename, merged_with_meta)

                # Append extra lines to Stage 4 if needed
                if extra_lines and first.get("__s3_key__"):
                    try:
                        s3_key = first["__s3_key__"]
                        txt = _read_s3_text(BUCKET, s3_key)
                        existing_lines = [l for l in txt.strip().split('\n') if l.strip()]
                        extra_records = []
                        for e in extra_lines:
                            new_rec = dict(first)
                            for k in header_edit_fields:
                                if k in header_fields and header_fields[k] != "":
                                    new_rec[k] = header_fields[k]
                            for k, v in (e or {}).items():
                                if v != "":
                                    new_rec[k] = v
                            for fname in UPPER_FIELDS:
                                if fname in new_rec and isinstance(new_rec[fname], str):
                                    new_rec[fname] = new_rec[fname].upper()
                            # Track which fields user explicitly set to avoid overwriting GL edits
                            extra_line_applied_fields = {k for k, v in (e or {}).items() if v != ""}
                            _ensure_hov(new_rec, user_edited_fields=extra_line_applied_fields)
                            new_rec["GL DESC_NEW"] = _build_gl_desc(new_rec)
                            for internal_key in ["__id__", "__s3_key__", "__row_idx__", "__manual__"]:
                                new_rec.pop(internal_key, None)
                            extra_records.append(json.dumps(new_rec, ensure_ascii=False))
                        new_content = '\n'.join(existing_lines + extra_records)
                        if s3_key.endswith('.gz'):
                            body = gzip.compress(new_content.encode('utf-8'))
                            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=body, ContentType='application/json', ContentEncoding='gzip')
                        else:
                            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=new_content.encode('utf-8'), ContentType='application/json')
                        for i in range(len(extra_lines)):
                            new_row_idx = len(existing_lines) + i
                            new_id = f"{s3_key}#{new_row_idx}"
                            put_status(new_id, "Submitted", user)
                        invalidate_day_cache(y, m, d)
                        print(f"[SUBMIT] Appended {len(extra_lines)} extra lines to Stage 4: {s3_key}")

                        # Delete new line drafts from DynamoDB to prevent re-adding on subsequent submits
                        try:
                            # Scan for all new line drafts for this pdf_id
                            paginator = ddb.get_paginator('scan')
                            for page in paginator.paginate(
                                TableName=DRAFTS_TABLE,
                                FilterExpression="begins_with(pk, :prefix)",
                                ExpressionAttributeValues={":prefix": {"S": f"draft#{pid0}#new-"}},
                            ):
                                for item in page.get("Items", []):
                                    pk = item.get("pk", {}).get("S", "")
                                    if pk:
                                        ddb.delete_item(TableName=DRAFTS_TABLE, Key={"pk": {"S": pk}})
                            print(f"[SUBMIT] Deleted new line drafts for pdf_id: {pid0}")
                        except Exception as del_err:
                            print(f"[SUBMIT] Warning: Failed to delete new line drafts: {del_err}")
                    except Exception as e:
                        print(f"[SUBMIT] Warning: Failed to append extra lines to Stage 4: {e}")

                # Update Stage 4 with header AND line-level values
                # Previously only header_edit_fields were applied here, so
                # line-level GL edits (e.g. 57066322 for late fees) were lost
                # on page reload because Stage 4 still had the original GL.
                if first.get("__s3_key__") and (header_fields or True):
                    try:
                        s3_key = first["__s3_key__"]
                        _s4_pid = pdf_id_from_key(s3_key)
                        txt = _read_s3_text(BUCKET, s3_key)
                        lines_raw = [l for l in txt.strip().split('\n') if l.strip()]
                        updated_lines = []
                        for row_idx, line in enumerate(lines_raw):
                            try:
                                rec = json.loads(line)
                                # Apply header edits (same for all lines)
                                for k in header_edit_fields:
                                    if k in header_fields and header_fields[k] != "":
                                        rec[k] = header_fields[k]
                                # Apply line-level edits from draft (per-line)
                                _lid = f"{_s4_pid}#{row_idx}"
                                _line_draft = get_draft(_s4_pid, _lid, user)
                                if _line_draft:
                                    _lf = _line_draft.get("fields", {})
                                    for k in line_edit_fields:
                                        if k in _lf and _lf[k] != "":
                                            rec[k] = _lf[k]
                                updated_lines.append(json.dumps(rec, ensure_ascii=False))
                            except Exception:
                                updated_lines.append(line)
                        new_content = '\n'.join(updated_lines)
                        if s3_key.endswith('.gz'):
                            body = gzip.compress(new_content.encode('utf-8'))
                            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=body, ContentType='application/json', ContentEncoding='gzip')
                        else:
                            s3.put_object(Bucket=BUCKET, Key=s3_key, Body=new_content.encode('utf-8'), ContentType='application/json')
                        invalidate_day_cache(y, m, d)
                        print(f"[SUBMIT] Updated Stage 4 with header + line-level values: {s3_key}")
                    except Exception as e:
                        print(f"[SUBMIT] Warning: Failed to update Stage 4: {e}")

                # Persist final snapshots
                try:
                    put_draft(pid0, "__header__", "__final__", header_fields, date, first.get("Invoice Number", ""))
                    for orig in originals:
                        key = orig.get("__s3_key__", ""); idx = orig.get("__row_idx__", 0)
                        pid = pdf_id_from_key(key) if key else ""
                        lid = line_id_from(key or "", idx)
                        line_draft = get_draft(pid, lid, user) or {"fields": {}}
                        lf = dict(line_draft.get("fields", {}) or {})
                        if str(orig.get("__id__")) in deleted_set:
                            lf["__deleted__"] = "1"
                        put_draft(pid, lid, "__final__", lf, date, first.get("Invoice Number", ""))
                except Exception:
                    pass

                # Update statuses and send SQS messages
                for id_ in id_list:
                    if str(id_) in deleted_set:
                        put_status(id_, "Deleted", user)
                        continue
                    if REVIEW_QUEUE_URL:
                        body = json.dumps({"id": id_, "submitted_by": user, "submitted_utc": dt.datetime.utcnow().isoformat(), "delta_key": delta_key, "merged_key": merged_key})
                        sqs.send_message(QueueUrl=REVIEW_QUEUE_URL, MessageBody=body)
                    put_status(id_, "Submitted", user)

                # Extract vendor/property/account for AI learning
                vendor_id = str(header_fields.get("EnrichedVendorID") or first.get("EnrichedVendorID") or "").strip()
                property_id = str(header_fields.get("EnrichedPropertyID") or first.get("EnrichedPropertyID") or "").strip()
                account_number = str(header_fields.get("Account Number") or first.get("Account Number") or "").strip()
                utility_type = str(first.get("Utility Type") or "").strip()

                # Track AI accuracy (compare AI suggestion to human actions)
                try:
                    # Determine if human made changes: any deltas with actual field changes, any deletions, or any extra lines
                    has_field_changes = any(d.get("fields", {}) for d in deltas if d)
                    human_made_changes = has_field_changes or len(deleted_set) > 0 or len(extra_lines) > 0
                    _track_ai_accuracy(pid0, deleted_set, human_made_changes, user)
                except Exception as track_err:
                    print(f"[SUBMIT] Warning: AI accuracy tracking failed: {track_err}")

                # Capture detailed human actions for learning
                try:
                    actions = _capture_human_actions(
                        pdf_id=pid0,
                        originals=originals,
                        merged=merged,
                        deleted_set=deleted_set,
                        header_fields=header_fields,
                        extra_lines=extra_lines,
                        deltas=deltas,
                        user=user,
                        vendor_id=vendor_id,
                        property_id=property_id,
                        account_number=account_number,
                    )

                    # Store correction patterns for learning (when human corrects AI mistakes)
                    if actions and (actions.get("lines_deleted_count", 0) > 0 or actions.get("gl_changes_count", 0) > 0):
                        ai_suggestion = _get_ai_suggestion(pid0)
                        # Quarantine patterns when user confirmed despite warnings
                        should_quarantine = len(confirmed_warning_types) > 0
                        quarantine_reason = ",".join(confirmed_warning_types) if should_quarantine else ""
                        _store_correction_patterns(
                            pdf_id=pid0,
                            actions=actions,
                            ai_suggestion=ai_suggestion,
                            vendor_id=vendor_id,
                            property_id=property_id,
                            utility_type=utility_type,
                            quarantine=should_quarantine,
                            quarantine_reason=quarantine_reason,
                        )
                except Exception as learn_err:
                    print(f"[SUBMIT] Warning: AI learning capture failed: {learn_err}")

                # Write history record for fast AI review lookups
                try:
                    bill_date = str(header_fields.get("Bill Date") or first.get("Bill Date") or "").strip()
                    total_amount = sum(float(str(r.get("Line Item Charge") or "0").replace("$", "").replace(",", "")) for r in merged)
                    line_count = len(merged)
                    if vendor_id and property_id and account_number:
                        _write_account_history_record(pid0, vendor_id, property_id, account_number, bill_date, total_amount, line_count, utility_type)
                except Exception as hist_err:
                    print(f"[SUBMIT] Warning: Failed to write history record: {hist_err}")

                # Invalidate caches
                _CACHE.pop(("day_status_counts", y, m, d), None)
                _CACHE.pop(("parse_dashboard",), None)
                print(f"[SUBMIT] Background processing complete for {len(id_list)} lines")
            except Exception as e:
                print(f"[SUBMIT] Background processing error: {e}")
                import traceback
                traceback.print_exc()

        # Run heavy I/O in background if BackgroundTasks is available
        if background_tasks:
            background_tasks.add_task(do_submit_io)
            return {"ok": True, "sent": total_lines, "async": True}
        else:
            # Fallback to synchronous execution
            do_submit_io()
            return {"ok": True, "sent": total_lines}

    except Exception as e:
        import traceback
        tb = traceback.format_exc()
        return JSONResponse({"ok": False, "error": str(e), "trace": tb}, status_code=500)


@app.get("/pdf")
def pdf_proxy(u: str = "", k: str = "", date: str = "", pdf_id: str = ""):
    """Proxy endpoint that regenerates a fresh presigned URL for a given (possibly expired) PDF link.
    Accepts query param u=<original_or_short_url> and redirects to a new presigned URL.
    """
    # If an explicit S3 key was provided, use it
    if k:
        key = k.lstrip('/')
        # Handle accidental inclusion of bucket in key
        if key.startswith(f"{BUCKET}/"):
            key = key[len(BUCKET)+1:]
        bucket = BUCKET
        try:
            print(f"/pdf proxy (k): streaming bucket={bucket} key={key}")
            obj = s3.get_object(Bucket=bucket, Key=key)
            body = obj['Body']
            from starlette.responses import StreamingResponse
            base_name = os.path.basename(key) or 'document.pdf'
            headers = {
                'Content-Disposition': f'inline; filename="{base_name}"',
                'Content-Type': 'application/pdf',
                'Cache-Control': 'private, max-age=300',
            }
            return StreamingResponse(body.iter_chunks(chunk_size=8192), headers=headers, media_type='application/pdf')
        except Exception as e:
            return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)
    # If 'u' looks like a bare S3 key (not a URL), treat it as key
    if u and not (u.startswith('http://') or u.startswith('https://') or u.startswith('s3://')) and ('/' in u):
        key = u.lstrip('/')
        if key.startswith(f"{BUCKET}/"):
            key = key[len(BUCKET)+1:]
        bucket = BUCKET
        try:
            print(f"/pdf proxy: bucket={bucket} key={key} (u treated as key)")
            base_name = os.path.basename(key) or 'document.pdf'
            url = s3.generate_presigned_url(
                'get_object',
                Params={
                    'Bucket': bucket,
                    'Key': key,
                    'ResponseContentDisposition': f'inline; filename=\"{base_name}\"',
                    'ResponseContentType': 'application/pdf',
                },
                ExpiresIn=3600
            )
            return RedirectResponse(url)
        except Exception as e:
            return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)

    # Expand legacy short links before anything else
    if u:
        # normalize percent-encoding once, then expand short
        try:
            from urllib.parse import unquote as _unq
            u = _unq(u)
            # some callers double-encode; a second pass is safe
            u = _unq(u)
        except Exception:
            pass
        u = _maybe_expand_short(u)
    # Try to parse original first, but also resolve and try final (prefer final if S3)
    parsed_orig = _parse_s3_from_url(u)
    final = _resolve_final_url(u)
    parsed_final = _parse_s3_from_url(final)
    # If still not parsed and looks like Lambda Function URL, aggressively resolve via GET
    if not parsed_final and u and (".lambda-url." in (urlparse(u).netloc or "")):
        try:
            r = requests.get(u, allow_redirects=True, timeout=12, stream=False)
            final = r.url or final or u
            parsed_final = _parse_s3_from_url(final)
        except Exception:
            pass
    parsed = parsed_final or parsed_orig
    print(f"/pdf debug: u={u} final={final} parsed_orig={bool(parsed_orig)} parsed_final={bool(parsed_final)}")
    # Last-ditch S3 parse for virtual-hosted links if parser failed
    if not parsed and (final or u):
        candidate = final or u
        try:
            from urllib.parse import urlparse as _up
            _p = _up(candidate)
            _host = _p.netloc or ""
            _path = (_p.path or "").lstrip('/')
            if ".s3.amazonaws.com" in _host and _path:
                _bucket = _host.split('.s3.amazonaws.com', 1)[0]
                if _bucket:
                    parsed = (_bucket, unquote(_path))
                    print(f"/pdf debug: applied last-ditch s3 host parse bucket={_bucket}")
        except Exception:
            pass
    if parsed:
        bucket, key = parsed
        try:
            print(f"/pdf proxy: streaming bucket={bucket} key={key}")
            obj = s3.get_object(Bucket=bucket, Key=key)
            body = obj['Body']
            from starlette.responses import StreamingResponse
            base_name = os.path.basename(key) or 'document.pdf'
            headers = {
                'Content-Disposition': f'inline; filename="{base_name}"',
                'Content-Type': 'application/pdf',
                'Cache-Control': 'private, max-age=300',
            }
            return StreamingResponse(body.iter_chunks(chunk_size=8192), headers=headers, media_type='application/pdf')
        except Exception as e:
            return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)
    # Fallback: if caller provided date+pdf_id, infer the S3 key from enriched outputs
    try:
        if date and pdf_id:
            y, m, d = date.split("-")
            rows = load_day(y, m, d)
            # Narrow rows to this document id
            doc_rows = [r for r in rows if r.get("__s3_key__") and pdf_id_from_key(r.get("__s3_key__")) == pdf_id]
            key_guess = _infer_pdf_key_for_doc(y, m, d, doc_rows, pdf_id) if doc_rows else ""
            if not key_guess:
                # Extract filename pattern from doc_rows for fallback search
                # pdf_id is a SHA1 hash, but we need the actual filename pattern
                filename_pattern = ""
                if doc_rows:
                    # Try source_file_page first, then derive from source_input_key or __s3_key__
                    r0 = doc_rows[0]
                    filename_pattern = str(r0.get("source_file_page", "")).strip()
                    if not filename_pattern:
                        src_key = str(r0.get("source_input_key", "")).strip()
                        if src_key:
                            filename_pattern = os.path.basename(src_key)
                    if not filename_pattern:
                        s3_key = str(r0.get("__s3_key__", "")).strip()
                        if s3_key:
                            # Derive from JSONL filename: 20251224T001002Z_0573_001.jsonl -> 20251224T001002Z_0573_001
                            base = os.path.basename(s3_key)
                            if base.endswith(".jsonl"):
                                filename_pattern = base[:-6]  # Remove .jsonl extension

                # Fallback: global scan by filename pattern across known prefixes
                if filename_pattern:
                    try:
                        cands = []
                        search_pattern = filename_pattern.lower().replace(".pdf", "")
                        for pfx in (REWORK_PREFIX, INPUT_PREFIX, PARSED_INPUTS_PREFIX, HIST_ARCHIVE_PREFIX):
                            pag = s3.get_paginator("list_objects_v2")
                            for page in pag.paginate(Bucket=BUCKET, Prefix=pfx):
                                for obj in page.get("Contents", []) or []:
                                    k = obj.get("Key", ""); base = os.path.basename(k).lower()
                                    if base.endswith('.pdf') and search_pattern in base:
                                        cands.append((k, obj.get("LastModified")))
                        if cands:
                            cands.sort(key=lambda t: (t[1] or 0), reverse=True)
                            key_guess = cands[0][0]
                    except Exception:
                        pass
            if key_guess:
                    # key_guess may be an absolute URL or s3:// URI; normalize to (bucket,key)
                    tgt_bucket = BUCKET
                    key2 = key_guess
                    try:
                        if key_guess.startswith('http://') or key_guess.startswith('https://') or key_guess.startswith('s3://'):
                            parsed = _parse_s3_from_url(key_guess)
                            if parsed:
                                tgt_bucket, key2 = parsed[0], parsed[1]
                            else:
                                # fall back to path portion if possible
                                from urllib.parse import urlparse as _up
                                _p = _up(key_guess)
                                key2 = (_p.path or '').lstrip('/')
                        key2 = key2.lstrip('/')
                        if key2.startswith(f"{BUCKET}/"):
                            key2 = key2[len(BUCKET)+1:]
                    except Exception:
                        key2 = key_guess.lstrip('/')
                    print(f"/pdf fallback infer: bucket={tgt_bucket} key={key2}")
                    obj = s3.get_object(Bucket=tgt_bucket, Key=key2)
                    body = obj['Body']
                    from starlette.responses import StreamingResponse
                    base_name = os.path.basename(key2) or 'document.pdf'
                    headers = {
                        'Content-Disposition': f'inline; filename="{base_name}"',
                        'Content-Type': 'application/pdf',
                        'Cache-Control': 'private, max-age=300',
                    }
                    return StreamingResponse(body.iter_chunks(chunk_size=8192), headers=headers, media_type='application/pdf')
    except Exception:
        pass
    # If we can't parse to S3 at this point, fail clearly instead of redirecting to expired links
    print(f"/pdf error: non-s3 url after resolution u={u} final={final}")
    return JSONResponse({"error": "unable to parse s3 url"}, status_code=400)


# ============================================================================
# CHART BY METER - Meter Usage Tracking and Analysis
# ============================================================================

# Utility types to include in meter tracking (filter out phones, trash, etc.)
METER_UTILITY_TYPES = {
    "electricity", "electric", "vacant electricity",
    "gas", "vacant gas",
    "water", "vacant water",
    "sewer"
}

# UOM normalization mappings
UOM_NORMALIZATIONS = {
    # Electric -> kWh
    "kwh": ("kWh", "electric", 1.0),
    "kw/h": ("kWh", "electric", 1.0),
    "kilowatt hours": ("kWh", "electric", 1.0),
    "kilowatt-hours": ("kWh", "electric", 1.0),
    "kilowatthours": ("kWh", "electric", 1.0),
    "kilowatt hour": ("kWh", "electric", 1.0),
    "kw hours": ("kWh", "electric", 1.0),
    "kw-hours": ("kWh", "electric", 1.0),
    "kw": ("kW", "electric_demand", 1.0),  # Demand, not consumption
    "kilowatt": ("kW", "electric_demand", 1.0),
    "kilowatts": ("kW", "electric_demand", 1.0),

    # Gas -> Therms
    "therms": ("Therms", "gas", 1.0),
    "therm": ("Therms", "gas", 1.0),
    "th": ("Therms", "gas", 1.0),
    "ccf": ("Therms", "gas", 1.024),  # 1 CCF = 1.024 Therms
    "mcf": ("Therms", "gas", 10.24),  # 1 MCF = 10.24 Therms
    "100 cubic feet": ("Therms", "gas", 1.024),
    "cubic feet": ("Therms", "gas", 0.01024),
    "cu ft": ("Therms", "gas", 0.01024),
    "cuft": ("Therms", "gas", 0.01024),
    "cf": ("Therms", "gas", 0.01024),  # 1 CF = 0.01024 Therms
    "mmbtu": ("Therms", "gas", 10.0),  # 1 MMBTU = 10 Therms
    "mbtu": ("Therms", "gas", 0.1),  # 1 MBTU = 0.1 Therms
    "btu": ("Therms", "gas", 0.00001),  # 1 BTU = 0.00001 Therms
    "dth": ("Therms", "gas", 10.0),  # Dekatherms = 10 therms

    # Water -> Gallons
    "gallons": ("Gallons", "water", 1.0),
    "gallon": ("Gallons", "water", 1.0),
    "gal": ("Gallons", "water", 1.0),
    "gals": ("Gallons", "water", 1.0),
    "kgal": ("Gallons", "water", 1000.0),
    "kgals": ("Gallons", "water", 1000.0),
    "k gal": ("Gallons", "water", 1000.0),
    "1,000gal": ("Gallons", "water", 1000.0),
    "1000gal": ("Gallons", "water", 1000.0),
    "1000 gal": ("Gallons", "water", 1000.0),
    "1,000 gal": ("Gallons", "water", 1000.0),
    "1000 gallons": ("Gallons", "water", 1000.0),
    "1,000 gallons": ("Gallons", "water", 1000.0),
    "thousands of gallons": ("Gallons", "water", 1000.0),
    "thousands gallons": ("Gallons", "water", 1000.0),
    "thousand gallons": ("Gallons", "water", 1000.0),
    "000's gallons": ("Gallons", "water", 1000.0),
    "1000s of gallons": ("Gallons", "water", 1000.0),
    "100 gallons": ("Gallons", "water", 100.0),
    "100 gal": ("Gallons", "water", 100.0),
    "hundreds of gallons": ("Gallons", "water", 100.0),
    "hundred gallons": ("Gallons", "water", 100.0),
    "hcf": ("Gallons", "water", 748.05),  # 1 HCF = 748.05 gallons
    "ccf water": ("Gallons", "water", 748.05),
    "cubic feet water": ("Gallons", "water", 7.4805),
    "acre feet": ("Gallons", "water", 325851.0),
    "acre-feet": ("Gallons", "water", 325851.0),
    "af": ("Gallons", "water", 325851.0),

    # Sewer (often same as water)
    "sewer gallons": ("Gallons", "sewer", 1.0),

    # Units/Each - keep as-is but flag for review
    "units": ("Units", "unknown", 1.0),
    "unit(s)": ("Units", "unknown", 1.0),
    "unit": ("Units", "unknown", 1.0),
    "each": ("Each", "unknown", 1.0),
    "ea": ("Each", "unknown", 1.0),
}


def _normalize_uom(raw_uom: str) -> tuple:
    """
    Normalize a UOM string to canonical form.
    Returns (canonical_uom, utility_category, conversion_factor)
    """
    if not raw_uom:
        return (None, None, None)

    clean = raw_uom.lower().strip()

    # Direct match
    if clean in UOM_NORMALIZATIONS:
        return UOM_NORMALIZATIONS[clean]

    # Fuzzy match common patterns
    for pattern, result in UOM_NORMALIZATIONS.items():
        if pattern in clean:
            return result

    return (raw_uom, "unknown", 1.0)


def _normalize_meter_number(raw_meter: str) -> str:
    """Normalize a meter number by removing common formatting differences."""
    if not raw_meter:
        return ""
    # Remove spaces, dashes, leading zeros (but keep alphanumeric)
    import re
    clean = re.sub(r'[\s\-\.]', '', str(raw_meter).strip())
    # Remove leading zeros only if all numeric
    if clean.isdigit():
        clean = clean.lstrip('0') or '0'
    return clean.upper()


def _parse_consumption(raw_value) -> float:
    """Parse a consumption value to float, handling various formats."""
    if raw_value is None:
        return None
    if isinstance(raw_value, (int, float)):
        return float(raw_value)
    try:
        # Remove $, commas, spaces
        clean = str(raw_value).replace('$', '').replace(',', '').replace(' ', '').strip()
        if not clean or clean == '-':
            return None
        return float(clean)
    except (ValueError, TypeError):
        return None


def _get_meter_data_from_s3() -> dict:
    """Load existing meter data from S3."""
    key = f"{METER_DATA_PREFIX}meter_master.json.gz"
    try:
        txt = _read_s3_text(BUCKET, key)
        return json.loads(txt)
    except Exception:
        return {"meters": {}, "readings": [], "aliases": {}, "last_scan": None}


def _save_meter_data_to_s3(data: dict):
    """Save meter data to S3."""
    key = f"{METER_DATA_PREFIX}meter_master.json.gz"
    content = json.dumps(data, ensure_ascii=False)
    body = gzip.compress(content.encode('utf-8'))
    s3.put_object(Bucket=BUCKET, Key=key, Body=body, ContentType='application/json', ContentEncoding='gzip')


def _get_invoice_base_name(s3_key: str) -> str:
    """Extract base invoice name from S3 key (strip timestamps and path)."""
    filename = s3_key.split('/')[-1]
    if filename.endswith('.jsonl.gz'):
        filename = filename[:-9]
    elif filename.endswith('.jsonl'):
        filename = filename[:-6]
    parts = filename.split('_')
    base_parts = []
    for part in parts:
        if len(part) == 16 and len(part) > 8 and part[8] == 'T' and part.endswith('Z'):
            break
        base_parts.append(part)
    return '_'.join(base_parts) if base_parts else filename


def _get_invoice_timestamp(s3_key: str) -> str:
    """Extract the latest timestamp from S3 key for version sorting."""
    filename = s3_key.split('/')[-1]
    parts = filename.replace('.jsonl.gz', '').replace('.jsonl', '').split('_')
    timestamps = [p for p in parts if len(p) == 16 and len(p) > 8 and p[8:9] == 'T' and p.endswith('Z')]
    return max(timestamps) if timestamps else ""


def _parse_service_date(date_str: str):
    """Parse date string in various formats, return datetime or None."""
    if not date_str:
        return None
    for fmt in ["%m/%d/%Y", "%Y-%m-%d", "%m-%d-%Y", "%Y/%m/%d"]:
        try:
            return datetime.strptime(str(date_str).strip(), fmt)
        except Exception:
            continue
    return None


def _calculate_daily_rate(consumption: float, start_date: str, end_date: str) -> float:
    """Calculate daily consumption rate for a billing period."""
    start = _parse_service_date(start_date)
    end = _parse_service_date(end_date)
    if not start or not end:
        return 0.0
    days = (end - start).days
    if days <= 0:
        days = 1
    return consumption / days


# Default utility rates for cost impact analysis
DEFAULT_UTILITY_RATES = {
    "electric": {"rate": 0.12, "unit": "kWh"},
    "electricity": {"rate": 0.12, "unit": "kWh"},
    "vacant electricity": {"rate": 0.12, "unit": "kWh"},
    "gas": {"rate": 1.50, "unit": "Therms"},
    "vacant gas": {"rate": 1.50, "unit": "Therms"},
    "water": {"rate": 0.005, "unit": "Gallons"},
    "vacant water": {"rate": 0.005, "unit": "Gallons"},
    "sewer": {"rate": 0.006, "unit": "Gallons"},
}


def _compute_meter_flags(readings_list: list, utility_type: str = "") -> list:
    """Compute actionable flags for a meter based on reading patterns.

    Flags include:
    - volume_spike: Usage >50% above average (volume issue)
    - rate_increase: Effective rate >20% above average (rate issue)
    - drop: Usage >50% below average
    - gap: Missing period (>45 days between readings)

    Uses actual effective_rate from invoices for cost calculations.
    Returns list of flag dicts with type, severity, message, date, and optional cost_impact.
    """
    flags = []
    if len(readings_list) < 2:
        return flags

    # Calculate daily rates with dates AND effective rates from invoices
    daily_data = []
    for r in readings_list:
        # Get daily_rate if pre-computed, otherwise calculate from consumption and dates
        daily_rate = r.get("daily_rate", 0)
        date = r.get("date", "") or r.get("sort_key", "") or r.get("service_end", "") or r.get("reading_date", "")
        effective_rate = r.get("effective_rate", 0) or 0  # $/unit from invoice
        consumption = r.get("enriched_consumption", 0) or r.get("normalized_consumption", 0) or 0

        # Calculate daily_rate if not provided
        if daily_rate <= 0 and consumption > 0:
            start = r.get("service_start", "")
            end = r.get("service_end", "") or r.get("reading_date", "")
            if start and end:
                try:
                    # Handle both MM/DD/YYYY and YYYY-MM-DD formats
                    if "/" in start:
                        d1 = datetime.strptime(start, "%m/%d/%Y")
                    else:
                        d1 = datetime.strptime(start[:10], "%Y-%m-%d")
                    if "/" in end:
                        d2 = datetime.strptime(end, "%m/%d/%Y")
                    else:
                        d2 = datetime.strptime(end[:10], "%Y-%m-%d")
                    days = max(1, (d2 - d1).days)
                    daily_rate = consumption / days
                except Exception:
                    daily_rate = consumption / 30  # Default to 30 days if date parsing fails
            else:
                daily_rate = consumption / 30  # Default assumption

        # Calculate effective_rate if not provided (amount / consumption)
        if effective_rate <= 0:
            amount = r.get("amount", 0) or 0
            if amount > 0 and consumption > 0:
                effective_rate = amount / consumption

        if daily_rate > 0 and date:
            daily_data.append({
                "rate": daily_rate,
                "date": date,
                "effective_rate": effective_rate,
                "consumption": consumption
            })

    if len(daily_data) < 2:
        return flags

    # Sort by date - properly parse dates for correct ordering
    def parse_date_for_sort(date_str):
        try:
            if "/" in date_str:
                return datetime.strptime(date_str, "%m/%d/%Y")
            else:
                return datetime.strptime(date_str[:10], "%Y-%m-%d")
        except Exception:
            return datetime(1900, 1, 1)  # Invalid dates sort first

    daily_data.sort(key=lambda x: parse_date_for_sort(x["date"]))

    # Calculate averages
    avg_daily_rate = sum(d["rate"] for d in daily_data) / len(daily_data)

    # Calculate average effective rate (only from readings that have it)
    rates_with_effective = [d["effective_rate"] for d in daily_data if d["effective_rate"] > 0]
    avg_effective_rate = sum(rates_with_effective) / len(rates_with_effective) if rates_with_effective else 0

    # Check last 3 periods for issues
    for d in daily_data[-3:]:
        volume_pct_change = ((d["rate"] - avg_daily_rate) / avg_daily_rate * 100) if avg_daily_rate > 0 else 0
        rate_pct_change = ((d["effective_rate"] - avg_effective_rate) / avg_effective_rate * 100) if avg_effective_rate > 0 and d["effective_rate"] > 0 else 0

        # Check for rate increase (utility raised prices)
        if rate_pct_change > 20 and d["effective_rate"] > 0:
            # Cost impact from rate increase (same volume, higher rate)
            excess_rate = d["effective_rate"] - avg_effective_rate
            typical_monthly_consumption = avg_daily_rate * 30
            cost_impact = typical_monthly_consumption * excess_rate

            message = f"+{rate_pct_change:.0f}% rate"
            if cost_impact > 10:
                message = f"+{rate_pct_change:.0f}% rate (+${cost_impact:.0f}/mo)"

            flags.append({
                "type": "rate_increase",
                "severity": "high" if rate_pct_change > 50 else "medium",
                "message": message,
                "date": d["date"],
                "cost_impact": round(cost_impact, 2) if cost_impact > 10 else None,
                "category": "rate"
            })

        # Check for volume spike (usage increased)
        if volume_pct_change > 50:
            excess_daily = d["rate"] - avg_daily_rate
            excess_monthly = excess_daily * 30
            # Use actual effective rate if available, otherwise estimate
            unit_rate = d["effective_rate"] if d["effective_rate"] > 0 else (avg_effective_rate if avg_effective_rate > 0 else DEFAULT_UTILITY_RATES.get(utility_type, {}).get("rate", 0))
            cost_impact = excess_monthly * unit_rate

            message = f"+{volume_pct_change:.0f}% usage"
            if cost_impact > 10:
                message = f"+{volume_pct_change:.0f}% usage (+${cost_impact:.0f}/mo)"

            flags.append({
                "type": "volume_spike",
                "severity": "high" if volume_pct_change > 100 else "medium",
                "message": message,
                "date": d["date"],
                "cost_impact": round(cost_impact, 2) if cost_impact > 10 else None,
                "category": "volume"
            })
        elif volume_pct_change < -50 and d["rate"] > 0:
            flags.append({
                "type": "drop",
                "severity": "medium",
                "message": f"-{abs(volume_pct_change):.0f}% usage",
                "date": d["date"],
                "category": "volume"
            })

    # Check for gaps (>45 days between readings) - data issue
    for i in range(1, len(daily_data)):
        try:
            d1_str = daily_data[i-1]["date"]
            d2_str = daily_data[i]["date"]
            # Handle both MM/DD/YYYY and YYYY-MM-DD formats
            if "/" in d1_str:
                d1 = datetime.strptime(d1_str, "%m/%d/%Y")
            else:
                d1 = datetime.strptime(d1_str[:10], "%Y-%m-%d")
            if "/" in d2_str:
                d2 = datetime.strptime(d2_str, "%m/%d/%Y")
            else:
                d2 = datetime.strptime(d2_str[:10], "%Y-%m-%d")
            gap = (d2 - d1).days
            if gap > 45:
                flags.append({
                    "type": "gap",
                    "severity": "medium",
                    "message": f"{gap}d gap",
                    "date": daily_data[i]["date"],
                    "category": "data"
                })
        except Exception:
            pass

    return flags


def _add_sparklines_to_meters(meters: dict, all_readings: list) -> dict:
    """Add sparkline data to each meter based on readings - using daily consumption rate."""

    # Step 1: Dedupe readings by invoice base name - keep only latest version
    invoice_versions = {}
    for r in all_readings:
        s3_key = r.get("source_s3_key", "")
        base_name = _get_invoice_base_name(s3_key)
        timestamp = _get_invoice_timestamp(s3_key)

        if base_name not in invoice_versions:
            invoice_versions[base_name] = {"timestamp": timestamp, "s3_key": s3_key, "readings": []}

        if timestamp >= invoice_versions[base_name]["timestamp"]:
            if timestamp > invoice_versions[base_name]["timestamp"]:
                invoice_versions[base_name] = {"timestamp": timestamp, "s3_key": s3_key, "readings": []}
            invoice_versions[base_name]["readings"].append(r)

    # Flatten to just the latest readings
    latest_readings = []
    for inv in invoice_versions.values():
        latest_readings.extend(inv["readings"])

    # Step 2: Group by meter
    meter_readings = {}
    for r in latest_readings:
        meter_id = r.get("canonical_meter_id")
        if not meter_id:
            continue
        if meter_id not in meter_readings:
            meter_readings[meter_id] = []

        consumption = r.get("enriched_consumption") or r.get("normalized_consumption") or r.get("consumption") or 0
        service_start = r.get("service_start") or ""
        service_end = r.get("service_end") or r.get("reading_date") or ""
        daily_rate = _calculate_daily_rate(consumption, service_start, service_end)

        end_dt = _parse_service_date(service_end)
        sort_key = end_dt.strftime("%Y-%m-%d") if end_dt else ""

        meter_readings[meter_id].append({
            "date": service_end,
            "sort_key": sort_key,
            "daily_rate": daily_rate,
            "total_consumption": consumption,
            "source_s3_key": r.get("source_s3_key"),
            "line_index": r.get("line_index")
        })

    # Step 3: For each meter, check for discrepancies
    for meter_id, readings_list in meter_readings.items():
        if meter_id not in meters:
            continue

        # Group by service date
        date_groups = {}
        for r in readings_list:
            key = r["sort_key"]
            if not key:
                continue
            if key not in date_groups:
                date_groups[key] = []
            date_groups[key].append(r)

        # Check for discrepancies - different consumption values on same date
        discrepancies = []
        for date, items in date_groups.items():
            if len(items) > 1:
                values = list(set(r["total_consumption"] for r in items if r["total_consumption"] > 0))
                if len(values) > 1:
                    discrepancies.append({
                        "date": date,
                        "values": sorted(values),
                        "min": min(values),
                        "max": max(values),
                        "count": len(items)
                    })

        if discrepancies:
            meters[meter_id]["has_discrepancies"] = True
            meters[meter_id]["discrepancies"] = discrepancies
            meters[meter_id]["discrepancy_count"] = len(discrepancies)

        # For sparkline, use most recent reading per date (handles duplicates)
        date_totals = {}
        for date, items in date_groups.items():
            # Take the last (most recent) reading for this date
            latest = items[-1]
            date_totals[date] = {"total": latest["total_consumption"], "daily_rate": latest["daily_rate"], "date": latest["date"]}

        # Sort by date and take last 12 periods
        sorted_dates = sorted(date_totals.keys())[-12:]
        meters[meter_id]["sparkline"] = [round(date_totals[d]["daily_rate"], 2) for d in sorted_dates]
        meters[meter_id]["sparkline_dates"] = [date_totals[d]["date"] for d in sorted_dates]

        if sorted_dates:
            last_date = sorted_dates[-1]
            meters[meter_id]["latest_daily_rate"] = round(date_totals[last_date]["daily_rate"], 2)
            meters[meter_id]["latest_consumption"] = date_totals[last_date]["total"]
            meters[meter_id]["latest_date"] = date_totals[last_date]["date"]

        # Compute actionable flags for this meter
        utility_type = meters[meter_id].get("utility_type", "")
        flags = _compute_meter_flags(readings_list, utility_type)
        if flags:
            meters[meter_id]["flags"] = flags

    return meters


@app.get("/api/meters/scan")
def api_meters_scan(user: str = Depends(require_user)):
    """
    Trigger incremental scan via Lambda (files modified in last day).
    """
    print("[METER SCAN] Triggering incremental scan via Lambda...")

    try:
        lambda_client = boto3.client("lambda", region_name="us-east-1")
        response = lambda_client.invoke(
            FunctionName="jrk-meter-cleaner",
            InvocationType="RequestResponse",
            Payload=json.dumps({"action": "scan", "days_back": 1})
        )
        result = json.loads(response["Payload"].read().decode("utf-8"))

        if "body" in result:
            body = json.loads(result["body"]) if isinstance(result["body"], str) else result["body"]
            return {
                "ok": True,
                "new_meters": body.get("total_meters", 0),
                "new_readings": body.get("new_readings", 0),
                "total_meters": body.get("total_meters", 0),
                "total_readings": body.get("total_readings", 0),
                "message": body.get("message", "Scan complete")
            }
        return {"ok": True, **result}
    except Exception as e:
        print(f"[METER SCAN] Lambda error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


def _api_meters_scan_local(user: str):
    """
    Local scan implementation (kept for reference, not used).
    Scans Stage 7 and Stage 8 for meter/consumption data.
    """
    print("[METER SCAN] Starting local scan of Stage 7 and Stage 8...")

    # Load existing data
    meter_data = _get_meter_data_from_s3()
    existing_reading_keys = set()
    for r in meter_data.get("readings", []):
        # Create unique key for deduplication
        rkey = f"{r.get('source_s3_key')}|{r.get('line_index', 0)}"
        existing_reading_keys.add(rkey)

    meters = meter_data.get("meters", {})
    readings = meter_data.get("readings", [])
    aliases = meter_data.get("aliases", {})

    new_readings = 0
    new_meters = 0
    skipped = 0

    # Scan Stage 7 (Posted) and Stage 8 (UBI Assigned)
    prefixes = [POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX]

    for prefix in prefixes:
        print(f"[METER SCAN] Scanning {prefix}...")
        try:
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    key = obj['Key']
                    if not (key.endswith('.jsonl') or key.endswith('.jsonl.gz')):
                        continue

                    try:
                        txt = _read_s3_text(BUCKET, key)
                        line_index = 0
                        for line in txt.strip().split('\n'):
                            if not line.strip():
                                continue

                            rec = json.loads(line)
                            line_index += 1

                            # Check deduplication
                            rkey = f"{key}|{line_index}"
                            if rkey in existing_reading_keys:
                                skipped += 1
                                continue

                            # Filter to relevant utility types
                            utility = (rec.get('Utility Type') or rec.get('Utility Name') or '').strip()
                            if utility.lower() not in METER_UTILITY_TYPES:
                                continue

                            # Extract meter data
                            raw_meter = (rec.get('Meter Number') or '').strip()
                            if not raw_meter:
                                continue

                            raw_consumption = rec.get('Consumption Amount') or rec.get('ENRICHED CONSUMPTION')
                            raw_uom = (rec.get('Unit of Measure') or rec.get('ENRICHED UOM') or '').strip()
                            raw_charge = rec.get('Line Item Charge') or rec.get('Amount') or rec.get('AMOUNT') or 0

                            consumption = _parse_consumption(raw_consumption)
                            if consumption is None:
                                continue

                            # Parse charge amount
                            charge_amount = 0.0
                            try:
                                if isinstance(raw_charge, str):
                                    charge_amount = float(raw_charge.replace("$", "").replace(",", "").strip() or 0)
                                else:
                                    charge_amount = float(raw_charge or 0)
                            except (ValueError, TypeError):
                                charge_amount = 0.0

                            # Normalize
                            normalized_meter = _normalize_meter_number(raw_meter)
                            canonical_uom, uom_category, conversion = _normalize_uom(raw_uom)
                            normalized_consumption = consumption * conversion if conversion else consumption

                            # Get property and date info
                            property_id = rec.get('EnrichedPropertyID') or rec.get('Property ID') or ''
                            property_name = rec.get('EnrichedPropertyName') or rec.get('Property Name') or ''
                            service_start = rec.get('Bill Period Start') or rec.get('Service Start') or ''
                            service_end = rec.get('Bill Period End') or rec.get('Current Reading Date') or ''
                            account = rec.get('Account Number') or ''
                            vendor = rec.get('EnrichedVendorName') or rec.get('Vendor Name') or ''

                            # Normalize utility type
                            utility_normalized = utility.lower().replace('vacant ', '')
                            if utility_normalized in ('electricity', 'electric'):
                                utility_normalized = 'electric'

                            # Create or find canonical meter
                            meter_key = f"{property_id}|{utility_normalized}|{normalized_meter}"

                            if meter_key not in meters:
                                meters[meter_key] = {
                                    "canonical_meter_id": meter_key,
                                    "property_id": str(property_id),
                                    "property_name": property_name,
                                    "utility_type": utility_normalized,
                                    "normalized_meter_number": normalized_meter,
                                    "display_name": f"{property_name} - {utility_normalized.title()} - {normalized_meter}",
                                    "canonical_uom": canonical_uom,
                                    "created_date": datetime.utcnow().isoformat(),
                                    "reading_count": 0
                                }
                                new_meters += 1

                            # Track alias
                            if raw_meter != normalized_meter:
                                aliases[raw_meter] = meter_key

                            # Calculate effective rate ($/unit)
                            effective_rate = charge_amount / normalized_consumption if normalized_consumption > 0 else 0.0

                            # Get PDF key from source_input_key (preferred) or derive from source_file_page
                            source_input_key = rec.get("source_input_key", "") or ""
                            if not source_input_key:
                                # Try to derive from source_file_page
                                source_file_page = rec.get("source_file_page", "") or ""
                                if source_file_page:
                                    source_input_key = f"Bill_Parser_2_Parsed_Inputs/{source_file_page}"

                            # Add reading
                            reading = {
                                "canonical_meter_id": meter_key,
                                "reading_date": service_end,
                                "service_start": service_start,
                                "service_end": service_end,
                                "raw_consumption": consumption,  # The parsed numeric consumption before UOM conversion
                                "consumption": consumption,
                                "normalized_consumption": normalized_consumption,
                                "enriched_consumption": normalized_consumption,  # Same as normalized, after UOM conversion
                                "charge_amount": charge_amount,  # Dollar amount from invoice
                                "amount": charge_amount,  # Alias used by other code
                                "effective_rate": round(effective_rate, 4),  # $/unit
                                "raw_uom": raw_uom,
                                "canonical_uom": canonical_uom,
                                "normalized_uom": canonical_uom,  # Alias for clarity
                                "conversion_factor": conversion,  # Factor applied to get normalized consumption
                                "account_number": account,
                                "vendor_name": vendor,
                                "vendor": vendor,
                                "source_s3_key": key,
                                "source_input_key": source_input_key,  # Direct PDF S3 key
                                "source_pdf_id": pdf_id_from_key(key),
                                "line_index": line_index,
                                "flagged": False,
                                "flag_reason": None
                            }

                            # Flag outliers (basic check)
                            if consumption < 0:
                                reading["flagged"] = True
                                reading["flag_reason"] = "Negative consumption"
                            elif consumption == 0:
                                reading["flagged"] = True
                                reading["flag_reason"] = "Zero consumption"

                            readings.append(reading)
                            existing_reading_keys.add(rkey)
                            meters[meter_key]["reading_count"] = meters[meter_key].get("reading_count", 0) + 1
                            new_readings += 1

                    except Exception as e:
                        print(f"[METER SCAN] Error processing {key}: {e}")
                        continue

        except Exception as e:
            print(f"[METER SCAN] Error scanning {prefix}: {e}")

    # Add sparkline data to meters for visualization
    print(f"[METER SCAN] Computing sparklines for {len(meters)} meters...")
    meters = _add_sparklines_to_meters(meters, readings)

    # Save updated data
    meter_data = {
        "meters": meters,
        "readings": readings,
        "aliases": aliases,
        "last_scan": datetime.utcnow().isoformat(),
        "stats": {
            "total_meters": len(meters),
            "total_readings": len(readings),
            "total_aliases": len(aliases)
        }
    }
    _save_meter_data_to_s3(meter_data)

    print(f"[METER SCAN] Complete: {new_meters} new meters, {new_readings} new readings, {skipped} skipped (duplicates)")

    return {
        "ok": True,
        "new_meters": new_meters,
        "new_readings": new_readings,
        "skipped": skipped,
        "total_meters": len(meters),
        "total_readings": len(readings)
    }


@app.get("/api/meters/list")
def api_meters_list(
    property_id: str = "",
    utility_type: str = "",
    state: str = "",
    user: str = Depends(require_user)
):
    """List all canonical meters with optional filtering."""
    meter_data = _get_meter_data_from_s3()
    meters = list(meter_data.get("meters", {}).values())

    # Load property data for state lookup if filtering by state
    property_states = {}
    if state:
        try:
            prop_rows = _load_dim_records(DIM_PROPERTY_PREFIX)
            for p in prop_rows:
                pid = str(p.get("PROPERTY_ID", ""))
                pstate = str(p.get("GEO_STATE", "")).strip().upper()
                if pid and pstate:
                    property_states[pid] = pstate
        except Exception as e:
            print(f"[METERS] Warning: Could not load property states: {e}")

    # Filter
    if property_id:
        meters = [m for m in meters if m.get("property_id") == property_id]
    if utility_type:
        # Handle both "electric" and "electricity" variants
        ut = utility_type.lower()
        if ut == "electric":
            meters = [m for m in meters if m.get("utility_type") in ("electric", "electricity")]
        else:
            meters = [m for m in meters if m.get("utility_type") == ut]
    if state and property_states:
        state_upper = state.upper()
        meters = [m for m in meters if property_states.get(m.get("property_id", "")) == state_upper]

    # Add state to each meter for display
    if property_states:
        for m in meters:
            m["state"] = property_states.get(m.get("property_id", ""), "")

    # Sort by property name, then utility type
    meters.sort(key=lambda m: (m.get("property_name", ""), m.get("utility_type", ""), m.get("normalized_meter_number", "")))

    # Get list of available states for filter dropdown
    all_states = sorted(set(property_states.values())) if property_states else []

    return {
        "meters": meters,
        "total": len(meters),
        "last_scan": meter_data.get("last_scan"),
        "available_states": all_states
    }


@app.get("/api/meters/{meter_id}/readings")
def api_meter_readings(meter_id: str, user: str = Depends(require_user)):
    """Get all readings for a specific meter."""
    meter_data = _get_meter_data_from_s3()

    # URL decode the meter_id (it contains | characters)
    from urllib.parse import unquote
    meter_id = unquote(meter_id)

    meter = meter_data.get("meters", {}).get(meter_id)
    if not meter:
        return JSONResponse({"error": "Meter not found"}, status_code=404)

    readings = [r for r in meter_data.get("readings", []) if r.get("canonical_meter_id") == meter_id]

    # Ensure source_pdf_id is set for each reading (compute from source_s3_key if missing)
    for r in readings:
        if not r.get("source_pdf_id") and r.get("source_s3_key"):
            r["source_pdf_id"] = pdf_id_from_key(r["source_s3_key"])

    # Sort by date
    readings.sort(key=lambda r: r.get("reading_date", ""))

    return {
        "meter": meter,
        "readings": readings,
        "total": len(readings)
    }


@app.get("/api/meters/stats")
def api_meters_stats(user: str = Depends(require_user)):
    """Get overall meter statistics."""
    meter_data = _get_meter_data_from_s3()
    meters = meter_data.get("meters", {})
    readings = meter_data.get("readings", [])

    # Stats by utility type
    by_utility = {}
    for m in meters.values():
        ut = m.get("utility_type", "unknown")
        if ut not in by_utility:
            by_utility[ut] = {"meters": 0, "readings": 0}
        by_utility[ut]["meters"] += 1

    for r in readings:
        meter = meters.get(r.get("canonical_meter_id", ""), {})
        ut = meter.get("utility_type", "unknown")
        if ut in by_utility:
            by_utility[ut]["readings"] += 1

    # Flagged readings
    flagged = [r for r in readings if r.get("flagged")]

    # Meters with discrepancies (different consumption values for same service date)
    discrepancy_meters = [m for m in meters.values() if m.get("has_discrepancies")]

    # Properties with meters
    properties = set(m.get("property_id") for m in meters.values() if m.get("property_id"))

    return {
        "total_meters": len(meters),
        "total_readings": len(readings),
        "total_properties": len(properties),
        "by_utility": by_utility,
        "flagged_count": len(discrepancy_meters),  # Show meters with discrepancies
        "last_scan": meter_data.get("last_scan")
    }


@app.get("/api/meters/flagged")
def api_meters_flagged(user: str = Depends(require_user)):
    """Get meters with discrepancies (different consumption values for same service date)."""
    meter_data = _get_meter_data_from_s3()
    meters = meter_data.get("meters", {})
    readings = meter_data.get("readings", [])

    # Build index of readings by meter for quick lookup
    readings_by_meter = {}
    for r in readings:
        mid = r.get("canonical_meter_id")
        if mid:
            if mid not in readings_by_meter:
                readings_by_meter[mid] = []
            readings_by_meter[mid].append(r)

    flagged = []
    for meter_id, meter in meters.items():
        if not meter.get("has_discrepancies"):
            continue

        meter_readings = readings_by_meter.get(meter_id, [])
        if not meter_readings:
            continue

        # Recompute discrepancies directly from readings (more robust)
        # Group readings by service date (use service_end or reading_date)
        by_date = {}
        for r in meter_readings:
            date_key = r.get("service_end") or r.get("reading_date") or ""
            if not date_key:
                continue
            if date_key not in by_date:
                by_date[date_key] = []
            by_date[date_key].append(r)

        # Find dates with different consumption values
        for date_key, date_readings in by_date.items():
            if len(date_readings) < 2:
                continue

            # Get unique consumption values for this date
            consumptions = set()
            for r in date_readings:
                c = r.get("enriched_consumption") or r.get("normalized_consumption") or 0
                consumptions.add(c)

            # If multiple different values, this is a discrepancy
            if len(consumptions) > 1:
                values_str = ", ".join(f"{v:,.0f}" for v in sorted(consumptions)[:3])
                for r in date_readings:
                    # Ensure source_pdf_id is set
                    if not r.get("source_pdf_id") and r.get("source_s3_key"):
                        r["source_pdf_id"] = pdf_id_from_key(r["source_s3_key"])

                    flagged.append({
                        **r,
                        "property_name": meter.get("property_name"),
                        "utility_type": meter.get("utility_type"),
                        "meter_display": meter.get("normalized_meter_number") or meter.get("display_name"),
                        "flag_reason": f"Discrepancy: {len(consumptions)} values ({values_str})",
                        "consumption": r.get("enriched_consumption") or r.get("normalized_consumption"),
                        "reading_date": date_key,
                        "flagged": True
                    })

    return {"flagged": flagged, "total": len(flagged)}


@app.post("/api/meters/reading/update")
async def api_meter_reading_update(request: Request, user: str = Depends(require_user)):
    """Update a meter reading (correct consumption, UOM, or flag status)."""
    try:
        payload = await request.json()
        source_s3_key = payload.get("source_s3_key")
        line_index = payload.get("line_index")

        if not source_s3_key or line_index is None:
            return JSONResponse({"error": "source_s3_key and line_index required"}, status_code=400)

        meter_data = _get_meter_data_from_s3()
        readings = meter_data.get("readings", [])

        # Find and update the reading
        updated = False
        for r in readings:
            if r.get("source_s3_key") == source_s3_key and r.get("line_index") == line_index:
                if "consumption" in payload:
                    r["consumption"] = float(payload["consumption"])
                    r["normalized_consumption"] = float(payload["consumption"])
                    r["manually_corrected"] = True
                if "flagged" in payload:
                    r["flagged"] = bool(payload["flagged"])
                if "flag_reason" in payload:
                    r["flag_reason"] = payload["flag_reason"]
                r["updated_by"] = user
                r["updated_date"] = datetime.utcnow().isoformat()
                updated = True
                break

        if not updated:
            return JSONResponse({"error": "Reading not found"}, status_code=404)

        _save_meter_data_to_s3(meter_data)
        return {"ok": True}

    except Exception as e:
        print(f"[METER UPDATE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/meters/properties")
def api_meters_properties(user: str = Depends(require_user)):
    """Get list of properties that have meter data."""
    meter_data = _get_meter_data_from_s3()
    meters = meter_data.get("meters", {})

    properties = {}
    for m in meters.values():
        pid = m.get("property_id")
        pname = m.get("property_name")
        if pid and pid not in properties:
            properties[pid] = {"id": pid, "name": pname, "meter_count": 0}
        if pid:
            properties[pid]["meter_count"] += 1

    props_list = sorted(properties.values(), key=lambda p: p.get("name", ""))
    return {"properties": props_list}


@app.get("/api/meters/duplicates")
def api_meters_duplicates(user: str = Depends(require_user)):
    """Find potential duplicate meters based on similar meter numbers."""
    meter_data = _get_meter_data_from_s3()
    meters = meter_data.get("meters", {})

    # Group meters by normalized name (stripped of common variations)
    def normalize_for_comparison(s):
        if not s:
            return ""
        # Remove common separators and normalize
        s = s.upper().replace("-", "").replace(" ", "").replace("_", "")
        # Strip leading zeros
        s = s.lstrip("0") or "0"
        return s

    # Find meters at same property with similar numbers
    duplicates = []
    meters_by_property = {}
    for meter_id, meter in meters.items():
        pid = meter.get("property_id", "unknown")
        if pid not in meters_by_property:
            meters_by_property[pid] = []
        meters_by_property[pid].append((meter_id, meter))

    for pid, property_meters in meters_by_property.items():
        if len(property_meters) < 2:
            continue

        # Compare each pair
        for i, (id1, m1) in enumerate(property_meters):
            for id2, m2 in property_meters[i+1:]:
                norm1 = normalize_for_comparison(m1.get("display_name", ""))
                norm2 = normalize_for_comparison(m2.get("display_name", ""))

                # Check if same utility type and similar meter number
                if m1.get("utility_type") == m2.get("utility_type"):
                    # Check for similarity (one is prefix of another, or Levenshtein-like)
                    if norm1 == norm2 or norm1.startswith(norm2) or norm2.startswith(norm1):
                        duplicates.append({
                            "property_id": pid,
                            "property_name": m1.get("property_name"),
                            "utility_type": m1.get("utility_type"),
                            "meter_1": {"id": id1, "display_name": m1.get("display_name"), "reading_count": m1.get("reading_count", 0)},
                            "meter_2": {"id": id2, "display_name": m2.get("display_name"), "reading_count": m2.get("reading_count", 0)}
                        })

    return {"duplicates": duplicates, "total": len(duplicates)}


@app.post("/api/meters/merge")
async def api_meters_merge(request: Request, user: str = Depends(require_user)):
    """Merge multiple meters into one (combine all readings under target meter)."""
    try:
        payload = await request.json()
        target_meter_id = payload.get("target_meter_id")
        source_meter_ids = payload.get("source_meter_ids", [])

        if not target_meter_id or not source_meter_ids:
            return JSONResponse({"error": "target_meter_id and source_meter_ids required"}, status_code=400)

        meter_data = _get_meter_data_from_s3()
        meters = meter_data.get("meters", {})
        readings = meter_data.get("readings", [])

        if target_meter_id not in meters:
            return JSONResponse({"error": "Target meter not found"}, status_code=404)

        # Update readings from source meters to point to target
        readings_moved = 0
        for r in readings:
            if r.get("meter_id") in source_meter_ids:
                r["meter_id"] = target_meter_id
                r["merged_from"] = r.get("meter_id")
                r["merged_by"] = user
                r["merged_date"] = datetime.utcnow().isoformat()
                readings_moved += 1

        # Remove source meters
        for src_id in source_meter_ids:
            if src_id in meters:
                del meters[src_id]

        # Update target meter reading count
        target_readings = [r for r in readings if r.get("meter_id") == target_meter_id]
        meters[target_meter_id]["reading_count"] = len(target_readings)

        _save_meter_data_to_s3(meter_data)

        return {"ok": True, "readings_moved": readings_moved, "meters_removed": len(source_meter_ids)}

    except Exception as e:
        print(f"[METER MERGE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/meters/bulk-dismiss")
async def api_meters_bulk_dismiss(request: Request, user: str = Depends(require_user)):
    """Dismiss multiple flagged readings at once."""
    try:
        payload = await request.json()
        reading_keys = payload.get("reading_keys", [])  # List of {source_s3_key, line_index}

        if not reading_keys:
            return JSONResponse({"error": "reading_keys required"}, status_code=400)

        meter_data = _get_meter_data_from_s3()
        readings = meter_data.get("readings", [])

        # Build lookup set for fast matching
        keys_to_dismiss = set()
        for rk in reading_keys:
            keys_to_dismiss.add(f"{rk.get('source_s3_key')}|{rk.get('line_index')}")

        dismissed_count = 0
        for r in readings:
            key = f"{r.get('source_s3_key')}|{r.get('line_index')}"
            if key in keys_to_dismiss and r.get("flagged"):
                r["flagged"] = False
                r["flag_dismissed_by"] = user
                r["flag_dismissed_date"] = datetime.utcnow().isoformat()
                dismissed_count += 1

        _save_meter_data_to_s3(meter_data)

        return {"ok": True, "dismissed": dismissed_count}

    except Exception as e:
        print(f"[BULK DISMISS] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/meters/{meter_id}/update")
async def api_meter_update(meter_id: str, request: Request, user: str = Depends(require_user)):
    """Update meter metadata (display name, property assignment)."""
    try:
        payload = await request.json()

        meter_data = _get_meter_data_from_s3()
        meters = meter_data.get("meters", {})

        if meter_id not in meters:
            return JSONResponse({"error": "Meter not found"}, status_code=404)

        meter = meters[meter_id]

        if "display_name" in payload:
            meter["display_name"] = payload["display_name"]
        if "property_id" in payload:
            meter["property_id"] = payload["property_id"]
        if "property_name" in payload:
            meter["property_name"] = payload["property_name"]
        if "notes" in payload:
            meter["notes"] = payload["notes"]

        meter["updated_by"] = user
        meter["updated_date"] = datetime.utcnow().isoformat()

        _save_meter_data_to_s3(meter_data)

        return {"ok": True}

    except Exception as e:
        print(f"[METER UPDATE] Error: {e}")
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.post("/api/meters/ai-clean")
async def api_meters_ai_clean(request: Request, user: str = Depends(require_user)):
    """Trigger AI-powered meter data analysis and cleaning suggestions."""
    try:
        payload = await request.json() if request.headers.get("content-type") == "application/json" else {}
        auto_apply_uom = payload.get("auto_apply_uom", False)

        meter_data = _get_meter_data_from_s3()
        meters = meter_data.get("meters", {})
        readings = meter_data.get("readings", [])

        if not meters:
            return {"ok": True, "message": "No meter data to analyze", "issues": 0}

        # Find duplicate candidates
        def normalize_for_comparison(s):
            if not s:
                return ""
            s = s.upper().replace("-", "").replace(" ", "").replace("_", "")
            s = s.lstrip("0") or "0"
            return s

        duplicate_candidates = []
        meters_by_property = {}
        for meter_id, meter in meters.items():
            key = f"{meter.get('property_id', 'unknown')}|{meter.get('utility_type', 'unknown')}"
            if key not in meters_by_property:
                meters_by_property[key] = []
            meters_by_property[key].append((meter_id, meter))

        for group_key, group_meters in meters_by_property.items():
            if len(group_meters) < 2:
                continue
            for i, (id1, m1) in enumerate(group_meters):
                for id2, m2 in group_meters[i+1:]:
                    norm1 = normalize_for_comparison(m1.get("display_name", ""))
                    norm2 = normalize_for_comparison(m2.get("display_name", ""))
                    if norm1 == norm2 or norm1.startswith(norm2) or norm2.startswith(norm1):
                        duplicate_candidates.append({
                            "meter_1_id": id1,
                            "meter_1_name": m1.get("display_name"),
                            "meter_2_id": id2,
                            "meter_2_name": m2.get("display_name"),
                            "property_name": m1.get("property_name"),
                            "utility_type": m1.get("utility_type")
                        })

        # Find UOM issues
        uom_issues = []
        for r in readings:
            if not r.get("raw_uom") or r.get("normalized_uom") == "Unknown":
                uom_issues.append({
                    "meter_id": r.get("meter_id"),
                    "raw_uom": r.get("raw_uom", ""),
                    "utility_type": r.get("utility_type", "")
                })

        # Try to use Gemini for analysis (if available)
        gemini_suggestions = None
        try:
            import google.generativeai as genai
            from config import get_secret

            secret = get_secret("gemini/matcher-keys")
            if secret:
                api_key = secret.get("api_key") or secret.get("GEMINI_API_KEY")
                if api_key:
                    genai.configure(api_key=api_key)
                    model = genai.GenerativeModel("gemini-1.5-flash")

                    # Build analysis prompt
                    unique_uoms = list(set(i["raw_uom"] for i in uom_issues[:50] if i["raw_uom"]))
                    prompt = f"""Analyze these utility meter UOM values and suggest corrections:
{json.dumps(unique_uoms, indent=2)}

Duplicate meter candidates (same property, similar numbers):
{json.dumps(duplicate_candidates[:10], indent=2)}

Respond in JSON:
{{"uom_corrections": [{{"raw": "original", "corrected": "standard", "factor": 1.0}}], "observations": "notes"}}"""

                    response = model.generate_content(prompt)
                    text = response.text.strip()
                    if "```json" in text:
                        text = text.split("```json")[1].split("```")[0].strip()
                    elif "```" in text:
                        text = text.split("```")[1].split("```")[0].strip()
                    gemini_suggestions = json.loads(text)
        except Exception as e:
            print(f"[AI CLEAN] Gemini error: {e}")

        # Store pending suggestions in meter data
        if "ai_suggestions" not in meter_data:
            meter_data["ai_suggestions"] = {}

        meter_data["ai_suggestions"]["last_run"] = datetime.utcnow().isoformat()
        meter_data["ai_suggestions"]["duplicate_candidates"] = duplicate_candidates
        meter_data["ai_suggestions"]["uom_issues_count"] = len(uom_issues)
        if gemini_suggestions:
            meter_data["ai_suggestions"]["gemini_response"] = gemini_suggestions

        # Try to save but don't fail if S3 write is denied (AppRunner may lack s3:PutObject)
        save_ok = True
        try:
            _save_meter_data_to_s3(meter_data)
        except Exception as save_err:
            print(f"[AI CLEAN] Warning: Could not save suggestions to S3: {save_err}")
            save_ok = False

        return {
            "ok": True,
            "duplicate_candidates": len(duplicate_candidates),
            "uom_issues": len(uom_issues),
            "gemini_available": gemini_suggestions is not None,
            "observations": gemini_suggestions.get("observations") if gemini_suggestions else None,
            "save_warning": None if save_ok else "Suggestions computed but not persisted (S3 write permission needed)"
        }

    except Exception as e:
        print(f"[AI CLEAN] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/meters/ai-suggestions")
def api_meters_ai_suggestions(user: str = Depends(require_user)):
    """Get the latest AI cleaning suggestions."""
    meter_data = _get_meter_data_from_s3()
    suggestions = meter_data.get("ai_suggestions", {})
    return {
        "last_run": suggestions.get("last_run"),
        "duplicate_candidates": suggestions.get("duplicate_candidates", []),
        "uom_issues_count": suggestions.get("uom_issues_count", 0),
        "gemini_response": suggestions.get("gemini_response")
    }


@app.get("/api/meters/trends")
def api_meters_trends(
    level: str = "portfolio",  # portfolio, state, property
    state: str = "",
    property_id: str = "",
    utility_type: str = "",
    months: int = 12,
    user: str = Depends(require_user)
):
    """Get MoM consumption trends aggregated by level."""
    meter_data = _get_meter_data_from_s3()
    readings = meter_data.get("readings", [])
    meters = meter_data.get("meters", {})

    # Filter by utility type if specified
    if utility_type:
        ut_lower = utility_type.lower()
        # Include vacant variants
        ut_variants = {ut_lower, f"vacant {ut_lower}"}
        if ut_lower == "electric":
            ut_variants.add("electricity")
            ut_variants.add("vacant electricity")
        meter_ids = {mid for mid, m in meters.items()
                     if m.get("utility_type", "").lower() in ut_variants}
        readings = [r for r in readings if r.get("canonical_meter_id") in meter_ids]

    # Filter by state if specified
    if state:
        state_upper = state.upper()
        meter_ids = {mid for mid, m in meters.items() if m.get("state", "").upper() == state_upper}
        readings = [r for r in readings if r.get("canonical_meter_id") in meter_ids]

    # Filter by property if specified
    if property_id:
        meter_ids = {mid for mid, m in meters.items() if m.get("property_id") == property_id}
        readings = [r for r in readings if r.get("canonical_meter_id") in meter_ids]

    # Group by month
    by_month = {}
    for r in readings:
        date_str = r.get("service_end") or r.get("reading_date")
        if not date_str:
            continue
        # Parse date - handle both MM/DD/YYYY and YYYY-MM-DD formats
        try:
            if "/" in date_str:
                dt = datetime.strptime(date_str, "%m/%d/%Y")
            else:
                dt = datetime.strptime(date_str[:10], "%Y-%m-%d")
            month_key = dt.strftime("%Y-%m")
        except Exception:
            continue

        consumption = r.get("enriched_consumption") or r.get("normalized_consumption") or 0
        amount = r.get("amount") or 0
        if consumption <= 0:
            continue

        if month_key not in by_month:
            by_month[month_key] = {"consumption": 0, "amount": 0, "reading_count": 0, "meters": set()}
        by_month[month_key]["consumption"] += consumption
        by_month[month_key]["amount"] += amount
        by_month[month_key]["reading_count"] += 1
        by_month[month_key]["meters"].add(r.get("canonical_meter_id"))

    # Calculate MoM changes (use per-meter average for meaningful comparison)
    months_sorted = sorted(by_month.keys())[-months:]
    trends = []
    for i, month in enumerate(months_sorted):
        data = by_month[month]
        meter_count = len(data["meters"]) or 1
        avg_consumption_per_meter = data["consumption"] / meter_count

        prev = by_month.get(months_sorted[i-1]) if i > 0 else None
        if prev:
            prev_meter_count = len(prev["meters"]) or 1
            prev_avg = prev["consumption"] / prev_meter_count
            mom_change = ((avg_consumption_per_meter - prev_avg) / prev_avg * 100) if prev_avg > 0 else 0
        else:
            mom_change = 0

        trends.append({
            "month": month,
            "consumption": round(data["consumption"], 0),
            "amount": round(data["amount"], 2),
            "reading_count": data["reading_count"],
            "meter_count": meter_count,
            "avg_per_meter": round(avg_consumption_per_meter, 0),
            "mom_change_pct": round(mom_change, 1),
            "mom_change_direction": "up" if mom_change > 5 else "down" if mom_change < -5 else "flat"
        })

    return {"trends": trends, "level": level, "utility_type": utility_type}


@app.get("/api/meters/executive-summary")
def api_meters_executive_summary(
    months: int = 3,
    user: str = Depends(require_user)
):
    """Executive summary for portfolio performance - designed for leadership dashboard.

    Returns:
    - data_freshness: When data was last updated
    - portfolio_summary: Total spend, consumption, avg rates
    - issues_by_category: Rate increases, volume spikes, data issues
    - top_issues: Actionable list with drill-down to property/meter
    - property_rankings: Best/worst performing properties
    """
    try:
        meter_data = _get_meter_data_from_s3()
        readings = meter_data.get("readings", [])
        meters = meter_data.get("meters", {})
        last_scan = meter_data.get("last_scan", "Unknown")

        # Filter to recent months - parse dates properly
        cutoff_dt = datetime.now() - timedelta(days=months * 31)
        recent_readings = []
        for r in readings:
            date_str = r.get("service_end") or r.get("reading_date") or ""
            if not date_str:
                continue
            try:
                if "/" in date_str:
                    dt = datetime.strptime(date_str, "%m/%d/%Y")
                else:
                    dt = datetime.strptime(date_str[:10], "%Y-%m-%d")
                if dt >= cutoff_dt:
                    recent_readings.append(r)
            except Exception:
                continue

        # Pre-group readings by meter_id for O(n) instead of O(n*m) lookup
        readings_by_meter = {}
        for r in recent_readings:
            mid = r.get("canonical_meter_id")
            if mid:
                readings_by_meter.setdefault(mid, []).append(r)

        # Portfolio summary calculations
        total_consumption = 0
        total_spend = 0
        rates_by_utility = {"electric": [], "gas": [], "water": []}

        for r in recent_readings:
            consumption = r.get("enriched_consumption") or r.get("normalized_consumption") or 0
            amount = r.get("amount") or 0  # Use 'amount' field (not 'charge_amount')
            total_consumption += consumption
            total_spend += amount

            # Calculate effective rate from amount/consumption
            effective_rate = (amount / consumption) if consumption > 0 and amount > 0 else 0

            # Track rates by utility type
            meter_id = r.get("canonical_meter_id")
            meter = meters.get(meter_id, {})
            ut = meter.get("utility_type", "").lower()
            if "electric" in ut and effective_rate > 0:
                rates_by_utility["electric"].append(effective_rate)
            elif "gas" in ut and effective_rate > 0:
                rates_by_utility["gas"].append(effective_rate)
            elif "water" in ut and effective_rate > 0:
                rates_by_utility["water"].append(effective_rate)

        # Calculate average rates
        avg_rates = {}
        for ut, rates in rates_by_utility.items():
            if rates:
                avg_rates[ut] = round(sum(rates) / len(rates), 4)

        # Collect all flags from meters (re-compute with charge data)
        all_issues = []
        property_issues = {}  # property_id -> {rate: [], volume: [], data: []}
        property_names = {}

        for meter_id, meter in meters.items():
            # Use pre-grouped readings instead of filtering each time
            meter_readings = readings_by_meter.get(meter_id, [])
            if len(meter_readings) < 2:
                continue

            # Sort by date for flag computation
            meter_readings.sort(key=lambda x: x.get("service_end") or x.get("reading_date") or "")

            # Compute flags using actual rates
            flags = _compute_meter_flags(meter_readings, meter.get("utility_type", ""))

            prop_id = meter.get("property_id", "unknown")
            prop_name = meter.get("property_name", "Unknown Property")
            property_names[prop_id] = prop_name

            if prop_id not in property_issues:
                property_issues[prop_id] = {"rate": [], "volume": [], "data": [], "total_impact": 0}

            for flag in flags:
                issue = {
                    "meter_id": meter_id,
                    "meter_number": meter.get("normalized_meter_number") or meter.get("display_name", "Unknown"),
                    "property_id": prop_id,
                    "property_name": prop_name,
                    "state": meter.get("state", ""),
                    "utility_type": meter.get("utility_type", ""),
                    **flag
                }
                all_issues.append(issue)

                category = flag.get("category", "volume")
                property_issues[prop_id][category].append(issue)
                if flag.get("cost_impact"):
                    property_issues[prop_id]["total_impact"] += flag["cost_impact"]

        # Categorize issues
        rate_issues = [i for i in all_issues if i.get("category") == "rate"]
        volume_issues = [i for i in all_issues if i.get("category") == "volume" and i.get("type") != "drop"]
        data_issues = [i for i in all_issues if i.get("category") == "data"]

        # Calculate total cost impact by category
        rate_impact = sum(i.get("cost_impact", 0) or 0 for i in rate_issues)
        volume_impact = sum(i.get("cost_impact", 0) or 0 for i in volume_issues)

        # Sort issues by cost impact for top issues list
        sorted_issues = sorted(
            [i for i in all_issues if i.get("cost_impact") and i.get("cost_impact") > 0],
            key=lambda x: x.get("cost_impact", 0),
            reverse=True
        )[:20]

        # Property rankings by total issue impact
        property_rankings = []
        for prop_id, issues in property_issues.items():
            total_issues = len(issues["rate"]) + len(issues["volume"]) + len(issues["data"])
            if total_issues > 0:
                property_rankings.append({
                    "property_id": prop_id,
                    "property_name": property_names.get(prop_id, "Unknown"),
                    "rate_issues": len(issues["rate"]),
                    "volume_issues": len(issues["volume"]),
                    "data_issues": len(issues["data"]),
                    "total_issues": total_issues,
                    "est_monthly_impact": round(issues["total_impact"], 2)
                })

        property_rankings.sort(key=lambda x: x["est_monthly_impact"], reverse=True)

        # Data freshness details - parse dates properly
        latest_reading_date = ""
        if recent_readings:
            parsed_dates = []
            for r in recent_readings:
                date_str = r.get("service_end") or r.get("reading_date") or ""
                if date_str:
                    try:
                        if "/" in date_str:
                            dt = datetime.strptime(date_str, "%m/%d/%Y")
                        else:
                            dt = datetime.strptime(date_str[:10], "%Y-%m-%d")
                        parsed_dates.append(dt)
                    except Exception:
                        pass
            if parsed_dates:
                latest_reading_date = max(parsed_dates).strftime("%Y-%m-%d")

        return {
            "data_freshness": {
                "last_scan": last_scan,
                "latest_reading_date": latest_reading_date,
                "readings_analyzed": len(recent_readings),
                "meters_analyzed": len(meters)
            },
            "portfolio_summary": {
                "total_spend": round(total_spend, 2),
                "total_consumption": round(total_consumption, 0),
                "avg_rates": avg_rates,
                "period_months": months
            },
            "issues_by_category": {
                "rate_increases": {
                    "count": len(rate_issues),
                    "est_monthly_impact": round(rate_impact, 2),
                    "description": "Utility rate increases detected"
                },
                "volume_spikes": {
                    "count": len(volume_issues),
                    "est_monthly_impact": round(volume_impact, 2),
                    "description": "Abnormal usage increases"
                },
                "data_issues": {
                    "count": len(data_issues),
                    "est_monthly_impact": 0,
                    "description": "Missing data or billing gaps"
                }
            },
            "top_issues": sorted_issues,
            "property_rankings": property_rankings[:15]
        }
    except Exception as e:
        return JSONResponse(
            {"error": f"Failed to load executive summary: {_sanitize_error(e, 'executive-summary')}"},
            status_code=500
        )


@app.post("/api/meters/bulk-rescan")
async def api_meters_bulk_rescan(request: Request, user: str = Depends(require_user)):
    """Trigger a full rescan of all Stage 7/8 files.

    For 'rescan' action: Clears existing data and rebuilds from scratch (local).
    For 'scan' action: Incremental scan via Lambda.
    """
    try:
        payload = await request.json() if request.headers.get("content-type") == "application/json" else {}
        action = payload.get("action", "rescan")  # rescan or scan

        if action == "rescan":
            # Full rescan - do it locally to capture source_input_key for PDFs
            print("[BULK RESCAN] Starting full local rescan (clearing existing data)...")
            return _api_meters_full_rescan_local(user)
        else:
            # Incremental scan via Lambda
            lambda_client = boto3.client("lambda", region_name="us-east-1")
            response = lambda_client.invoke(
                FunctionName="jrk-meter-cleaner",
                InvocationType="RequestResponse",
                Payload=json.dumps({"action": "scan", "days_back": 1})
            )
            result = json.loads(response["Payload"].read().decode("utf-8"))
            if "body" in result:
                body = json.loads(result["body"]) if isinstance(result["body"], str) else result["body"]
                return {"ok": True, **body}
            return {"ok": True, **result}

    except Exception as e:
        print(f"[BULK RESCAN] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


def _api_meters_full_rescan_local(user: str):
    """Full rescan - clears existing data and rebuilds from all Stage 7/8 files."""
    print("[METER RESCAN] Starting full rescan - clearing existing data...")

    # Start fresh - no existing data
    meters = {}
    readings = []
    aliases = {}
    new_readings = 0
    new_meters = 0
    files_scanned = 0

    # Scan Stage 7 (Posted) and Stage 8 (UBI Assigned)
    prefixes = [POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX]

    for prefix in prefixes:
        print(f"[METER RESCAN] Scanning {prefix}...")
        try:
            paginator = s3.get_paginator('list_objects_v2')
            for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                for obj in page.get('Contents', []):
                    key = obj['Key']
                    if not (key.endswith('.jsonl') or key.endswith('.jsonl.gz')):
                        continue

                    files_scanned += 1
                    if files_scanned % 500 == 0:
                        print(f"[METER RESCAN] Processed {files_scanned} files, {new_readings} readings...")

                    try:
                        txt = _read_s3_text(BUCKET, key)
                        line_index = 0
                        for line in txt.strip().split('\n'):
                            if not line.strip():
                                continue

                            rec = json.loads(line)
                            line_index += 1

                            # Filter to relevant utility types
                            utility = (rec.get('Utility Type') or rec.get('Utility Name') or '').strip()
                            if utility.lower() not in METER_UTILITY_TYPES:
                                continue

                            # Extract meter data
                            raw_meter = (rec.get('Meter Number') or '').strip()
                            if not raw_meter:
                                continue

                            raw_consumption = rec.get('Consumption Amount') or rec.get('ENRICHED CONSUMPTION')
                            raw_uom = (rec.get('Unit of Measure') or rec.get('ENRICHED UOM') or '').strip()
                            raw_charge = rec.get('Line Item Charge') or rec.get('Amount') or rec.get('AMOUNT') or 0

                            consumption = _parse_consumption(raw_consumption)
                            if consumption is None:
                                continue

                            # Parse charge amount
                            charge_amount = 0.0
                            try:
                                if isinstance(raw_charge, str):
                                    charge_amount = float(raw_charge.replace("$", "").replace(",", "").strip() or 0)
                                else:
                                    charge_amount = float(raw_charge or 0)
                            except (ValueError, TypeError):
                                charge_amount = 0.0

                            # Normalize
                            normalized_meter = _normalize_meter_number(raw_meter)
                            canonical_uom, uom_category, conversion = _normalize_uom(raw_uom)
                            normalized_consumption = consumption * conversion if conversion else consumption

                            # Get property and date info
                            property_id = rec.get('EnrichedPropertyID') or rec.get('Property ID') or ''
                            property_name = rec.get('EnrichedPropertyName') or rec.get('Property Name') or ''
                            service_start = rec.get('Bill Period Start') or rec.get('Service Start') or ''
                            service_end = rec.get('Bill Period End') or rec.get('Current Reading Date') or ''
                            account = rec.get('Account Number') or ''
                            vendor = rec.get('EnrichedVendorName') or rec.get('Vendor Name') or ''

                            # Normalize utility type
                            utility_normalized = utility.lower().replace('vacant ', '')
                            if utility_normalized in ('electricity', 'electric'):
                                utility_normalized = 'electric'

                            # Create or find canonical meter
                            meter_key = f"{property_id}|{utility_normalized}|{normalized_meter}"

                            if meter_key not in meters:
                                meters[meter_key] = {
                                    "canonical_meter_id": meter_key,
                                    "property_id": str(property_id),
                                    "property_name": property_name,
                                    "utility_type": utility_normalized,
                                    "normalized_meter_number": normalized_meter,
                                    "display_name": f"{property_name} - {utility_normalized.title()} - {normalized_meter}",
                                    "canonical_uom": canonical_uom,
                                    "created_date": datetime.utcnow().isoformat(),
                                    "reading_count": 0
                                }
                                new_meters += 1

                            # Track alias
                            if raw_meter != normalized_meter:
                                aliases[raw_meter] = meter_key

                            # Calculate effective rate ($/unit)
                            effective_rate = charge_amount / normalized_consumption if normalized_consumption > 0 else 0.0

                            # Get PDF key from source_input_key (preferred) or derive from source_file_page
                            source_input_key = rec.get("source_input_key", "") or ""
                            if not source_input_key:
                                source_file_page = rec.get("source_file_page", "") or ""
                                if source_file_page:
                                    source_input_key = f"Bill_Parser_2_Parsed_Inputs/{source_file_page}"

                            # Add reading
                            reading = {
                                "canonical_meter_id": meter_key,
                                "reading_date": service_end,
                                "service_start": service_start,
                                "service_end": service_end,
                                "raw_consumption": consumption,
                                "consumption": consumption,
                                "normalized_consumption": normalized_consumption,
                                "enriched_consumption": normalized_consumption,
                                "charge_amount": charge_amount,
                                "amount": charge_amount,
                                "effective_rate": round(effective_rate, 4),
                                "raw_uom": raw_uom,
                                "canonical_uom": canonical_uom,
                                "normalized_uom": canonical_uom,
                                "conversion_factor": conversion,
                                "account_number": account,
                                "vendor_name": vendor,
                                "vendor": vendor,
                                "source_s3_key": key,
                                "source_input_key": source_input_key,
                                "source_pdf_id": pdf_id_from_key(key),
                                "line_index": line_index,
                                "flagged": False,
                                "flag_reason": None
                            }

                            # Flag outliers
                            if consumption < 0:
                                reading["flagged"] = True
                                reading["flag_reason"] = "Negative consumption"
                            elif consumption == 0:
                                reading["flagged"] = True
                                reading["flag_reason"] = "Zero consumption"

                            readings.append(reading)
                            meters[meter_key]["reading_count"] = meters[meter_key].get("reading_count", 0) + 1
                            new_readings += 1

                    except Exception as e:
                        print(f"[METER RESCAN] Error processing {key}: {e}")
                        continue

        except Exception as e:
            print(f"[METER RESCAN] Error scanning {prefix}: {e}")

    # Add sparkline data
    print(f"[METER RESCAN] Computing sparklines for {len(meters)} meters...")
    meters = _add_sparklines_to_meters(meters, readings)

    # Save
    meter_data = {
        "meters": meters,
        "readings": readings,
        "aliases": aliases,
        "last_scan": datetime.utcnow().isoformat(),
        "scan_metadata": {
            "type": "full_rescan",
            "files_scanned": files_scanned,
            "timestamp": datetime.utcnow().isoformat()
        }
    }
    _save_meter_data_to_s3(meter_data)

    print(f"[METER RESCAN] Complete: {new_meters} meters, {new_readings} readings from {files_scanned} files")

    return {
        "ok": True,
        "status": "complete",
        "message": f"Full rescan complete: {new_meters} meters, {new_readings} readings",
        "new_meters": new_meters,
        "new_readings": new_readings,
        "total_meters": len(meters),
        "total_readings": len(readings),
        "files_scanned": files_scanned
    }


@app.get("/chart-by-meter")
def page_chart_by_meter(request: Request, user: str = Depends(require_user)):
    """Render the Chart by Meter page."""
    return templates.TemplateResponse("chart-by-meter.html", {"request": request, "user": user})


# ============================================================================
# BILLBACK SUMMARY REPORT GENERATION
# ============================================================================

@app.get("/api/billback/report/data")
def api_billback_report_data(
    user: str = Depends(require_user),
    period: str = "",
    months_back: int = 12
):
    """
    Aggregate billback data for report generation.
    Returns data grouped by Asset Manager -> Property -> GL Code.

    If period is specified (e.g., "2026-01"), returns data for that specific period.
    Otherwise returns data for the most recent complete month.
    """
    from collections import defaultdict
    from datetime import datetime, timedelta
    from concurrent.futures import ThreadPoolExecutor, as_completed

    def normalize_period(p: str) -> str:
        """Convert period to YYYY-MM format. Handles MM/YYYY, YYYY-MM, YYYY/MM formats."""
        if not p:
            return ""
        p = p.strip()
        # Handle MM/YYYY format (e.g., "01/2026")
        if "/" in p and len(p) >= 7:
            parts = p.split("/")
            if len(parts) == 2:
                if len(parts[0]) == 2 and len(parts[1]) == 4:  # MM/YYYY
                    return f"{parts[1]}-{parts[0]}"
                elif len(parts[0]) == 4 and len(parts[1]) == 2:  # YYYY/MM
                    return f"{parts[0]}-{parts[1]}"
        # Handle YYYY-MM format
        if "-" in p and len(p) >= 7:
            return p[:7]
        # Try to parse as is
        return p[:7] if len(p) >= 7 else p

    try:
        print(f"[REPORT DATA] Generating report data for period={period}, months_back={months_back}")

        # Determine target period
        if period:
            parts = period.split("-")
            if len(parts) < 2 or not parts[0].isdigit() or not parts[1].isdigit():
                return JSONResponse({"error": f"Invalid period format: '{period}'. Expected YYYY-MM."}, status_code=400)
            target_year, target_month = int(parts[0]), int(parts[1])
            if target_month < 1 or target_month > 12:
                return JSONResponse({"error": f"Invalid month in period: '{period}'."}, status_code=400)
        else:
            # Use previous month
            today = datetime.now()
            if today.month == 1:
                target_year, target_month = today.year - 1, 12
            else:
                target_year, target_month = today.year, today.month - 1

        target_period = f"{target_year}-{target_month:02d}"
        print(f"[REPORT DATA] Target period: {target_period}")

        # Calculate historical periods for T-1 and T-12
        historical_periods = []
        for i in range(months_back):
            m = target_month - i
            y = target_year
            while m <= 0:
                m += 12
                y -= 1
            historical_periods.append(f"{y}-{m:02d}")

        # Load data from Stage 8 (UBI Assigned) for all periods
        all_items = []
        prefixes_to_scan = set()

        # Use month-level prefixes instead of day-level (12-13 prefixes vs 462)
        today = datetime.now()
        for i in range(months_back + 1):
            m = today.month - i
            y = today.year
            while m <= 0:
                m += 12
                y -= 1
            prefixes_to_scan.add(f"{UBI_ASSIGNED_PREFIX}yyyy={y}/mm={m:02d}/")

        prefixes_to_scan = list(prefixes_to_scan)
        print(f"[REPORT DATA] Scanning {len(prefixes_to_scan)} month-level prefixes")

        # Collect S3 keys IN PARALLEL
        def _list_report_prefix(prefix):
            keys = []
            try:
                paginator = s3.get_paginator('list_objects_v2')
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get('Contents', []):
                        key = obj['Key']
                        if key.endswith('.jsonl'):
                            keys.append(key)
            except Exception as e:
                print(f"[REPORT DATA] Error listing {prefix}: {e}")
            return keys

        all_keys = []
        list_futures = [_GLOBAL_EXECUTOR.submit(_list_report_prefix, p) for p in prefixes_to_scan]
        for future in as_completed(list_futures):
            all_keys.extend(future.result())

        print(f"[REPORT DATA] Found {len(all_keys)} Stage 8 files to scan")

        def process_file(key):
            """Process a single S3 file."""
            try:
                body = _read_s3_text(BUCKET, key)
                lines = [ln.strip() for ln in body.splitlines() if ln.strip()]
                results = []
                for line in lines:
                    rec = json.loads(line)

                    # Extract UBI period(s)
                    ubi_assignments = rec.get("ubi_assignments", [])
                    if ubi_assignments:
                        for asn in ubi_assignments:
                            ubi_period = asn.get("period", "")
                            if not ubi_period:
                                continue

                            charge = asn.get("amount", 0.0)
                            if charge == 0.0:
                                charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                                charge = float(charge_str) if charge_str else 0.0

                            # Normalize period to YYYY-MM
                            normalized_period = normalize_period(ubi_period)
                            if not normalized_period:
                                continue

                            # Get property name - prefer EnrichedPropertyName
                            prop_name = rec.get("EnrichedPropertyName") or rec.get("Property Name") or rec.get("property_name") or ""

                            results.append({
                                "ubi_period": normalized_period,
                                "property_code": rec.get("EnrichedPropertyID") or rec.get("Property Code") or rec.get("property_code") or "",
                                "property_name": prop_name,
                                "vendor_name": rec.get("EnrichedVendorName") or rec.get("Vendor Name") or "",
                                "account": rec.get("Account Number", ""),
                                "gl_code": rec.get("Charge Code") or rec.get("charge_code") or rec.get("GL Code") or rec.get("EnrichedGLAccountNumber") or "",
                                "gl_description": rec.get("EnrichedGLAccountName") or rec.get("Charge Code Description") or rec.get("GL Description") or "",
                                "charge": charge,
                                "excluded": rec.get("excluded", False) or rec.get("is_excluded", False)
                            })
                    else:
                        # Legacy format
                        ubi_period = rec.get("ubi_period", "")
                        if not ubi_period:
                            continue

                        # Normalize period to YYYY-MM
                        normalized_period = normalize_period(ubi_period)
                        if not normalized_period:
                            continue

                        charge = rec.get("ubi_amount", 0.0)
                        if charge == 0.0:
                            charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                            charge = float(charge_str) if charge_str else 0.0

                        # Get property name - prefer EnrichedPropertyName
                        prop_name = rec.get("EnrichedPropertyName") or rec.get("Property Name") or rec.get("property_name") or ""

                        results.append({
                            "ubi_period": normalized_period,
                            "property_code": rec.get("EnrichedPropertyID") or rec.get("Property Code") or rec.get("property_code") or "",
                            "property_name": prop_name,
                            "vendor_name": rec.get("EnrichedVendorName") or rec.get("Vendor Name") or "",
                            "account": rec.get("Account Number", ""),
                            "gl_code": rec.get("Charge Code") or rec.get("charge_code") or rec.get("GL Code") or rec.get("EnrichedGLAccountNumber") or "",
                            "gl_description": rec.get("EnrichedGLAccountName") or rec.get("Charge Code Description") or rec.get("GL Description") or "",
                            "charge": charge,
                            "excluded": rec.get("excluded", False) or rec.get("is_excluded", False)
                        })
                return results
            except Exception as e:
                return []

        # Process files concurrently
        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                items = future.result()
                all_items.extend(items)

        print(f"[REPORT DATA] Loaded {len(all_items)} line items from Stage 8")

        # Load portfolio master for Asset Manager mapping
        portfolio = _load_portfolio_master()
        portfolio_props = portfolio.get("properties", {})

        # Build lookup by property name (case-insensitive)
        portfolio_by_name = {}
        for code, pdata in portfolio_props.items():
            name_key = (pdata.get("property_name") or "").lower().strip()
            if name_key:
                portfolio_by_name[name_key] = pdata
            # Also index by code
            portfolio_by_name[code.lower().strip()] = pdata

        print(f"[REPORT DATA] Loaded {len(portfolio_props)} properties from portfolio master")

        # Group data by period, then by property, then by GL code
        by_period = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: {"total": 0.0, "excluded": 0.0, "count": 0})))

        for item in all_items:
            period_key = item["ubi_period"]
            prop_key = item["property_name"] or item["property_code"] or "Unknown"
            gl_key = item["gl_code"] or "Unknown"

            if item["excluded"]:
                by_period[period_key][prop_key][gl_key]["excluded"] += item["charge"]
            else:
                by_period[period_key][prop_key][gl_key]["total"] += item["charge"]
            by_period[period_key][prop_key][gl_key]["count"] += 1

        # Build report structure for target period with historical comparison
        report_data = {
            "period": target_period,
            "generated_at": datetime.now().isoformat(),
            "generated_by": user,
            "properties": [],
            "summary": {
                "total_billed_back": 0.0,
                "total_excluded": 0.0,
                "property_count": 0,
                "t1_total": 0.0,
                "t12_avg": 0.0
            }
        }

        # Get T-1 period
        t1_month = target_month - 1
        t1_year = target_year
        if t1_month <= 0:
            t1_month += 12
            t1_year -= 1
        t1_period = f"{t1_year}-{t1_month:02d}"

        # Calculate property summaries
        target_data = by_period.get(target_period, {})
        t1_data = by_period.get(t1_period, {})

        # Collect all historical data for T-12 calculation
        all_props = set(target_data.keys())
        for period_key in historical_periods:
            all_props.update(by_period.get(period_key, {}).keys())

        for prop_name in sorted(all_props):
            # Look up portfolio data for this property
            prop_key = prop_name.lower().strip()
            port_data = portfolio_by_name.get(prop_key, {})

            prop_entry = {
                "name": prop_name,
                "code": port_data.get("property_code", ""),
                "region": port_data.get("region", ""),
                "fund": port_data.get("fund", ""),
                "units": port_data.get("units", 0) or 0,
                "asset_manager": port_data.get("asset_manager", "Unassigned"),
                "analyst": port_data.get("analyst", ""),
                "gl_codes": [],
                "totals": {
                    "billed_back": 0.0,
                    "excluded": 0.0,
                    "t1": 0.0,
                    "t12_avg": 0.0
                }
            }

            # Current period GL breakdown
            prop_current = target_data.get(prop_name, {})
            prop_t1 = t1_data.get(prop_name, {})

            # Calculate T-12 by GL
            all_gl_codes = set(prop_current.keys())
            for period_key in historical_periods:
                if prop_name in by_period.get(period_key, {}):
                    all_gl_codes.update(by_period[period_key][prop_name].keys())

            for gl_code in sorted(all_gl_codes):
                gl_current = prop_current.get(gl_code, {"total": 0.0, "excluded": 0.0, "count": 0})
                gl_t1 = prop_t1.get(gl_code, {"total": 0.0, "excluded": 0.0, "count": 0})

                # Calculate T-12 average
                t12_totals = []
                for period_key in historical_periods[1:13]:  # Skip current, use last 12
                    if prop_name in by_period.get(period_key, {}):
                        if gl_code in by_period[period_key][prop_name]:
                            t12_totals.append(by_period[period_key][prop_name][gl_code]["total"])

                t12_avg = sum(t12_totals) / len(t12_totals) if t12_totals else 0.0

                gl_entry = {
                    "code": gl_code,
                    "description": "",  # Would come from GL mapping
                    "billed_back": round(gl_current["total"], 2),
                    "excluded": round(gl_current["excluded"], 2),
                    "t1": round(gl_t1["total"], 2),
                    "t12_avg": round(t12_avg, 2),
                    "variance_t12": round(((gl_current["total"] - t12_avg) / t12_avg * 100) if t12_avg else 0, 1)
                }

                prop_entry["gl_codes"].append(gl_entry)
                prop_entry["totals"]["billed_back"] += gl_current["total"]
                prop_entry["totals"]["excluded"] += gl_current["excluded"]
                prop_entry["totals"]["t1"] += gl_t1["total"]
                prop_entry["totals"]["t12_avg"] += t12_avg

            # Round totals
            prop_entry["totals"]["billed_back"] = round(prop_entry["totals"]["billed_back"], 2)
            prop_entry["totals"]["excluded"] = round(prop_entry["totals"]["excluded"], 2)
            prop_entry["totals"]["t1"] = round(prop_entry["totals"]["t1"], 2)
            prop_entry["totals"]["t12_avg"] = round(prop_entry["totals"]["t12_avg"], 2)

            if prop_entry["totals"]["billed_back"] > 0 or prop_entry["totals"]["excluded"] > 0:
                report_data["properties"].append(prop_entry)
                report_data["summary"]["total_billed_back"] += prop_entry["totals"]["billed_back"]
                report_data["summary"]["total_excluded"] += prop_entry["totals"]["excluded"]
                report_data["summary"]["t1_total"] += prop_entry["totals"]["t1"]
                report_data["summary"]["t12_avg"] += prop_entry["totals"]["t12_avg"]

        report_data["summary"]["property_count"] = len(report_data["properties"])
        report_data["summary"]["total_billed_back"] = round(report_data["summary"]["total_billed_back"], 2)
        report_data["summary"]["total_excluded"] = round(report_data["summary"]["total_excluded"], 2)
        report_data["summary"]["t1_total"] = round(report_data["summary"]["t1_total"], 2)
        report_data["summary"]["t12_avg"] = round(report_data["summary"]["t12_avg"], 2)

        # Calculate variance percentages
        if report_data["summary"]["t1_total"]:
            report_data["summary"]["variance_t1"] = round(
                (report_data["summary"]["total_billed_back"] - report_data["summary"]["t1_total"]) /
                report_data["summary"]["t1_total"] * 100, 1
            )
        else:
            report_data["summary"]["variance_t1"] = 0.0

        if report_data["summary"]["t12_avg"]:
            report_data["summary"]["variance_t12"] = round(
                (report_data["summary"]["total_billed_back"] - report_data["summary"]["t12_avg"]) /
                report_data["summary"]["t12_avg"] * 100, 1
            )
        else:
            report_data["summary"]["variance_t12"] = 0.0

        # Group properties by Asset Manager
        by_asset_manager = defaultdict(lambda: {"properties": [], "totals": {"billed_back": 0.0, "excluded": 0.0, "t1": 0.0, "t12_avg": 0.0}})

        for prop in report_data["properties"]:
            am = prop.get("asset_manager", "Unassigned") or "Unassigned"
            by_asset_manager[am]["properties"].append(prop)
            by_asset_manager[am]["totals"]["billed_back"] += prop["totals"]["billed_back"]
            by_asset_manager[am]["totals"]["excluded"] += prop["totals"]["excluded"]
            by_asset_manager[am]["totals"]["t1"] += prop["totals"]["t1"]
            by_asset_manager[am]["totals"]["t12_avg"] += prop["totals"]["t12_avg"]

        # Convert to sorted list
        report_data["asset_managers"] = []
        for am_name in sorted(by_asset_manager.keys()):
            am_data = by_asset_manager[am_name]
            # Round totals
            am_data["totals"]["billed_back"] = round(am_data["totals"]["billed_back"], 2)
            am_data["totals"]["excluded"] = round(am_data["totals"]["excluded"], 2)
            am_data["totals"]["t1"] = round(am_data["totals"]["t1"], 2)
            am_data["totals"]["t12_avg"] = round(am_data["totals"]["t12_avg"], 2)

            # Calculate variance
            if am_data["totals"]["t12_avg"]:
                am_data["totals"]["variance_t12"] = round(
                    (am_data["totals"]["billed_back"] - am_data["totals"]["t12_avg"]) /
                    am_data["totals"]["t12_avg"] * 100, 1
                )
            else:
                am_data["totals"]["variance_t12"] = 0.0

            report_data["asset_managers"].append({
                "name": am_name,
                "properties": am_data["properties"],
                "totals": am_data["totals"],
                "property_count": len(am_data["properties"])
            })

        report_data["summary"]["asset_manager_count"] = len(report_data["asset_managers"])

        print(f"[REPORT DATA] Generated report with {len(report_data['properties'])} properties across {len(report_data['asset_managers'])} asset managers, total=${report_data['summary']['total_billed_back']:,.2f}")

        return report_data

    except Exception as e:
        print(f"[REPORT DATA] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


@app.get("/api/billback/report/pdf")
def api_billback_report_pdf(
    request: Request,
    user: str = Depends(require_user),
    period: str = "",
    style: str = "corporate"
):
    """
    Generate a PDF billback summary report.

    Styles:
    - corporate: Professional/formal financial statement look
    - simple: Basic functional tables, minimal styling
    - dashboard: Modern cards, charts, progress bars
    """
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter, LETTER
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, Image
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT, TA_CENTER
    from io import BytesIO
    from datetime import datetime

    try:
        # Get report data
        data = api_billback_report_data(user=user, period=period, months_back=12)
        if isinstance(data, JSONResponse):
            return data

        # Check if there's actually data to report
        if not data.get("properties") and not data.get("asset_managers"):
            return JSONResponse({"error": f"No billback data found for period {data.get('period', period)}."}, status_code=404)

        # Create PDF buffer
        buffer = BytesIO()

        # Generate PDF based on style
        if style == "corporate":
            _generate_corporate_pdf(buffer, data)
        elif style == "simple":
            _generate_simple_pdf(buffer, data)
        elif style == "dashboard":
            _generate_dashboard_pdf(buffer, data)
        else:
            _generate_corporate_pdf(buffer, data)  # Default

        buffer.seek(0)

        # Generate filename
        period_str = data.get("period", "report")
        filename = f"UBI_Billback_Summary_{period_str}_{style}.pdf"

        return StreamingResponse(
            buffer,
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={filename}"}
        )

    except Exception as e:
        print(f"[REPORT PDF] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": _sanitize_error(e, "request")}, status_code=500)


def _generate_corporate_pdf(buffer, data):
    """Generate Professional/Corporate style PDF."""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT, TA_CENTER
    from datetime import datetime

    # Colors
    NAVY = colors.HexColor("#1e3a5f")
    LIGHT_BLUE = colors.HexColor("#e8eef3")
    GRAY = colors.HexColor("#f5f5f5")

    doc = SimpleDocTemplate(buffer, pagesize=letter, topMargin=0.5*inch, bottomMargin=0.5*inch)
    styles = getSampleStyleSheet()
    story = []

    # Custom styles
    title_style = ParagraphStyle('Title', parent=styles['Heading1'], fontSize=24, textColor=NAVY, spaceAfter=6)
    subtitle_style = ParagraphStyle('Subtitle', parent=styles['Normal'], fontSize=12, textColor=colors.gray)
    section_style = ParagraphStyle('Section', parent=styles['Heading2'], fontSize=14, textColor=colors.white, backColor=NAVY, spaceBefore=20, spaceAfter=10, leftIndent=6, rightIndent=6)
    property_style = ParagraphStyle('Property', parent=styles['Heading3'], fontSize=12, textColor=NAVY, spaceBefore=12, spaceAfter=6)

    # Header
    story.append(Paragraph("JRK RESIDENTIAL", ParagraphStyle('Company', fontSize=16, textColor=NAVY, fontName='Helvetica-Bold')))
    story.append(Spacer(1, 0.1*inch))
    story.append(Paragraph("UBI Billback Summary Report", title_style))
    story.append(Paragraph(f"Billing Period: {data['period']} | Generated: {datetime.now().strftime('%B %d, %Y')}", subtitle_style))
    story.append(Spacer(1, 0.3*inch))

    # Executive Summary Box
    summary = data.get("summary", {})
    summary_data = [
        ["Total Billed Back", "Total Excluded", "vs Prior Month (T-1)", "vs 12-Mo Avg (T-12)"],
        [
            f"${summary.get('total_billed_back', 0):,.2f}",
            f"${summary.get('total_excluded', 0):,.2f}",
            f"{summary.get('variance_t1', 0):+.1f}%",
            f"{summary.get('variance_t12', 0):+.1f}%"
        ]
    ]

    summary_table = Table(summary_data, colWidths=[1.8*inch, 1.8*inch, 1.8*inch, 1.8*inch])
    summary_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), LIGHT_BLUE),
        ('TEXTCOLOR', (0, 0), (-1, 0), NAVY),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('FONTNAME', (0, 1), (-1, 1), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 1), (-1, 1), 16),
        ('TEXTCOLOR', (0, 1), (-1, 1), NAVY),
        ('BOTTOMPADDING', (0, 0), (-1, 0), 8),
        ('TOPPADDING', (0, 1), (-1, 1), 12),
        ('BOTTOMPADDING', (0, 1), (-1, 1), 12),
        ('BOX', (0, 0), (-1, -1), 1, NAVY),
        ('LINEBELOW', (0, 0), (-1, 0), 1, NAVY),
    ]))
    story.append(summary_table)
    story.append(Spacer(1, 0.3*inch))

    # Asset Manager style
    am_header_style = ParagraphStyle('AMHeader', parent=styles['Heading2'], fontSize=14, textColor=colors.white, backColor=NAVY, spaceBefore=20, spaceAfter=10, leftIndent=10, rightIndent=10)

    # Group by Asset Manager
    for am in data.get("asset_managers", []):
        # Asset Manager header
        am_totals = am.get("totals", {})
        am_header_text = f"{am['name']} - ${am_totals.get('billed_back', 0):,.2f}"
        story.append(Paragraph(am_header_text, am_header_style))

        # Properties under this AM
        for prop in am.get("properties", []):
            # Property header with metadata
            prop_meta = []
            if prop.get("region"):
                prop_meta.append(prop["region"])
            if prop.get("fund"):
                prop_meta.append(prop["fund"])
            if prop.get("units"):
                prop_meta.append(f"{prop['units']} Units")
            meta_str = " | ".join(prop_meta) if prop_meta else ""

            prop_header = f"{prop['name']}"
            if meta_str:
                prop_header += f" <font size='9' color='gray'>({meta_str})</font>"
            story.append(Paragraph(prop_header, property_style))

            # GL Code table
            table_data = [["GL Code", "Billed Back", "Excluded", "T-1", "T-12 Avg", "Variance"]]

            for gl in prop.get("gl_codes", []):
                variance = gl.get("variance_t12", 0)
                variance_str = f"{variance:+.1f}%" if variance else "-"
                table_data.append([
                    gl.get("code", ""),
                    f"${gl.get('billed_back', 0):,.2f}",
                    f"${gl.get('excluded', 0):,.2f}",
                    f"${gl.get('t1', 0):,.2f}",
                    f"${gl.get('t12_avg', 0):,.2f}",
                    variance_str
                ])

            # Property totals row
            totals = prop.get("totals", {})
            t12_var = ((totals.get('billed_back', 0) - totals.get('t12_avg', 0)) / totals.get('t12_avg', 1) * 100) if totals.get('t12_avg') else 0
            table_data.append([
                "TOTAL",
                f"${totals.get('billed_back', 0):,.2f}",
                f"${totals.get('excluded', 0):,.2f}",
                f"${totals.get('t1', 0):,.2f}",
                f"${totals.get('t12_avg', 0):,.2f}",
                f"{t12_var:+.1f}%" if totals.get('t12_avg') else "-"
            ])

            col_widths = [1.2*inch, 1.2*inch, 1.0*inch, 1.0*inch, 1.0*inch, 0.9*inch]
            table = Table(table_data, colWidths=col_widths)
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), GRAY),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 9),
                ('ALIGN', (1, 0), (-1, -1), 'RIGHT'),
                ('GRID', (0, 0), (-1, -1), 0.5, colors.lightgrey),
                ('BACKGROUND', (0, -1), (-1, -1), LIGHT_BLUE),
                ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
                ('TOPPADDING', (0, 0), (-1, -1), 4),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 4),
            ]))
            story.append(table)
            story.append(Spacer(1, 0.15*inch))

    # Build PDF
    doc.build(story)


def _generate_simple_pdf(buffer, data):
    """Generate Simple/Functional style PDF."""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT
    from datetime import datetime

    doc = SimpleDocTemplate(buffer, pagesize=letter, topMargin=0.5*inch, bottomMargin=0.5*inch)
    styles = getSampleStyleSheet()
    story = []

    # Simple header
    story.append(Paragraph("UBI Billback Summary Report", styles['Heading1']))
    story.append(Paragraph(f"Period: {data['period']} | Generated: {datetime.now().strftime('%Y-%m-%d')}", styles['Normal']))
    story.append(Spacer(1, 0.2*inch))

    # Summary
    summary = data.get("summary", {})
    summary_text = f"<b>Total Billed Back:</b> ${summary.get('total_billed_back', 0):,.2f} | "
    summary_text += f"<b>Excluded:</b> ${summary.get('total_excluded', 0):,.2f} | "
    summary_text += f"<b>vs T-1:</b> {summary.get('variance_t1', 0):+.1f}% | "
    summary_text += f"<b>vs T-12:</b> {summary.get('variance_t12', 0):+.1f}%"
    story.append(Paragraph(summary_text, styles['Normal']))
    story.append(Spacer(1, 0.2*inch))

    # Group by Asset Manager
    for am in data.get("asset_managers", []):
        # Asset Manager header
        am_totals = am.get("totals", {})
        story.append(Paragraph(f"<b>{am['name']}</b> - ${am_totals.get('billed_back', 0):,.2f}", styles['Heading2']))

        # Properties under this AM
        for prop in am.get("properties", []):
            # Property name as header
            story.append(Paragraph(f"<b>{prop['name']}</b>", styles['Heading3']))

            # Table
            table_data = [["GL Code", "Billed Back", "Excluded", "T-1", "T-12 Avg", "Var %"]]

            for gl in prop.get("gl_codes", []):
                table_data.append([
                    gl.get("code", ""),
                    f"${gl.get('billed_back', 0):,.2f}",
                    f"${gl.get('excluded', 0):,.2f}",
                    f"${gl.get('t1', 0):,.2f}",
                    f"${gl.get('t12_avg', 0):,.2f}",
                    f"{gl.get('variance_t12', 0):+.1f}%"
                ])

            # Total row
            totals = prop.get("totals", {})
            t12_var = ((totals.get('billed_back', 0) - totals.get('t12_avg', 0)) / totals.get('t12_avg', 1) * 100) if totals.get('t12_avg') else 0
            table_data.append([
                "Total",
                f"${totals.get('billed_back', 0):,.2f}",
                f"${totals.get('excluded', 0):,.2f}",
                f"${totals.get('t1', 0):,.2f}",
                f"${totals.get('t12_avg', 0):,.2f}",
                f"{t12_var:+.1f}%"
            ])

            table = Table(table_data, colWidths=[1.3*inch, 1.1*inch, 0.9*inch, 1.0*inch, 1.0*inch, 0.8*inch])
            table.setStyle(TableStyle([
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, -1), 9),
                ('ALIGN', (1, 0), (-1, -1), 'RIGHT'),
                ('GRID', (0, 0), (-1, -1), 1, colors.black),
                ('BACKGROUND', (0, 0), (-1, 0), colors.lightgrey),
                ('BACKGROUND', (0, -1), (-1, -1), colors.lightgrey),
                ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
            ]))
            story.append(table)
            story.append(Spacer(1, 0.15*inch))

    doc.build(story)


def _generate_dashboard_pdf(buffer, data):
    """Generate Dashboard style PDF with modern visual elements."""
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Flowable
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT, TA_CENTER
    from reportlab.graphics.shapes import Drawing, Rect, String
    from reportlab.graphics.charts.barcharts import VerticalBarChart
    from reportlab.graphics.charts.piecharts import Pie
    from datetime import datetime

    # Colors
    NAVY = colors.HexColor("#1e3a5f")
    BLUE = colors.HexColor("#3498db")
    LIGHT_BLUE = colors.HexColor("#e8eef3")
    GREEN = colors.HexColor("#28a745")
    RED = colors.HexColor("#dc3545")

    doc = SimpleDocTemplate(buffer, pagesize=letter, topMargin=0.5*inch, bottomMargin=0.5*inch)
    styles = getSampleStyleSheet()
    story = []

    # Header with gradient effect (simulated with colored box)
    header_style = ParagraphStyle('Header', fontSize=20, textColor=colors.white, fontName='Helvetica-Bold')
    subtitle_style = ParagraphStyle('Subtitle', fontSize=11, textColor=colors.white)

    # Header table (simulates gradient header)
    header_data = [[
        Paragraph("UBI Billback Summary", header_style),
        Paragraph(f"Period: {data['period']}<br/>Generated: {datetime.now().strftime('%B %d, %Y')}", subtitle_style)
    ]]
    header_table = Table(header_data, colWidths=[4*inch, 3*inch])
    header_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, -1), NAVY),
        ('ALIGN', (0, 0), (0, 0), 'LEFT'),
        ('ALIGN', (1, 0), (1, 0), 'RIGHT'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('LEFTPADDING', (0, 0), (-1, -1), 15),
        ('RIGHTPADDING', (0, 0), (-1, -1), 15),
        ('TOPPADDING', (0, 0), (-1, -1), 15),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 15),
        ('ROUNDEDCORNERS', [8, 8, 8, 8]),
    ]))
    story.append(header_table)
    story.append(Spacer(1, 0.2*inch))

    # KPI Cards
    summary = data.get("summary", {})
    kpi_data = [[
        f"<b>${summary.get('total_billed_back', 0):,.0f}</b><br/><font size='8' color='gray'>Total Billed Back</font>",
        f"<b>${summary.get('total_excluded', 0):,.0f}</b><br/><font size='8' color='gray'>Total Excluded</font>",
        f"<b>{summary.get('property_count', 0)}</b><br/><font size='8' color='gray'>Properties</font>",
        f"<b>{summary.get('variance_t12', 0):+.1f}%</b><br/><font size='8' color='gray'>vs 12-Mo Avg</font>"
    ]]

    kpi_table = Table([[Paragraph(cell, ParagraphStyle('KPI', fontSize=18, textColor=NAVY, alignment=TA_CENTER)) for cell in kpi_data[0]]],
                      colWidths=[1.75*inch, 1.75*inch, 1.75*inch, 1.75*inch])
    kpi_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, -1), colors.white),
        ('BOX', (0, 0), (0, 0), 1, colors.lightgrey),
        ('BOX', (1, 0), (1, 0), 1, colors.lightgrey),
        ('BOX', (2, 0), (2, 0), 1, colors.lightgrey),
        ('BOX', (3, 0), (3, 0), 1, colors.lightgrey),
        ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
        ('VALIGN', (0, 0), (-1, -1), 'MIDDLE'),
        ('TOPPADDING', (0, 0), (-1, -1), 15),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 15),
    ]))
    story.append(kpi_table)
    story.append(Spacer(1, 0.3*inch))

    # Asset Manager sections
    for am in data.get("asset_managers", []):
        am_totals = am.get("totals", {})

        # Asset Manager header
        am_header = [[
            Paragraph(f"<b>{am['name']}</b>", ParagraphStyle('AMName', fontSize=14, textColor=colors.white, fontName='Helvetica-Bold')),
            Paragraph(f"<b>${am_totals.get('billed_back', 0):,.2f}</b>", ParagraphStyle('AMTotal', fontSize=14, textColor=colors.white, alignment=TA_RIGHT))
        ]]
        am_table = Table(am_header, colWidths=[5*inch, 2*inch])
        am_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, -1), NAVY),
            ('ALIGN', (1, 0), (1, 0), 'RIGHT'),
            ('LEFTPADDING', (0, 0), (-1, -1), 12),
            ('RIGHTPADDING', (0, 0), (-1, -1), 12),
            ('TOPPADDING', (0, 0), (-1, -1), 10),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 10),
        ]))
        story.append(am_table)

        # Property Cards under this AM
        for prop in am.get("properties", []):
            totals = prop.get("totals", {})
            t12_var = ((totals.get('billed_back', 0) - totals.get('t12_avg', 0)) / totals.get('t12_avg', 1) * 100) if totals.get('t12_avg') else 0

            # Property header with metadata
            prop_meta = []
            if prop.get("region"):
                prop_meta.append(prop["region"])
            if prop.get("fund"):
                prop_meta.append(prop["fund"])
            if prop.get("units"):
                prop_meta.append(f"{prop['units']} Units")
            meta_str = " | ".join(prop_meta) if prop_meta else ""

            prop_name_text = f"<b>{prop['name']}</b>"
            if meta_str:
                prop_name_text += f"<br/><font size='8' color='gray'>{meta_str}</font>"

            prop_header = [[
                Paragraph(prop_name_text, ParagraphStyle('PropName', fontSize=12, textColor=NAVY)),
                Paragraph(f"<b>${totals.get('billed_back', 0):,.2f}</b>", ParagraphStyle('PropTotal', fontSize=14, textColor=NAVY, alignment=TA_RIGHT))
            ]]
            prop_table = Table(prop_header, colWidths=[5*inch, 2*inch])
            prop_table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, -1), LIGHT_BLUE),
                ('ALIGN', (1, 0), (1, 0), 'RIGHT'),
                ('LEFTPADDING', (0, 0), (-1, -1), 10),
                ('RIGHTPADDING', (0, 0), (-1, -1), 10),
                ('TOPPADDING', (0, 0), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
            ]))
            story.append(prop_table)

            # GL breakdown as mini progress bars (text representation)
            gl_data = []
            max_amount = max([gl.get('billed_back', 0) for gl in prop.get("gl_codes", [])] or [1])

            for gl in prop.get("gl_codes", []):
                pct = (gl.get('billed_back', 0) / max_amount * 100) if max_amount else 0
                bar_width = int(pct / 5)  # Scale to ~20 chars max
                bar = '' * bar_width + '' * (20 - bar_width)

                gl_data.append([
                    gl.get("code", ""),
                    f"${gl.get('billed_back', 0):,.2f}",
                    bar,
                    f"{gl.get('variance_t12', 0):+.1f}%"
                ])

            if gl_data:
                gl_table = Table(gl_data, colWidths=[1*inch, 1.2*inch, 3.5*inch, 1*inch])
                gl_table.setStyle(TableStyle([
                    ('FONTSIZE', (0, 0), (-1, -1), 9),
                    ('ALIGN', (1, 0), (1, -1), 'RIGHT'),
                    ('ALIGN', (3, 0), (3, -1), 'RIGHT'),
                    ('TEXTCOLOR', (2, 0), (2, -1), BLUE),
                    ('FONTNAME', (2, 0), (2, -1), 'Courier'),
                    ('LEFTPADDING', (0, 0), (-1, -1), 10),
                    ('TOPPADDING', (0, 0), (-1, -1), 3),
                    ('BOTTOMPADDING', (0, 0), (-1, -1), 3),
                ]))
                story.append(gl_table)

            story.append(Spacer(1, 0.1*inch))

        story.append(Spacer(1, 0.15*inch))

    # Footer
    story.append(Spacer(1, 0.3*inch))
    footer_text = f"JRK Residential - UBI Billback Summary Report - {data['period']} - Confidential"
    story.append(Paragraph(footer_text, ParagraphStyle('Footer', fontSize=9, textColor=colors.gray, alignment=TA_CENTER)))

    doc.build(story)


@app.get("/api/billback/report/periods")
def api_billback_report_periods(user: str = Depends(require_user)):
    """Get available periods for report generation based on Stage 8 data."""
    from datetime import datetime, timedelta
    from collections import defaultdict

    try:
        # Scan Stage 8 to find periods with data
        prefixes_to_scan = []
        today = datetime.now()

        # Check last 18 months
        for i in range(18 * 31):
            d = today - timedelta(days=i)
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        prefixes_to_scan = list(set(prefixes_to_scan))

        # Quick scan to find which months have data
        periods_with_data = set()

        for prefix in prefixes_to_scan[:100]:  # Sample first 100 prefixes
            try:
                response = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix, MaxKeys=5)
                if response.get('Contents'):
                    # Extract month from prefix
                    parts = prefix.split('/')
                    for part in parts:
                        if part.startswith('yyyy='):
                            year = part.replace('yyyy=', '')
                        if part.startswith('mm='):
                            month = part.replace('mm=', '')
                            periods_with_data.add(f"{year}-{month}")
            except Exception:
                pass

        # Generate 3 months ahead + 18 months back as options
        periods = []
        for i in range(-3, 19):
            m = today.month - i
            y = today.year
            while m <= 0:
                m += 12
                y -= 1
            while m > 12:
                m -= 12
                y += 1
            period = f"{y}-{m:02d}"
            periods.append({
                "value": period,
                "label": datetime(y, m, 1).strftime("%B %Y"),
                "has_data": period in periods_with_data
            })

        return {"periods": periods}

    except Exception as e:
        print(f"[REPORT PERIODS] Error: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ============================================================================
# PORTFOLIO MASTER DATA MANAGEMENT
# ============================================================================

def _load_portfolio_master() -> dict:
    """Load portfolio master data from S3."""
    try:
        body = _read_s3_text(CONFIG_BUCKET, PORTFOLIO_MASTER_KEY)
        return json.loads(body)
    except Exception as e:
        print(f"[PORTFOLIO] No existing portfolio data: {e}")
        return {"properties": {}, "updated_at": None, "updated_by": None}


def _save_portfolio_master(data: dict):
    """Save portfolio master data to S3."""
    s3.put_object(
        Bucket=CONFIG_BUCKET,
        Key=PORTFOLIO_MASTER_KEY,
        Body=json.dumps(data, ensure_ascii=False, indent=2),
        ContentType="application/json"
    )


@app.get("/portfolio-config")
def portfolio_config_page(request: Request, user: str = Depends(require_user)):
    """Render the Portfolio Configuration page."""
    return templates.TemplateResponse("portfolio-config.html", {"request": request, "user": user})


@app.get("/api/portfolio")
def api_portfolio_list(user: str = Depends(require_user)):
    """Get all properties from the portfolio master."""
    try:
        data = _load_portfolio_master()
        properties = list(data.get("properties", {}).values())

        # Sort by asset manager, then property name
        properties.sort(key=lambda x: (x.get("asset_manager", "").lower(), x.get("property_name", "").lower()))

        # Group by asset manager for summary
        by_am = {}
        for prop in properties:
            am = prop.get("asset_manager", "Unassigned")
            if am not in by_am:
                by_am[am] = {"count": 0, "total_units": 0}
            by_am[am]["count"] += 1
            by_am[am]["total_units"] += prop.get("units", 0) or 0

        return {
            "properties": properties,
            "by_asset_manager": by_am,
            "total_properties": len(properties),
            "updated_at": data.get("updated_at"),
            "updated_by": data.get("updated_by")
        }
    except Exception as e:
        print(f"[PORTFOLIO] Error listing: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/portfolio/{property_code}")
def api_portfolio_get(property_code: str, user: str = Depends(require_user)):
    """Get a single property from the portfolio master."""
    try:
        data = _load_portfolio_master()
        prop = data.get("properties", {}).get(property_code)
        if not prop:
            return JSONResponse({"error": "Property not found"}, status_code=404)
        return prop
    except Exception as e:
        print(f"[PORTFOLIO] Error getting {property_code}: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/portfolio/upload")
async def api_portfolio_upload(
    request: Request,
    file: UploadFile = File(...),
    user: str = Depends(require_user)
):
    """
    Upload an Excel file to update the portfolio master.
    Expects columns: Code, Asset Manager, Analyst, Property, Region, Fund, Units
    """
    import pandas as pd
    from io import BytesIO
    from datetime import datetime

    try:
        print(f"[PORTFOLIO UPLOAD] User {user} uploading file: {file.filename}")

        # Read Excel file
        contents = await file.read()
        df = pd.read_excel(BytesIO(contents))

        print(f"[PORTFOLIO UPLOAD] Read {len(df)} rows, columns: {list(df.columns)}")

        # Normalize column names (case-insensitive matching)
        col_map = {}
        for col in df.columns:
            col_lower = str(col).lower().strip()
            if 'code' in col_lower and 'charge' not in col_lower:
                col_map[col] = 'property_code'
            elif 'asset' in col_lower and 'manager' in col_lower:
                col_map[col] = 'asset_manager'
            elif col_lower == 'analyst':
                col_map[col] = 'analyst'
            elif 'property' in col_lower or col_lower == 'name':
                col_map[col] = 'property_name'
            elif 'region' in col_lower:
                col_map[col] = 'region'
            elif 'fund' in col_lower:
                col_map[col] = 'fund'
            elif 'unit' in col_lower:
                col_map[col] = 'units'
            elif 'email' in col_lower:
                col_map[col] = 'manager_email'

        df = df.rename(columns=col_map)
        print(f"[PORTFOLIO UPLOAD] Mapped columns: {col_map}")

        # Validate required columns
        required = ['property_code', 'property_name']
        missing = [c for c in required if c not in df.columns]
        if missing:
            return JSONResponse({
                "error": f"Missing required columns: {missing}. Found columns: {list(df.columns)}"
            }, status_code=400)

        # Load existing data
        existing = _load_portfolio_master()
        properties = existing.get("properties", {})

        # Process rows
        added = 0
        updated = 0
        skipped = 0

        for _, row in df.iterrows():
            code = str(row.get('property_code', '')).strip()
            if not code or code == 'nan':
                skipped += 1
                continue

            prop_data = {
                "property_code": code,
                "property_name": str(row.get('property_name', '')).strip() if pd.notna(row.get('property_name')) else '',
                "asset_manager": str(row.get('asset_manager', '')).strip() if pd.notna(row.get('asset_manager')) else '',
                "analyst": str(row.get('analyst', '')).strip() if pd.notna(row.get('analyst')) else '',
                "region": str(row.get('region', '')).strip() if pd.notna(row.get('region')) else '',
                "fund": str(row.get('fund', '')).strip() if pd.notna(row.get('fund')) else '',
                "units": int(row.get('units', 0)) if pd.notna(row.get('units')) else 0,
                "manager_email": str(row.get('manager_email', '')).strip() if pd.notna(row.get('manager_email')) else '',
            }

            if code in properties:
                updated += 1
            else:
                added += 1

            properties[code] = prop_data

        # Save updated data
        save_data = {
            "properties": properties,
            "updated_at": datetime.utcnow().isoformat(),
            "updated_by": user
        }
        _save_portfolio_master(save_data)

        print(f"[PORTFOLIO UPLOAD] Complete: {added} added, {updated} updated, {skipped} skipped")

        return {
            "ok": True,
            "added": added,
            "updated": updated,
            "skipped": skipped,
            "total": len(properties)
        }

    except Exception as e:
        print(f"[PORTFOLIO UPLOAD] Error: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse({"error": str(e)}, status_code=500)


@app.delete("/api/portfolio/{property_code}")
def api_portfolio_delete(property_code: str, user: str = Depends(require_user)):
    """Delete a property from the portfolio master."""
    from datetime import datetime

    try:
        data = _load_portfolio_master()
        properties = data.get("properties", {})

        if property_code not in properties:
            return JSONResponse({"error": "Property not found"}, status_code=404)

        del properties[property_code]

        data["properties"] = properties
        data["updated_at"] = datetime.utcnow().isoformat()
        data["updated_by"] = user

        _save_portfolio_master(data)

        return {"ok": True, "deleted": property_code}

    except Exception as e:
        print(f"[PORTFOLIO] Error deleting {property_code}: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@app.post("/api/portfolio/clear")
def api_portfolio_clear(user: str = Depends(require_user)):
    """Clear all portfolio data."""
    from datetime import datetime

    try:
        data = {
            "properties": {},
            "updated_at": datetime.utcnow().isoformat(),
            "updated_by": user
        }
        _save_portfolio_master(data)
        return {"ok": True, "message": "Portfolio data cleared"}
    except Exception as e:
        print(f"[PORTFOLIO] Error clearing: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


# ========== PRINT CHECKS & REVIEW CHECKS MODULE ==========

# -------- Posted Invoice Metadata (DynamoDB cache for CHECK REVIEW) --------

def _write_posted_invoice_metadata(pdf_id: str, s3_key: str, rows: list[dict]):
    """Write lightweight invoice metadata to DynamoDB at post time.
    This lets CHECK REVIEW query DynamoDB instead of scanning hundreds of S3 files.
    Schema: PK=POSTED_INVOICES, SK={posted_at}#{pdf_id}
    """
    if not rows:
        return
    try:
        first = rows[0]
        # Compute total from all line charges + detect late fees
        total = 0.0
        late_fee = 0.0
        for row in rows:
            charge = row.get("Line Item Charge") or row.get("line_item_charge") or 0
            try:
                amt = float(str(charge).replace("$", "").replace(",", ""))
                total += amt
                # Check if this line is a late fee
                desc = str(row.get("Line Item Description") or row.get("line_item_description") or "").lower()
                for pattern in LATE_FEE_PATTERNS:
                    if re.search(pattern, desc, re.IGNORECASE):
                        late_fee += abs(amt)
                        break  # Don't double-count if multiple patterns match
            except (ValueError, TypeError):
                pass

        # PostedAt is the actual Entrata posting time (set by post_to_entrata).
        # SubmittedAt is when the bill was submitted to review  NOT the posting date.
        # Fall back to current time if PostedAt is missing (old invoices before field was added).
        posted_at = first.get("PostedAt") or ""
        if not posted_at:
            posted_at = dt.datetime.utcnow().isoformat()
        # Build sort key: posted_at ISO + pdf_id for uniqueness
        sk = f"{posted_at}#{pdf_id}" if posted_at else f"unknown#{pdf_id}"

        item = {
            "PK": {"S": "POSTED_INVOICES"},
            "SK": {"S": sk},
            "pdf_id": {"S": pdf_id},
            "s3_key": {"S": s3_key},
            "property_id": {"S": str(first.get("EnrichedPropertyID") or first.get("propertyId") or first.get("Property Id") or "")},
            "property_name": {"S": str(first.get("EnrichedPropertyName") or first.get("propertyName") or first.get("Property Name") or "")},
            "vendor_id": {"S": str(first.get("EnrichedVendorID") or first.get("vendorId") or first.get("Vendor ID") or "")},
            "vendor_name": {"S": str(first.get("EnrichedVendorName") or first.get("vendorName") or first.get("Vendor Name") or "")},
            "account_number": {"S": str(first.get("Account Number") or first.get("accountNumber") or "")},
            "invoice_date": {"S": str(first.get("Bill Date") or first.get("billDate") or first.get("Invoice Date") or "")},
            "service_start": {"S": str(first.get("Bill Period Start") or first.get("billPeriodStart") or "")},
            "service_end": {"S": str(first.get("Bill Period End") or first.get("billPeriodEnd") or "")},
            "invoice_total": {"N": str(round(total, 2))},
            "late_fee": {"N": str(round(late_fee, 2))},
            "line_count": {"N": str(len(rows))},
            "posted_by": {"S": str(first.get("PostedBy") or first.get("submit_by") or "")},
            "posted_at": {"S": posted_at},
            "created_at": {"S": dt.datetime.utcnow().isoformat() + "Z"},
        }
        ddb.put_item(TableName=CONFIG_TABLE, Item=item)
    except Exception as e:
        print(f"[POSTED META] Error writing metadata for {pdf_id}: {e}")


def _delete_posted_invoice_metadata(pdf_id: str, posted_at: str = ""):
    """Delete posted invoice metadata from DynamoDB when invoice leaves Stage 7."""
    try:
        if posted_at:
            # Direct delete with known sort key
            sk = f"{posted_at}#{pdf_id}"
            ddb.delete_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POSTED_INVOICES"}, "SK": {"S": sk}})
        else:
            # Need to find the SK first  query by PK and filter by pdf_id
            resp = ddb.query(
                TableName=CONFIG_TABLE,
                KeyConditionExpression="PK = :pk",
                FilterExpression="pdf_id = :pid",
                ExpressionAttributeValues={":pk": {"S": "POSTED_INVOICES"}, ":pid": {"S": pdf_id}},
            )
            for item in resp.get("Items", []):
                ddb.delete_item(TableName=CONFIG_TABLE, Key={"PK": {"S": "POSTED_INVOICES"}, "SK": item["SK"]})
    except Exception as e:
        print(f"[POSTED META] Error deleting metadata for {pdf_id}: {e}")


def _load_posted_invoices_from_ddb(start_date=None, end_date=None) -> list[dict]:
    """Load posted invoice metadata from DynamoDB. Returns list of invoice dicts
    in the same format as the S3-based loader for backwards compatibility."""
    try:
        kwargs = {
            "TableName": CONFIG_TABLE,
            "KeyConditionExpression": "PK = :pk",
            "ExpressionAttributeValues": {":pk": {"S": "POSTED_INVOICES"}},
        }
        # Add SK range filter if dates provided
        if start_date and end_date:
            start_str = start_date.isoformat()
            end_str = (end_date + dt.timedelta(days=1)).isoformat()  # inclusive end
            kwargs["KeyConditionExpression"] += " AND SK BETWEEN :start AND :end"
            kwargs["ExpressionAttributeValues"][":start"] = {"S": start_str}
            kwargs["ExpressionAttributeValues"][":end"] = {"S": end_str}

        invoices = []
        while True:
            resp = ddb.query(**kwargs)
            for item in resp.get("Items", []):
                vid = item.get("vendor_id", {}).get("S", "")
                invoices.append({
                    "pdf_id": item.get("pdf_id", {}).get("S", ""),
                    "s3_key": item.get("s3_key", {}).get("S", ""),
                    "property_id": item.get("property_id", {}).get("S", ""),
                    "property_name": item.get("property_name", {}).get("S", ""),
                    "vendor_id": vid,
                    "vendor_name": item.get("vendor_name", {}).get("S", ""),
                    "vendor_code": "",  # Filled in by caller from vendor code cache
                    "account_number": item.get("account_number", {}).get("S", ""),
                    "invoice_date": item.get("invoice_date", {}).get("S", ""),
                    "service_start": item.get("service_start", {}).get("S", ""),
                    "service_end": item.get("service_end", {}).get("S", ""),
                    "invoice_total": float(item.get("invoice_total", {}).get("N", "0")),
                    "late_fee": float(item.get("late_fee", {}).get("N", "0")),
                    "line_count": int(item.get("line_count", {}).get("N", "0")),
                    "posted_by": item.get("posted_by", {}).get("S", ""),
                    "posted_at": item.get("posted_at", {}).get("S", ""),
                })
            if "LastEvaluatedKey" not in resp:
                break
            kwargs["ExclusiveStartKey"] = resp["LastEvaluatedKey"]

        print(f"[POSTED META] Loaded {len(invoices)} invoices from DynamoDB")
        return invoices
    except Exception as e:
        print(f"[POSTED META] Error loading from DynamoDB: {e}")
        return []


@app.get("/print-checks", response_class=HTMLResponse)
def page_print_checks(request: Request, user: str = Depends(require_user)):
    """Render PRINT CHECKS page for AP reps. Admin only."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("print_checks.html", {"request": request, "user": user})


@app.get("/review-checks", response_class=HTMLResponse)
def page_review_checks(request: Request, user: str = Depends(require_user)):
    """Render REVIEW CHECKS page for Treasury. Admin only."""
    if user not in ADMIN_USERS:
        return RedirectResponse("/", status_code=302)
    return templates.TemplateResponse("review_checks.html", {"request": request, "user": user})


@app.get("/api/print-checks/posted-invoices")
def api_print_checks_posted_invoices(
    request: Request,
    user: str = Depends(require_user),
    start: str = "",
    end: str = "",
    refresh: str = "0"
):
    """Load posted invoices from Stage 7 that are NOT already in check slips.
    Filters by PostedAt date (when invoice was actually posted), not S3 path date.
    Uses caching + parallel S3 reads for speed.
    Returns invoices grouped by vendor.
    """
    from collections import defaultdict
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import hashlib

    today = dt.date.today()

    # Default to last 1 day if not specified (filter by PostedAt, not S3 path)
    if not start:
        start_date = today - dt.timedelta(days=1)
    else:
        try:
            start_date = dt.datetime.strptime(start, "%Y-%m-%d").date()
        except:
            start_date = today - dt.timedelta(days=1)

    if not end:
        end_date = today
    else:
        try:
            end_date = dt.datetime.strptime(end, "%Y-%m-%d").date()
        except:
            end_date = today

    # Check cache first (unless refresh requested)
    cache = _PRINT_CHECKS_CACHE
    now = dt.datetime.now()
    cache_valid = (
        refresh != "1" and
        cache["last_refresh"] and
        (now - cache["last_refresh"]).total_seconds() < cache["ttl_seconds"] and
        cache["invoices"]
    )

    if cache_valid:
        print(f"[PRINT CHECKS] CACHE HIT - {len(cache['invoices'])} invoices cached")
        # Filter cached invoices by date range and exclude those now in check slips
        invoices_in_slips = _get_cached_invoices_in_slips()  # USE CACHED VERSION
        all_invoices = []
        for inv in cache["invoices"]:
            if inv["pdf_id"] in invoices_in_slips:
                continue
            # STRICT: Only include if PostedAt is valid and within range
            posted_at_str = inv.get("posted_at", "")
            if not posted_at_str:
                continue  # No posted_at = skip
            try:
                posted_date = dt.datetime.fromisoformat(posted_at_str.replace("Z", "")).date()
                if not (start_date <= posted_date <= end_date):
                    continue  # Outside date range = skip
            except:
                continue  # Invalid date = skip
            all_invoices.append(inv)
        print(f"[PRINT CHECKS] Filtered to {len(all_invoices)} invoices for date range")
    else:
        # Full refresh  try DynamoDB metadata first, fall back to S3
        import time as _time
        _start_time = _time.time()

        # Load vendor code lookup (CACHED - 1 hour)
        vendor_code_map = _get_cached_vendor_codes()

        # Get set of pdf_ids already in check slips (CACHED - 5 min, force refresh on full reload)
        invoices_in_slips = _get_cached_invoices_in_slips(force_refresh=(refresh == "1"))
        print(f"[PRINT CHECKS] {len(invoices_in_slips)} invoices already in check slips")

        # ---- Try DynamoDB metadata first (fast path) ----
        ddb_invoices = _load_posted_invoices_from_ddb()
        if ddb_invoices:
            print(f"[PRINT CHECKS] DDB FAST PATH  {len(ddb_invoices)} invoices from DynamoDB in {_time.time() - _start_time:.1f}s")
            # Fill in vendor codes and filter out slips
            all_cached_invoices = []
            for inv in ddb_invoices:
                if inv["pdf_id"] in invoices_in_slips:
                    continue
                inv["vendor_code"] = vendor_code_map.get(inv["vendor_id"], "")
                all_cached_invoices.append(inv)
        else:
            # ---- S3 fallback (slow path, used until backfill populates DDB) ----
            print(f"[PRINT CHECKS] CACHE MISS - Loading from S3 for {start_date} to {end_date}")

            # Step 1: List all S3 keys in Stage 7, Stage 8 (UBI assigned), AND Stage 99 (Archive) IN PARALLEL
            keys_to_process = []
            prefixes_to_scan = [POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX, HIST_ARCHIVE_PREFIX]

            def list_prefix(prefix):
                """List all JSONL keys under a prefix."""
                keys = []
                try:
                    paginator = s3.get_paginator('list_objects_v2')
                    for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                        for obj in page.get('Contents', []):
                            key = obj['Key']
                            if key.endswith('.jsonl'):
                                pdf_id = hashlib.sha1(key.encode()).hexdigest()
                                if pdf_id not in invoices_in_slips:
                                    keys.append((key, pdf_id))
                except Exception as e:
                    print(f"[PRINT CHECKS] Error listing {prefix}: {e}")
                return keys

            # Parallel S3 listing (dedicated check-review executor)
            list_futures = [_CHECK_REVIEW_EXECUTOR.submit(list_prefix, p) for p in prefixes_to_scan]
            for future in as_completed(list_futures):
                keys_to_process.extend(future.result())

            print(f"[PRINT CHECKS] Found {len(keys_to_process)} candidates from Stage 7 + Archive")

            # Step 2: Define worker function for parallel reads
            def process_invoice(key_pdf_tuple):
                key, pdf_id = key_pdf_tuple
                try:
                    resp = s3.get_object(Bucket=BUCKET, Key=key)
                    # Read up to 8MB to handle files with embedded PDFs (base64 encoded)
                    chunk = resp["Body"].read(8388608).decode("utf-8", errors="ignore")
                    first_line = chunk.split("\n")[0].strip()
                    if not first_line:
                        return None

                    # Try to parse first line; if it fails, try reading full file
                    try:
                        rec = json.loads(first_line)
                    except json.JSONDecodeError:
                        # First line might be incomplete - read the rest of the file
                        remaining = resp["Body"].read().decode("utf-8", errors="ignore")
                        chunk = chunk + remaining
                        first_line = chunk.split("\n")[0].strip()
                        rec = json.loads(first_line)
                    # Try PostedAt first, then SubmittedAt (the actual field name), then legacy submit_date
                    posted_at_str = rec.get("PostedAt") or rec.get("SubmittedAt") or rec.get("submit_date") or ""

                    # Calculate total from all lines
                    lines = chunk.strip().split("\n")
                    total = 0.0
                    line_count = 0
                    for line in lines:
                        try:
                            item = json.loads(line.strip())
                            charge = item.get("Line Item Charge") or item.get("line_item_charge") or 0
                            try:
                                total += float(str(charge).replace("$", "").replace(",", ""))
                            except:
                                pass
                            line_count += 1
                        except:
                            pass

                    vid = rec.get("EnrichedVendorID") or rec.get("vendorId") or ""
                    return {
                        "pdf_id": pdf_id,
                        "s3_key": key,
                        "property_id": rec.get("EnrichedPropertyID") or rec.get("propertyId") or "",
                        "property_name": rec.get("EnrichedPropertyName") or rec.get("propertyName") or rec.get("Property Name") or "",
                        "vendor_id": vid,
                        "vendor_name": rec.get("EnrichedVendorName") or rec.get("vendorName") or rec.get("Vendor Name") or "",
                        "vendor_code": vendor_code_map.get(vid, ""),
                        "account_number": rec.get("Account Number") or rec.get("accountNumber") or "",
                        "invoice_date": rec.get("Bill Date") or rec.get("billDate") or "",
                        "service_start": rec.get("Bill Period Start") or rec.get("billPeriodStart") or "",
                        "service_end": rec.get("Bill Period End") or rec.get("billPeriodEnd") or "",
                        "invoice_total": round(total, 2),
                        "line_count": line_count,
                        "posted_by": rec.get("PostedBy") or rec.get("submit_by") or "",
                        "posted_at": posted_at_str
                    }
                except Exception as e:
                    print(f"[PRINT CHECKS] Error reading {key}: {e}")
                    return None

            # Step 3: Parallel S3 reads using dedicated check-review executor
            all_cached_invoices = []
            futures = {_CHECK_REVIEW_EXECUTOR.submit(process_invoice, kp): kp for kp in keys_to_process}
            for future in as_completed(futures):
                result = future.result()
                if result:
                    all_cached_invoices.append(result)

            print(f"[PRINT CHECKS] Loaded {len(all_cached_invoices)} invoices from S3 in {_time.time() - _start_time:.1f}s")

        # Update cache with ALL invoices (no date filter - filter on read)
        cache["invoices"] = all_cached_invoices
        cache["last_refresh"] = now
        _elapsed = _time.time() - _start_time
        print(f"[PRINT CHECKS] Cached {len(all_cached_invoices)} invoices in {_elapsed:.1f}s (cache valid for 10 min)")

        # Now filter by date range for response - STRICT filtering
        all_invoices = []
        for inv in all_cached_invoices:
            posted_at_str = inv.get("posted_at", "")
            if not posted_at_str:
                continue  # No posted_at = skip
            try:
                posted_date = dt.datetime.fromisoformat(posted_at_str.replace("Z", "")).date()
                if not (start_date <= posted_date <= end_date):
                    continue  # Outside date range = skip
            except:
                continue  # Invalid date = skip
            all_invoices.append(inv)

        print(f"[PRINT CHECKS] {len(all_invoices)} invoices match date filter (total load time: {_time.time() - _start_time:.1f}s)")

    print(f"[PRINT CHECKS] Found {len(all_invoices)} available invoices")

    # Group by vendor
    by_vendor = defaultdict(lambda: {"invoices": [], "total": 0.0, "count": 0})
    for inv in all_invoices:
        vid = inv["vendor_id"]
        vname = inv["vendor_name"]
        vcode = inv.get("vendor_code", "")
        key = f"{vid}|{vname}"
        by_vendor[key]["vendor_id"] = vid
        by_vendor[key]["vendor_name"] = vname
        by_vendor[key]["vendor_code"] = vcode
        by_vendor[key]["invoices"].append(inv)
        by_vendor[key]["total"] += inv["invoice_total"]
        by_vendor[key]["count"] += 1

    # Sort invoices within each vendor by account number
    for vendor_data in by_vendor.values():
        vendor_data["invoices"].sort(key=lambda x: str(x.get("account_number", "")).lower())

    # Convert to list and sort by vendor name
    vendors = sorted(by_vendor.values(), key=lambda x: x.get("vendor_name", "").lower())

    return {
        "vendors": vendors,
        "total_invoices": len(all_invoices),
        "total_amount": round(sum(inv["invoice_total"] for inv in all_invoices), 2),
        "vendor_count": len(vendors),
        "date_range": {"start": start_date.isoformat(), "end": end_date.isoformat()}
    }


@app.post("/api/print-checks/create-slip")
async def api_print_checks_create_slip(
    request: Request,
    user: str = Depends(require_user)
):
    """Create a new check slip from selected invoices."""
    try:
        body = await request.json()
        vendor_id = body.get("vendor_id", "")
        vendor_name = body.get("vendor_name", "")
        vendor_code = body.get("vendor_code", "")
        invoice_data = body.get("invoices", [])  # List of {pdf_id, s3_key, property_name, account_number, invoice_date, invoice_total}
        # Accept client's local date to avoid timezone mismatch
        client_date = body.get("created_date", "")

        if not invoice_data:
            return JSONResponse({"error": "No invoices selected"}, status_code=400)

        # Calculate totals
        total_amount = sum(float(inv.get("invoice_total", 0)) for inv in invoice_data)
        invoice_count = len(invoice_data)

        # Generate check slip ID
        check_slip_id = _generate_check_slip_id()
        now_utc = dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        # Use client's date if provided (avoids timezone mismatch), else server date
        today_str = client_date if client_date else dt.date.today().isoformat()

        # Create check slip record
        slip = {
            "check_slip_id": check_slip_id,
            "created_date": today_str,
            "created_at": now_utc,
            "created_by": user,
            "vendor_id": vendor_id,
            "vendor_name": vendor_name,
            "vendor_code": vendor_code,
            "total_amount": round(total_amount, 2),
            "invoice_count": invoice_count,
            "invoices": invoice_data,
            "status": "PENDING",
            "approved_by": "",
            "approved_at": "",
            "notes": ""
        }

        # Save check slip
        if not _ddb_create_check_slip(slip):
            return JSONResponse({"error": "Failed to create check slip"}, status_code=500)

        # Invalidate the posted invoices cache since we just used some
        _invalidate_print_checks_cache()

        # Mark all invoices as belonging to this check slip
        for inv in invoice_data:
            pdf_id = inv.get("pdf_id", "")
            if pdf_id:
                _ddb_add_invoice_to_check_slip(pdf_id, check_slip_id)

        print(f"[PRINT CHECKS] Created check slip {check_slip_id} with {invoice_count} invoices, total ${total_amount:.2f}")

        return {
            "ok": True,
            "check_slip_id": check_slip_id,
            "invoice_count": invoice_count,
            "total_amount": round(total_amount, 2)
        }

    except Exception as e:
        print(f"[PRINT CHECKS] Error creating slip: {e}")
        return JSONResponse({"error": str(e)}, status_code=500)


@app.get("/api/print-checks/my-slips")
def api_print_checks_my_slips(user: str = Depends(require_user), status: str = ""):
    """List check slips created by the current user."""
    slips = _ddb_list_check_slips_by_user(user, status)
    # Sort by created_at descending
    slips.sort(key=lambda x: x.get("created_at", ""), reverse=True)
    return {"slips": slips}


@app.get("/api/print-checks/slip/{check_slip_id}")
def api_print_checks_get_slip(check_slip_id: str, user: str = Depends(require_user)):
    """Get details of a specific check slip."""
    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)
    return slip


@app.delete("/api/print-checks/slip/{check_slip_id}")
def api_print_checks_delete_slip(check_slip_id: str, user: str = Depends(require_user)):
    """Delete a pending check slip and release its invoices back to the pool."""
    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    if slip.get("status") != "PENDING":
        return JSONResponse({"error": "Can only delete PENDING check slips"}, status_code=400)

    # Only allow creator or admin to delete
    if slip.get("created_by") != user and user not in ADMIN_USERS:
        return JSONResponse({"error": "Not authorized to delete this check slip"}, status_code=403)

    # Collect posted_at dates from released invoices so frontend can auto-expand date range
    released_dates = []

    # Release all invoices from this check slip
    for inv in slip.get("invoices", []):
        pdf_id = inv.get("pdf_id", "")
        if pdf_id:
            _ddb_remove_invoice_from_check_slip(pdf_id)
            # Try to find the posting date for this invoice from DDB
            posted_at = inv.get("posted_at", "") or inv.get("invoice_date", "")
            if posted_at:
                released_dates.append(posted_at[:10])  # Just the date part

    # If we couldn't get dates from the slip, look them up from POSTED_INVOICES
    if not released_dates:
        for inv in slip.get("invoices", []):
            pid = inv.get("pdf_id", "")
            if not pid:
                continue
            try:
                resp = ddb.query(
                    TableName=CONFIG_TABLE,
                    KeyConditionExpression="PK = :pk",
                    FilterExpression="pdf_id = :pid",
                    ExpressionAttributeValues={":pk": {"S": "POSTED_INVOICES"}, ":pid": {"S": pid}},
                    ProjectionExpression="posted_at",
                    Limit=1,
                )
                for item in resp.get("Items", []):
                    pa = item.get("posted_at", {}).get("S", "")
                    if pa:
                        released_dates.append(pa[:10])
            except Exception:
                pass

    # Delete the check slip
    _ddb_delete_check_slip(check_slip_id)

    # Invalidate cache since invoices are back in the pool
    _invalidate_print_checks_cache()

    earliest_date = min(released_dates) if released_dates else ""
    released_count = len(slip.get("invoices", []))
    print(f"[PRINT CHECKS] Deleted check slip {check_slip_id}, released {released_count} invoices (earliest={earliest_date})")

    return {"ok": True, "message": f"Check slip {check_slip_id} deleted", "released_count": released_count, "earliest_date": earliest_date}


@app.get("/api/print-checks/slip/{check_slip_id}/pdf")
def api_print_checks_slip_pdf(check_slip_id: str, user: str = Depends(require_user)):
    """Generate PDF for a check slip with all source invoice PDFs appended."""
    from io import BytesIO
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT, TA_CENTER
    from PyPDF2 import PdfReader, PdfWriter

    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    # Generate the cover page
    cover_buffer = BytesIO()
    doc = SimpleDocTemplate(cover_buffer, pagesize=letter, topMargin=0.5*inch, bottomMargin=0.5*inch,
                            leftMargin=0.5*inch, rightMargin=0.5*inch)

    # Colors
    NAVY = colors.HexColor("#1e3a5f")
    LIGHT_BLUE = colors.HexColor("#e8eef3")
    LIGHT_GREY = colors.HexColor("#f5f5f5")

    styles = getSampleStyleSheet()
    title_style = ParagraphStyle('Title', parent=styles['Heading1'], fontSize=20, textColor=NAVY,
                                  alignment=TA_CENTER, spaceAfter=6)
    subtitle_style = ParagraphStyle('Subtitle', parent=styles['Normal'], fontSize=14, textColor=NAVY,
                                     alignment=TA_CENTER, spaceAfter=12)
    normal_style = styles['Normal']
    small_style = ParagraphStyle('Small', parent=styles['Normal'], fontSize=9)

    story = []

    # Header
    story.append(Paragraph("JRK RESIDENTIAL", title_style))
    story.append(Paragraph("CHECK SLIP", subtitle_style))
    story.append(Spacer(1, 0.2*inch))

    # Check slip info table
    created_date = slip.get("created_at", "")[:10]
    try:
        from datetime import datetime
        dt_obj = datetime.fromisoformat(slip.get("created_at", "").replace("Z", "+00:00"))
        created_date = dt_obj.strftime("%B %d, %Y")
    except:
        pass

    info_data = [
        [f"Check Slip #: {slip.get('check_slip_id', '')}", f"Date: {created_date}"],
        [f"Vendor: {slip.get('vendor_name', '')}", f"Vendor Code: {slip.get('vendor_code', '')}"],
    ]
    info_table = Table(info_data, colWidths=[4*inch, 3.5*inch])
    info_table.setStyle(TableStyle([
        ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
        ('FONTSIZE', (0, 0), (-1, -1), 11),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
    ]))
    story.append(info_table)
    story.append(Spacer(1, 0.3*inch))

    # Invoice table header
    table_data = [["PROPERTY", "ACCOUNT #", "INVOICE DATE", "AMOUNT"]]

    # Add invoice rows
    invoices = slip.get("invoices", [])
    for inv in invoices:
        amount = inv.get("invoice_total", 0)
        try:
            amount = float(amount)
        except:
            amount = 0
        table_data.append([
            str(inv.get("property_name", ""))[:30],
            str(inv.get("account_number", "")),
            str(inv.get("invoice_date", "")),
            f"${amount:,.2f}"
        ])

    # Total row
    total = slip.get("total_amount", 0)
    table_data.append([f"TOTAL ({len(invoices)} invoices)", "", "", f"${total:,.2f}"])

    inv_table = Table(table_data, colWidths=[2.5*inch, 1.8*inch, 1.5*inch, 1.5*inch])
    inv_table.setStyle(TableStyle([
        ('BACKGROUND', (0, 0), (-1, 0), NAVY),
        ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
        ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
        ('FONTSIZE', (0, 0), (-1, 0), 10),
        ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
        ('FONTNAME', (0, 1), (-1, -2), 'Helvetica'),
        ('FONTSIZE', (0, 1), (-1, -2), 9),
        ('ALIGN', (3, 1), (3, -1), 'RIGHT'),
        ('BACKGROUND', (0, -1), (-1, -1), LIGHT_BLUE),
        ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
        ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
        ('TOPPADDING', (0, 0), (-1, -1), 6),
        ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
        ('ROWBACKGROUNDS', (0, 1), (-1, -2), [colors.white, LIGHT_GREY]),
    ]))
    story.append(inv_table)
    story.append(Spacer(1, 0.5*inch))

    # Footer info
    story.append(Paragraph(f"Created By: {slip.get('created_by', '')}", small_style))
    story.append(Paragraph(f"Created At: {slip.get('created_at', '')}", small_style))
    story.append(Spacer(1, 0.2*inch))

    status = slip.get("status", "PENDING")
    if status == "APPROVED":
        story.append(Paragraph(f"<b>Status: APPROVED</b>", normal_style))
        story.append(Paragraph(f"Approved By: {slip.get('approved_by', '')}", small_style))
        story.append(Paragraph(f"Approved At: {slip.get('approved_at', '')}", small_style))
    else:
        story.append(Paragraph("<b>Status: PENDING APPROVAL</b>", normal_style))
        story.append(Spacer(1, 0.5*inch))
        story.append(Paragraph("_" * 60, normal_style))
        story.append(Paragraph("Treasury Approval Signature                                          Date", small_style))

    doc.build(story)
    cover_buffer.seek(0)

    # Create PDF writer and add cover page
    pdf_writer = PdfWriter()
    cover_reader = PdfReader(cover_buffer)
    for page in cover_reader.pages:
        pdf_writer.add_page(page)

    # Fetch all source invoice PDFs in PARALLEL for speed
    from concurrent.futures import ThreadPoolExecutor, as_completed
    pdf_bucket = os.getenv("SCRAPER_BUCKET", "jrk-utility-pdfs")
    pdf_errors = []  # Track errors for each invoice
    successful_pdfs = 0

    def _read_jsonl_and_fetch_pdf(jsonl_key):
        """Read a JSONL file from S3 and fetch its source PDF. Returns (pdf_bytes, error_str)."""
        obj = s3.get_object(Bucket=BUCKET, Key=jsonl_key)
        content = obj["Body"].read().decode("utf-8", errors="ignore")
        lines = [ln.strip() for ln in content.strip().split("\n") if ln.strip()]
        if not lines:
            return (None, "Invoice file is empty")

        first_rec = json.loads(lines[0])
        pdf_key = first_rec.get("source_input_key") or first_rec.get("PDF_LINK") or first_rec.get("pdf_key") or ""
        if not pdf_key:
            return (None, "No source PDF path in invoice data")

        source_bucket = BUCKET
        if pdf_key.startswith("s3://"):
            parts = pdf_key.replace("s3://", "").split("/", 1)
            if len(parts) == 2:
                source_bucket = parts[0]
                pdf_key = parts[1]
        elif "jrk-utility-pdfs" in pdf_key or not pdf_key.startswith("Bill_Parser"):
            source_bucket = pdf_bucket

        pdf_obj = s3.get_object(Bucket=source_bucket, Key=pdf_key)
        pdf_bytes = pdf_obj["Body"].read()
        print(f"[CHECK SLIP PDF] Fetched PDF from {source_bucket}/{pdf_key}")
        return (pdf_bytes, None)

    def fetch_invoice_pdf(inv_tuple):
        """Fetch a single invoice PDF. Returns (index, pdf_bytes, error_info)."""
        idx, inv = inv_tuple
        s3_key = inv.get("s3_key", "")
        property_name = inv.get("property_name", "Unknown")
        account_number = inv.get("account_number", "Unknown")

        if not s3_key:
            return (idx, None, {"property": property_name, "account": account_number, "error": "No S3 key found for invoice"})

        try:
            pdf_bytes, err = _read_jsonl_and_fetch_pdf(s3_key)
            if err:
                return (idx, None, {"property": property_name, "account": account_number, "error": err})
            return (idx, pdf_bytes, None)

        except Exception as e:
            error_msg = str(e)
            # If the JSONL file moved stages, try to find it elsewhere
            if "NoSuchKey" in error_msg:
                print(f"[CHECK SLIP PDF] JSONL not found at {s3_key}, searching other stages...")
                found_key = _fallback_find_jsonl_for_invoice(s3_key, account_number)
                if found_key:
                    try:
                        pdf_bytes, err = _read_jsonl_and_fetch_pdf(found_key)
                        if err:
                            return (idx, None, {"property": property_name, "account": account_number, "error": err})
                        print(f"[CHECK SLIP PDF] Fallback: found {found_key} (was {s3_key})")
                        return (idx, pdf_bytes, None)
                    except Exception as e2:
                        error_msg = str(e2)
                else:
                    error_msg = "PDF file not found in S3"
            if "NoSuchKey" in error_msg:
                error_msg = "PDF file not found in S3"
            elif "AccessDenied" in error_msg:
                error_msg = "Access denied to PDF file"
            print(f"[CHECK SLIP PDF] Error fetching PDF for {s3_key}: {e}")
            return (idx, None, {"property": property_name, "account": account_number, "error": error_msg[:200]})

    # Fetch all PDFs in parallel using dedicated check-review executor
    pdf_results = {}
    futures = {_CHECK_REVIEW_EXECUTOR.submit(fetch_invoice_pdf, (i, inv)): i for i, inv in enumerate(invoices)}
    for future in as_completed(futures):
        idx, pdf_bytes, error = future.result()
        pdf_results[idx] = (pdf_bytes, error)

    # Add PDFs to writer IN ORDER (must be sequential for proper page ordering)
    for idx in range(len(invoices)):
        pdf_bytes, error = pdf_results.get(idx, (None, None))
        if error:
            pdf_errors.append(error)
            continue
        if pdf_bytes:
            try:
                pdf_buffer = BytesIO(pdf_bytes)
                pdf_reader = PdfReader(pdf_buffer)
                for page in pdf_reader.pages:
                    pdf_writer.add_page(page)
                print(f"[CHECK SLIP PDF] Added {len(pdf_reader.pages)} pages for invoice {idx}")
                successful_pdfs += 1
            except Exception as e:
                inv = invoices[idx]
                pdf_errors.append({
                    "property": inv.get("property_name", "Unknown"),
                    "account": inv.get("account_number", "Unknown"),
                    "error": f"Failed to parse PDF: {str(e)[:100]}"
                })

    # Store PDF errors in DynamoDB for later retrieval
    _ddb_update_check_slip_pdf_errors(check_slip_id, pdf_errors)

    print(f"[CHECK SLIP PDF] Generated PDF with {successful_pdfs}/{len(invoices)} invoices. Errors: {len(pdf_errors)}")

    # Write final merged PDF
    output_buffer = BytesIO()
    pdf_writer.write(output_buffer)
    output_buffer.seek(0)

    # Build filename: {Vendor Name}-{Check Slip Date}-{Check Slip ID}.pdf
    import re
    safe_vendor = re.sub(r'[^\w\s-]', '', slip.get('vendor_name', 'Unknown')).strip().replace(' ', '_')[:30]
    slip_date = slip.get('created_date', '')
    filename = f"{safe_vendor}-{slip_date}-{check_slip_id}.pdf"
    return StreamingResponse(
        output_buffer,
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


@app.get("/api/print-checks/bulk-pdf")
def api_print_checks_bulk_pdf(ids: str = "", user: str = Depends(require_user)):
    """Merge multiple check slip PDFs into one document."""
    from io import BytesIO
    from reportlab.lib import colors
    from reportlab.lib.pagesizes import letter
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
    from reportlab.lib.enums import TA_LEFT, TA_RIGHT, TA_CENTER
    from PyPDF2 import PdfReader, PdfWriter
    from concurrent.futures import as_completed
    import re

    slip_ids = [s.strip() for s in ids.split(",") if s.strip()]
    if not slip_ids:
        return JSONResponse({"error": "No check slip IDs provided"}, status_code=400)
    if len(slip_ids) > 50:
        return JSONResponse({"error": "Too many slips (max 50)"}, status_code=400)

    pdf_writer = PdfWriter()
    pdf_bucket = os.getenv("SCRAPER_BUCKET", "jrk-utility-pdfs")
    slip_count = 0

    for slip_id in slip_ids:
        slip = _ddb_get_check_slip(slip_id)
        if not slip:
            continue

        # ---- Cover page (same layout as single-slip endpoint) ----
        NAVY = colors.HexColor("#1e3a5f")
        LIGHT_BLUE = colors.HexColor("#e8eef3")
        LIGHT_GREY = colors.HexColor("#f5f5f5")

        styles = getSampleStyleSheet()
        title_style = ParagraphStyle('Title', parent=styles['Heading1'], fontSize=20, textColor=NAVY,
                                      alignment=TA_CENTER, spaceAfter=6)
        subtitle_style = ParagraphStyle('Subtitle', parent=styles['Normal'], fontSize=14, textColor=NAVY,
                                         alignment=TA_CENTER, spaceAfter=12)
        normal_style = styles['Normal']
        small_style = ParagraphStyle('Small', parent=styles['Normal'], fontSize=9)

        story = []
        story.append(Paragraph("JRK RESIDENTIAL", title_style))
        story.append(Paragraph("CHECK SLIP", subtitle_style))
        story.append(Spacer(1, 0.2 * inch))

        created_date = slip.get("created_at", "")[:10]
        try:
            from datetime import datetime as _dt
            dt_obj = _dt.fromisoformat(slip.get("created_at", "").replace("Z", "+00:00"))
            created_date = dt_obj.strftime("%B %d, %Y")
        except Exception:
            pass

        info_data = [
            [f"Check Slip #: {slip.get('check_slip_id', '')}", f"Date: {created_date}"],
            [f"Vendor: {slip.get('vendor_name', '')}", f"Vendor Code: {slip.get('vendor_code', '')}"],
        ]
        info_table = Table(info_data, colWidths=[4 * inch, 3.5 * inch])
        info_table.setStyle(TableStyle([
            ('FONTNAME', (0, 0), (-1, -1), 'Helvetica'),
            ('FONTSIZE', (0, 0), (-1, -1), 11),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 8),
        ]))
        story.append(info_table)
        story.append(Spacer(1, 0.3 * inch))

        invoices = slip.get("invoices", [])
        table_data = [["PROPERTY", "ACCOUNT #", "INVOICE DATE", "AMOUNT"]]
        for inv in invoices:
            amount = inv.get("invoice_total", 0)
            try:
                amount = float(amount)
            except Exception:
                amount = 0
            table_data.append([
                str(inv.get("property_name", ""))[:30],
                str(inv.get("account_number", "")),
                str(inv.get("invoice_date", "")),
                f"${amount:,.2f}"
            ])
        total = slip.get("total_amount", 0)
        table_data.append([f"TOTAL ({len(invoices)} invoices)", "", "", f"${total:,.2f}"])

        inv_table = Table(table_data, colWidths=[2.5 * inch, 1.8 * inch, 1.5 * inch, 1.5 * inch])
        inv_table.setStyle(TableStyle([
            ('BACKGROUND', (0, 0), (-1, 0), NAVY),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.white),
            ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
            ('FONTSIZE', (0, 0), (-1, 0), 10),
            ('ALIGN', (0, 0), (-1, 0), 'CENTER'),
            ('FONTNAME', (0, 1), (-1, -2), 'Helvetica'),
            ('FONTSIZE', (0, 1), (-1, -2), 9),
            ('ALIGN', (3, 1), (3, -1), 'RIGHT'),
            ('BACKGROUND', (0, -1), (-1, -1), LIGHT_BLUE),
            ('FONTNAME', (0, -1), (-1, -1), 'Helvetica-Bold'),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('TOPPADDING', (0, 0), (-1, -1), 6),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 6),
            ('ROWBACKGROUNDS', (0, 1), (-1, -2), [colors.white, LIGHT_GREY]),
        ]))
        story.append(inv_table)
        story.append(Spacer(1, 0.5 * inch))

        story.append(Paragraph(f"Created By: {slip.get('created_by', '')}", small_style))
        story.append(Paragraph(f"Created At: {slip.get('created_at', '')}", small_style))
        story.append(Spacer(1, 0.2 * inch))

        status = slip.get("status", "PENDING")
        if status == "APPROVED":
            story.append(Paragraph("<b>Status: APPROVED</b>", normal_style))
            story.append(Paragraph(f"Approved By: {slip.get('approved_by', '')}", small_style))
            story.append(Paragraph(f"Approved At: {slip.get('approved_at', '')}", small_style))
        else:
            story.append(Paragraph("<b>Status: PENDING APPROVAL</b>", normal_style))
            story.append(Spacer(1, 0.5 * inch))
            story.append(Paragraph("_" * 60, normal_style))
            story.append(Paragraph("Treasury Approval Signature                                          Date", small_style))

        cover_buffer = BytesIO()
        doc = SimpleDocTemplate(cover_buffer, pagesize=letter, topMargin=0.5 * inch, bottomMargin=0.5 * inch,
                                leftMargin=0.5 * inch, rightMargin=0.5 * inch)
        doc.build(story)
        cover_buffer.seek(0)

        cover_reader = PdfReader(cover_buffer)
        for page in cover_reader.pages:
            pdf_writer.add_page(page)

        # ---- Invoice PDFs ----
        def _fetch_inv_pdf(inv_tuple):
            idx, inv = inv_tuple
            s3_key = inv.get("s3_key", "")
            if not s3_key:
                return (idx, None)
            try:
                obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
                content = obj["Body"].read().decode("utf-8", errors="ignore")
                lines = [ln.strip() for ln in content.strip().split("\n") if ln.strip()]
                if not lines:
                    return (idx, None)
                first_rec = json.loads(lines[0])
                pdf_key = first_rec.get("source_input_key") or first_rec.get("PDF_LINK") or first_rec.get("pdf_key") or ""
                if not pdf_key:
                    return (idx, None)
                source_bucket = BUCKET
                if pdf_key.startswith("s3://"):
                    parts = pdf_key.replace("s3://", "").split("/", 1)
                    if len(parts) == 2:
                        source_bucket = parts[0]
                        pdf_key = parts[1]
                elif "jrk-utility-pdfs" in pdf_key or not pdf_key.startswith("Bill_Parser"):
                    source_bucket = pdf_bucket
                pdf_obj = s3.get_object(Bucket=source_bucket, Key=pdf_key)
                return (idx, pdf_obj["Body"].read())
            except Exception as e:
                if "NoSuchKey" in str(e):
                    found_key = _fallback_find_jsonl_for_invoice(s3_key, inv.get("account_number", ""))
                    if found_key:
                        try:
                            obj = s3.get_object(Bucket=BUCKET, Key=found_key)
                            content = obj["Body"].read().decode("utf-8", errors="ignore")
                            lines = [ln.strip() for ln in content.strip().split("\n") if ln.strip()]
                            if lines:
                                first_rec = json.loads(lines[0])
                                pdf_key = first_rec.get("source_input_key") or first_rec.get("PDF_LINK") or first_rec.get("pdf_key") or ""
                                if pdf_key:
                                    source_bucket = BUCKET
                                    if pdf_key.startswith("s3://"):
                                        parts = pdf_key.replace("s3://", "").split("/", 1)
                                        if len(parts) == 2:
                                            source_bucket = parts[0]
                                            pdf_key = parts[1]
                                    elif "jrk-utility-pdfs" in pdf_key or not pdf_key.startswith("Bill_Parser"):
                                        source_bucket = pdf_bucket
                                    pdf_obj = s3.get_object(Bucket=source_bucket, Key=pdf_key)
                                    return (idx, pdf_obj["Body"].read())
                        except Exception:
                            pass
                return (idx, None)

        pdf_results = {}
        futures = {_CHECK_REVIEW_EXECUTOR.submit(_fetch_inv_pdf, (i, inv)): i for i, inv in enumerate(invoices)}
        for future in as_completed(futures):
            idx, pdf_bytes = future.result()
            pdf_results[idx] = pdf_bytes

        for idx in range(len(invoices)):
            pdf_bytes = pdf_results.get(idx)
            if pdf_bytes:
                try:
                    reader = PdfReader(BytesIO(pdf_bytes))
                    for page in reader.pages:
                        pdf_writer.add_page(page)
                except Exception:
                    pass

        slip_count += 1

    if slip_count == 0:
        return JSONResponse({"error": "No valid check slips found"}, status_code=404)

    output_buffer = BytesIO()
    pdf_writer.write(output_buffer)
    output_buffer.seek(0)

    filename = f"check_slips_bulk_{len(slip_ids)}.pdf"
    return StreamingResponse(
        output_buffer,
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )


@app.get("/api/print-checks/slip/{check_slip_id}/pdf-status")
def api_print_checks_pdf_status(check_slip_id: str, user: str = Depends(require_user)):
    """Get PDF generation status and any errors for a check slip."""
    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    pdf_errors = slip.get("pdf_errors", [])
    pdf_generated_at = slip.get("pdf_generated_at", "")
    total_invoices = slip.get("invoice_count", 0)
    successful = total_invoices - len(pdf_errors)

    return {
        "check_slip_id": check_slip_id,
        "total_invoices": total_invoices,
        "successful_pdfs": successful,
        "failed_pdfs": len(pdf_errors),
        "pdf_errors": pdf_errors,
        "pdf_generated_at": pdf_generated_at,
        "has_errors": len(pdf_errors) > 0
    }


# ========== REVIEW CHECKS API ENDPOINTS ==========

@app.get("/api/review-checks/pending")
def api_review_checks_pending(user: str = Depends(require_user), date: str = ""):
    """Load pending check slips for a given date (default: today)."""
    if not date:
        date = dt.date.today().isoformat()

    slips = _ddb_list_all_check_slips_for_date(date)
    # Sort: PENDING first, then by created_at descending
    slips.sort(key=lambda x: (0 if x.get("status") == "PENDING" else 1, x.get("created_at", "")), reverse=False)

    # Separate by status
    pending = [s for s in slips if s.get("status") == "PENDING"]
    approved = [s for s in slips if s.get("status") == "APPROVED"]

    return {
        "date": date,
        "pending": pending,
        "approved": approved,
        "pending_count": len(pending),
        "approved_count": len(approved),
        "pending_total": round(sum(s.get("total_amount", 0) for s in pending), 2),
        "approved_total": round(sum(s.get("total_amount", 0) for s in approved), 2)
    }


@app.get("/api/review-checks/slip/{check_slip_id}")
def api_review_checks_get_slip(check_slip_id: str, user: str = Depends(require_user)):
    """Get full details of a check slip for review."""
    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)
    return slip


@app.get("/api/review-checks/slip/{check_slip_id}/invoice/{invoice_index}/pdf")
def api_review_checks_invoice_pdf(check_slip_id: str, invoice_index: int, user: str = Depends(require_user)):
    """Get the source PDF for a specific invoice in a check slip."""
    from io import BytesIO

    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    invoices = slip.get("invoices", [])
    if invoice_index < 0 or invoice_index >= len(invoices):
        return JSONResponse({"error": "Invoice not found"}, status_code=404)

    inv = invoices[invoice_index]
    s3_key = inv.get("s3_key", "")
    if not s3_key:
        return JSONResponse({"error": "No S3 key for this invoice"}, status_code=404)

    try:
        # Read the JSONL file to get the source PDF path
        resolved_key = s3_key
        try:
            obj = s3.get_object(Bucket=BUCKET, Key=s3_key)
        except Exception as s3e:
            if "NoSuchKey" in str(s3e):
                # JSONL may have moved stages - search Stage 7/8/99
                account_number = inv.get("account_number", "")
                found = _fallback_find_jsonl_for_invoice(s3_key, account_number)
                if found:
                    print(f"[INVOICE PDF] Fallback: found {found} (was {s3_key})")
                    resolved_key = found
                    obj = s3.get_object(Bucket=BUCKET, Key=found)
                else:
                    raise
            else:
                raise

        content = obj["Body"].read().decode("utf-8", errors="ignore")
        lines = [ln.strip() for ln in content.strip().split("\n") if ln.strip()]
        if not lines:
            return JSONResponse({"error": "Empty JSONL file"}, status_code=404)

        first_rec = json.loads(lines[0])

        # Get the source PDF key - try multiple field names
        pdf_key = first_rec.get("source_input_key") or first_rec.get("PDF_LINK") or first_rec.get("pdf_key") or ""

        if not pdf_key:
            return JSONResponse({"error": "No PDF key found in invoice data"}, status_code=404)

        # Determine which bucket the PDF is in
        pdf_bucket = os.getenv("SCRAPER_BUCKET", "jrk-utility-pdfs")
        source_bucket = BUCKET
        if pdf_key.startswith("s3://"):
            parts = pdf_key.replace("s3://", "").split("/", 1)
            if len(parts) == 2:
                source_bucket = parts[0]
                pdf_key = parts[1]
        elif "jrk-utility-pdfs" in pdf_key or not pdf_key.startswith("Bill_Parser"):
            source_bucket = pdf_bucket

        print(f"[INVOICE PDF] Fetching PDF from {source_bucket}/{pdf_key}")

        # Fetch the PDF
        pdf_obj = s3.get_object(Bucket=source_bucket, Key=pdf_key)
        pdf_bytes = pdf_obj["Body"].read()

        # Extract filename from key
        filename = pdf_key.split("/")[-1] if "/" in pdf_key else pdf_key

        return StreamingResponse(
            BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={"Content-Disposition": f"inline; filename={filename}"}
        )

    except Exception as e:
        print(f"[INVOICE PDF] Error: {e}")
        return JSONResponse({"error": f"Failed to fetch PDF: {str(e)}"}, status_code=500)


@app.post("/api/review-checks/approve/{check_slip_id}")
async def api_review_checks_approve(
    check_slip_id: str,
    request: Request,
    user: str = Depends(require_user)
):
    """Approve a check slip."""
    try:
        body = await request.json()
        notes = body.get("notes", "")
    except:
        notes = ""

    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    if slip.get("status") != "PENDING":
        return JSONResponse({"error": "Check slip is not pending"}, status_code=400)

    now_utc = dt.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")

    success = _ddb_update_check_slip_status(
        check_slip_id,
        status="APPROVED",
        approved_by=user,
        approved_at=now_utc,
        notes=notes
    )

    if not success:
        return JSONResponse({"error": "Failed to approve check slip"}, status_code=500)

    print(f"[REVIEW CHECKS] Approved check slip {check_slip_id} by {user}")

    return {"ok": True, "message": f"Check slip {check_slip_id} approved"}


@app.post("/api/review-checks/reject/{check_slip_id}")
async def api_review_checks_reject(
    check_slip_id: str,
    request: Request,
    user: str = Depends(require_user)
):
    """Reject a check slip - releases invoices back to the pool."""
    try:
        body = await request.json()
        notes = body.get("notes", "")
    except:
        notes = ""

    slip = _ddb_get_check_slip(check_slip_id)
    if not slip:
        return JSONResponse({"error": "Check slip not found"}, status_code=404)

    if slip.get("status") != "PENDING":
        return JSONResponse({"error": "Check slip is not pending"}, status_code=400)

    # Release all invoices from this check slip
    for inv in slip.get("invoices", []):
        pdf_id = inv.get("pdf_id", "")
        if pdf_id:
            _ddb_remove_invoice_from_check_slip(pdf_id)

    # Delete the check slip (or could mark as REJECTED if we want history)
    _ddb_delete_check_slip(check_slip_id)

    # Invalidate cache since invoices are back in the pool
    _invalidate_print_checks_cache()

    print(f"[REVIEW CHECKS] Rejected check slip {check_slip_id} by {user}, released {len(slip.get('invoices', []))} invoices")

    return {"ok": True, "message": f"Check slip {check_slip_id} rejected, invoices released"}


# ============================================================================
# KNOWLEDGE BASE - AP institutional knowledge storage
# ============================================================================

@app.get("/knowledge-base", response_class=HTMLResponse)
def knowledge_base_view(request: Request, user: str = Depends(require_user)):
    """Knowledge base UI for documenting vendor/property knowledge."""
    return templates.TemplateResponse("knowledge_base.html", {"request": request, "user": user})


@app.get("/api/knowledge")
def api_knowledge_list(
    entity_type: str = None,  # vendor, property, account
    entity_id: str = None,
    category: str = None,
    search: str = None,
    limit: int = 100,
    user: str = Depends(require_user)
):
    """Get knowledge notes, filtered by entity, category, or search term."""
    try:
        items = []

        if entity_type and entity_id:
            # Query by specific entity
            pk = f"{entity_type.upper()}#{entity_id}"
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": pk}},
                ScanIndexForward=False,  # newest first
                Limit=limit
            )
            items = resp.get("Items", [])
        elif search:
            # Scan with filter for search term (not efficient but works for small datasets)
            resp = ddb.scan(
                TableName=KNOWLEDGE_TABLE,
                FilterExpression="contains(content, :search) OR contains(entity_name, :search)",
                ExpressionAttributeValues={":search": {"S": search}},
                Limit=limit
            )
            items = resp.get("Items", [])
        elif entity_type:
            # Scan for all of an entity type
            resp = ddb.scan(
                TableName=KNOWLEDGE_TABLE,
                FilterExpression="entity_type = :et",
                ExpressionAttributeValues={":et": {"S": entity_type}},
                Limit=limit
            )
            items = resp.get("Items", [])
        else:
            # Get recent notes across all entities
            resp = ddb.scan(
                TableName=KNOWLEDGE_TABLE,
                Limit=limit
            )
            items = resp.get("Items", [])

        # Convert DynamoDB items to plain dicts
        notes = []
        for item in items:
            note = {
                "pk": item.get("pk", {}).get("S", ""),
                "sk": item.get("sk", {}).get("S", ""),
                "entity_type": item.get("entity_type", {}).get("S", ""),
                "entity_id": item.get("entity_id", {}).get("S", ""),
                "entity_name": item.get("entity_name", {}).get("S", ""),
                "category": item.get("category", {}).get("S", ""),
                "content": item.get("content", {}).get("S", ""),
                "author": item.get("author", {}).get("S", ""),
                "created_at": item.get("created_at", {}).get("S", ""),
                "updated_at": item.get("updated_at", {}).get("S", ""),
                "tags": item.get("tags", {}).get("SS", []) if item.get("tags") else [],
                "applies_to_properties": item.get("applies_to_properties", {}).get("SS", ["*"]) if item.get("applies_to_properties") else ["*"],
                "confidence": item.get("confidence", {}).get("S", "medium"),
                "verified_by": item.get("verified_by", {}).get("SS", []) if item.get("verified_by") else [],
            }
            # Filter by category if specified
            if category and note["category"] != category:
                continue
            notes.append(note)

        # Sort by created_at descending
        notes.sort(key=lambda x: x.get("created_at", ""), reverse=True)

        return {"notes": notes[:limit], "count": len(notes)}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge list")}, status_code=500)


@app.post("/api/knowledge")
async def api_knowledge_add(
    request: Request,
    user: str = Depends(require_user)
):
    """Add a knowledge note."""
    try:
        body = await request.json()
        entity_type = body.get("entity_type", "").strip().lower()
        entity_id = body.get("entity_id", "").strip()
        entity_name = body.get("entity_name", "").strip()
        category = body.get("category", "").strip()
        content = body.get("content", "").strip()
        tags = body.get("tags", [])
        applies_to = body.get("applies_to_properties", ["*"])
        confidence = body.get("confidence", "medium")

        if not entity_type or not entity_id or not content:
            return JSONResponse({"error": "entity_type, entity_id, and content are required"}, status_code=400)

        if entity_type not in ("vendor", "property", "account"):
            return JSONResponse({"error": "entity_type must be vendor, property, or account"}, status_code=400)

        now = datetime.now(timezone.utc).isoformat()
        pk = f"{entity_type.upper()}#{entity_id}"
        sk = f"NOTE#{now}#{user}"

        item = {
            "pk": {"S": pk},
            "sk": {"S": sk},
            "entity_type": {"S": entity_type},
            "entity_id": {"S": entity_id},
            "entity_name": {"S": entity_name},
            "category": {"S": category},
            "content": {"S": content},
            "author": {"S": user},
            "created_at": {"S": now},
            "updated_at": {"S": now},
            "confidence": {"S": confidence},
        }

        # Add tags as string set if provided
        if tags and isinstance(tags, list) and len(tags) > 0:
            item["tags"] = {"SS": [str(t) for t in tags]}

        # Add applies_to_properties as string set
        if applies_to and isinstance(applies_to, list) and len(applies_to) > 0:
            item["applies_to_properties"] = {"SS": [str(p) for p in applies_to]}

        ddb.put_item(TableName=KNOWLEDGE_TABLE, Item=item)

        print(f"[KNOWLEDGE] Added note by {user} for {entity_type}/{entity_id}: {content[:50]}...")

        return {"ok": True, "pk": pk, "sk": sk}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge add")}, status_code=500)


@app.put("/api/knowledge/{entity_type}/{entity_id}")
async def api_knowledge_update(
    entity_type: str,
    entity_id: str,
    request: Request,
    user: str = Depends(require_user)
):
    """Update a knowledge note."""
    try:
        body = await request.json()
        sk = body.get("sk", "").strip()

        if not sk:
            return JSONResponse({"error": "sk (sort key) is required"}, status_code=400)

        pk = f"{entity_type.upper()}#{entity_id}"

        # Verify the note exists
        resp = ddb.get_item(TableName=KNOWLEDGE_TABLE, Key={"pk": {"S": pk}, "sk": {"S": sk}})
        if "Item" not in resp:
            return JSONResponse({"error": "Note not found"}, status_code=404)

        now = datetime.now(timezone.utc).isoformat()

        # Build update expression
        update_parts = ["updated_at = :updated_at"]
        expr_values = {":updated_at": {"S": now}}

        if "content" in body:
            update_parts.append("content = :content")
            expr_values[":content"] = {"S": body["content"]}

        if "category" in body:
            update_parts.append("category = :category")
            expr_values[":category"] = {"S": body["category"]}

        if "confidence" in body:
            update_parts.append("confidence = :confidence")
            expr_values[":confidence"] = {"S": body["confidence"]}

        if "entity_name" in body:
            update_parts.append("entity_name = :entity_name")
            expr_values[":entity_name"] = {"S": body["entity_name"]}

        ddb.update_item(
            TableName=KNOWLEDGE_TABLE,
            Key={"pk": {"S": pk}, "sk": {"S": sk}},
            UpdateExpression="SET " + ", ".join(update_parts),
            ExpressionAttributeValues=expr_values
        )

        print(f"[KNOWLEDGE] Updated note {pk}/{sk} by {user}")

        return {"ok": True}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge update")}, status_code=500)


@app.post("/api/knowledge/{entity_type}/{entity_id}/verify")
async def api_knowledge_verify(
    entity_type: str,
    entity_id: str,
    request: Request,
    user: str = Depends(require_user)
):
    """Verify/confirm a knowledge note from another AP."""
    try:
        body = await request.json()
        sk = body.get("sk", "").strip()

        if not sk:
            return JSONResponse({"error": "sk (sort key) is required"}, status_code=400)

        pk = f"{entity_type.upper()}#{entity_id}"

        # Get current note
        resp = ddb.get_item(TableName=KNOWLEDGE_TABLE, Key={"pk": {"S": pk}, "sk": {"S": sk}})
        if "Item" not in resp:
            return JSONResponse({"error": "Note not found"}, status_code=404)

        item = resp["Item"]
        verified_by = item.get("verified_by", {}).get("SS", []) if item.get("verified_by") else []

        # Don't let author verify their own note
        author = item.get("author", {}).get("S", "")
        if user == author:
            return JSONResponse({"error": "Cannot verify your own note"}, status_code=400)

        # Add user to verified_by if not already there
        if user not in verified_by:
            verified_by.append(user)

        now = datetime.now(timezone.utc).isoformat()

        ddb.update_item(
            TableName=KNOWLEDGE_TABLE,
            Key={"pk": {"S": pk}, "sk": {"S": sk}},
            UpdateExpression="SET verified_by = :vb, last_verified = :lv",
            ExpressionAttributeValues={
                ":vb": {"SS": verified_by},
                ":lv": {"S": now}
            }
        )

        print(f"[KNOWLEDGE] Note {pk}/{sk} verified by {user}")

        return {"ok": True, "verified_by": verified_by}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge verify")}, status_code=500)


@app.delete("/api/knowledge/{entity_type}/{entity_id}")
async def api_knowledge_delete(
    entity_type: str,
    entity_id: str,
    request: Request,
    user: str = Depends(require_user)
):
    """Delete a knowledge note (only by author or admin)."""
    try:
        body = await request.json()
        sk = body.get("sk", "").strip()

        if not sk:
            return JSONResponse({"error": "sk (sort key) is required"}, status_code=400)

        pk = f"{entity_type.upper()}#{entity_id}"

        # Verify the note exists and check ownership
        resp = ddb.get_item(TableName=KNOWLEDGE_TABLE, Key={"pk": {"S": pk}, "sk": {"S": sk}})
        if "Item" not in resp:
            return JSONResponse({"error": "Note not found"}, status_code=404)

        item = resp["Item"]
        author = item.get("author", {}).get("S", "")

        # Only author or admin can delete
        is_admin = user in ("cbeach@jrkholding.com", "admin")  # TODO: proper admin check
        if user != author and not is_admin:
            return JSONResponse({"error": "Only the author or admin can delete this note"}, status_code=403)

        ddb.delete_item(TableName=KNOWLEDGE_TABLE, Key={"pk": {"S": pk}, "sk": {"S": sk}})

        print(f"[KNOWLEDGE] Note {pk}/{sk} deleted by {user}")

        return {"ok": True}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge delete")}, status_code=500)


@app.get("/api/knowledge/for-invoice")
def api_knowledge_for_invoice(
    vendor_id: str = None,
    vendor_name: str = None,
    property_id: str = None,
    property_name: str = None,
    account_number: str = None,
    user: str = Depends(require_user)
):
    """Get all relevant knowledge notes for an invoice being reviewed."""
    try:
        all_notes = []

        # Get vendor knowledge
        if vendor_id:
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"VENDOR#{vendor_id}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                note = _parse_knowledge_item(item)
                note["match_type"] = "vendor"
                all_notes.append(note)

        # Also try vendor name as ID if different
        if vendor_name and vendor_name != vendor_id:
            # Normalize vendor name for lookup
            vendor_key = vendor_name.lower().replace(" ", "_").replace(".", "")
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"VENDOR#{vendor_key}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                note = _parse_knowledge_item(item)
                note["match_type"] = "vendor"
                all_notes.append(note)

        # Get property knowledge
        if property_id:
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"PROPERTY#{property_id}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                note = _parse_knowledge_item(item)
                note["match_type"] = "property"
                all_notes.append(note)

        # Get vendor+property combo knowledge (no account number)
        if property_id and vendor_id:
            combo_key = f"{property_id}|{vendor_id}"
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"ACCOUNT#{combo_key}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                note = _parse_knowledge_item(item)
                note["match_type"] = "vendor_property"
                all_notes.append(note)

        # Get account-specific knowledge
        if account_number and property_id and vendor_id:
            account_key = f"{property_id}|{vendor_id}|{account_number}"
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"ACCOUNT#{account_key}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                note = _parse_knowledge_item(item)
                note["match_type"] = "account"
                all_notes.append(note)

        # Sort by relevance (account > vendor_property > property > vendor) then by date
        def sort_key(n):
            priority = {"account": 0, "vendor_property": 1, "property": 2, "vendor": 3}.get(n.get("match_type"), 4)
            return (priority, n.get("created_at", ""))

        all_notes.sort(key=sort_key)

        return {"notes": all_notes, "count": len(all_notes)}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "knowledge for invoice")}, status_code=500)


def _parse_knowledge_item(item: dict) -> dict:
    """Convert DynamoDB item to plain dict."""
    return {
        "pk": item.get("pk", {}).get("S", ""),
        "sk": item.get("sk", {}).get("S", ""),
        "entity_type": item.get("entity_type", {}).get("S", ""),
        "entity_id": item.get("entity_id", {}).get("S", ""),
        "entity_name": item.get("entity_name", {}).get("S", ""),
        "category": item.get("category", {}).get("S", ""),
        "content": item.get("content", {}).get("S", ""),
        "author": item.get("author", {}).get("S", ""),
        "created_at": item.get("created_at", {}).get("S", ""),
        "updated_at": item.get("updated_at", {}).get("S", ""),
        "tags": item.get("tags", {}).get("SS", []) if item.get("tags") else [],
        "confidence": item.get("confidence", {}).get("S", "medium"),
        "verified_by": item.get("verified_by", {}).get("SS", []) if item.get("verified_by") else [],
    }


# ========== AI Review Endpoints (Phase 2: Shadow Mode) ==========

def _save_ai_suggestion(pdf_id: str, suggestion: dict) -> bool:
    """Save AI suggestion to DynamoDB."""
    try:
        now = suggestion.get("analyzed_at") or datetime.now(timezone.utc).isoformat()
        item = {
            "pk": {"S": pdf_id},
            "sk": {"S": f"SUGGESTION#{now}"},
            "analyzed_at": {"S": now},
            "analyzed_by": {"S": suggestion.get("analyzed_by", "ai")},
            "would_auto_pass": {"BOOL": suggestion.get("would_auto_pass", False)},
            "auto_pass_confidence": {"N": str(suggestion.get("auto_pass_confidence", 0))},
            "ai_reasoning": {"S": suggestion.get("ai_reasoning", "")},
        }

        # Add garbage lines as JSON string
        if suggestion.get("garbage_lines"):
            item["garbage_lines"] = {"S": json.dumps(suggestion["garbage_lines"])}

        # Add suggested deletions as string set
        if suggestion.get("suggested_deletions"):
            item["suggested_deletions"] = {"SS": [str(x) for x in suggestion["suggested_deletions"]]}

        # Add historical flags
        if suggestion.get("historical_flags"):
            item["historical_flags"] = {"SS": suggestion["historical_flags"]}

        # Add knowledge flags
        if suggestion.get("knowledge_flags"):
            item["knowledge_flags"] = {"SS": suggestion["knowledge_flags"]}

        ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
        return True
    except Exception as e:
        print(f"[AI Review] Failed to save suggestion: {e}")
        return False


def _get_ai_suggestion(pdf_id: str) -> dict | None:
    """Get the most recent AI suggestion for a pdf_id."""
    try:
        resp = ddb.query(
            TableName=AI_SUGGESTIONS_TABLE,
            KeyConditionExpression="pk = :pk AND begins_with(sk, :prefix)",
            ExpressionAttributeValues={
                ":pk": {"S": pdf_id},
                ":prefix": {"S": "SUGGESTION#"}
            },
            ScanIndexForward=False,  # Most recent first
            Limit=1
        )
        items = resp.get("Items", [])
        if not items:
            return None

        item = items[0]
        return {
            "pdf_id": pdf_id,
            "analyzed_at": item.get("analyzed_at", {}).get("S", ""),
            "analyzed_by": item.get("analyzed_by", {}).get("S", ""),
            "would_auto_pass": item.get("would_auto_pass", {}).get("BOOL", False),
            "auto_pass_confidence": float(item.get("auto_pass_confidence", {}).get("N", "0")),
            "ai_reasoning": item.get("ai_reasoning", {}).get("S", ""),
            "garbage_lines": json.loads(item.get("garbage_lines", {}).get("S", "[]")),
            "suggested_deletions": list(item.get("suggested_deletions", {}).get("SS", [])),
            "historical_flags": list(item.get("historical_flags", {}).get("SS", [])),
            "knowledge_flags": list(item.get("knowledge_flags", {}).get("SS", [])),
        }
    except Exception as e:
        print(f"[AI Review] Failed to get suggestion: {e}")
        return None


def _gemini_review_bill(lines: list[dict], history: list[dict], knowledge: list[dict], garbage: list[dict]) -> dict:
    """Use Gemini to semantically review a bill and provide additional insights.

    Args:
        lines: Line items from the bill
        history: Historical bills for comparison
        knowledge: Knowledge base notes
        garbage: Already-detected garbage lines

    Returns:
        Dict with AI analysis results
    """
    if not _init_gemini() or _gemini_model is None:
        return {"error": "Gemini not configured", "would_auto_pass": False, "confidence": 0}

    try:
        # Get bill metadata from first line
        first = lines[0] if lines else {}
        vendor_name = first.get("EnrichedVendorName") or first.get("Vendor Name") or "Unknown"
        property_name = first.get("EnrichedPropertyName") or "Unknown"
        account_number = first.get("Account Number") or first.get("Line Item Account Number") or ""
        bill_date = first.get("Bill Date") or ""

        # Calculate total
        total = 0.0
        for line in lines:
            charge_raw = line.get("Line Item Charge", 0)
            try:
                if isinstance(charge_raw, str):
                    total += float(charge_raw.replace("$", "").replace(",", "").strip() or 0)
                else:
                    total += float(charge_raw or 0)
            except (ValueError, TypeError):
                pass

        # Format line items for prompt
        line_summary = []
        for i, line in enumerate(lines):
            desc = line.get("Line Item Description") or ""
            charge = line.get("Line Item Charge") or 0
            gl = line.get("EnrichedGLAccountNumber") or ""
            line_summary.append(f"  {i+1}. [{gl}] {desc}: ${charge}")

        # Format history
        history_summary = "No historical data available."
        if history:
            import statistics
            totals = [h.get("total_amount", 0) for h in history]
            avg = statistics.mean(totals) if totals else 0
            history_summary = f"Last {len(history)} bills: avg ${avg:,.2f}, range ${min(totals):,.2f}-${max(totals):,.2f}"

        # Format knowledge
        knowledge_summary = "No knowledge notes."
        if knowledge:
            knowledge_summary = "\n".join([f"  - [{n.get('category', 'note')}] {n.get('content', '')[:100]}" for n in knowledge[:5]])

        # Format garbage detections
        garbage_summary = "None detected."
        if garbage:
            garbage_summary = "\n".join([f"  - Line {g.get('line_index', 0)+1}: {g.get('description', '')} (reason: {g.get('reason', '')})" for g in garbage[:5]])

        prompt = f"""You are an AP (Accounts Payable) review assistant analyzing a utility bill for quality issues.

=== BILL DATA ===
Vendor: {vendor_name}
Property: {property_name}
Account: {account_number}
Bill Date: {bill_date}
Total: ${total:,.2f}
Line Count: {len(lines)}

=== LINE ITEMS ===
{chr(10).join(line_summary[:20])}
{f"... and {len(line_summary) - 20} more lines" if len(line_summary) > 20 else ""}

=== HISTORICAL PATTERN ===
{history_summary}

=== AP KNOWLEDGE NOTES ===
{knowledge_summary}

=== GARBAGE LINES DETECTED (by pattern matching) ===
{garbage_summary}

=== YOUR TASK ===
Analyze this bill and respond with ONLY a JSON object (no other text):

{{
  "would_auto_pass": true/false,
  "confidence": 0-100,
  "additional_issues": ["list of any issues NOT already caught by pattern matching"],
  "garbage_validation": "correct" or "needs_review" (are the detected garbage lines correct?),
  "historical_concern": true/false (is amount or pattern unusual compared to history?),
  "reasoning": "Brief 1-2 sentence explanation"
}}

Rules:
- would_auto_pass = true ONLY if: no garbage lines, no issues, amount is normal, and you're confident the bill is correct
- confidence should reflect how certain you are (higher = more certain)
- Look for: duplicate charges, incorrect GL codes, unusual amounts, missing information
- If no history exists, be more conservative (lower confidence)
"""

        # Call Gemini
        response = _gemini_model.generate_content(prompt)
        text = response.text.strip()

        # Parse JSON from response
        if '```' in text:
            text = text.split('```')[1]
            if text.startswith('json'):
                text = text[4:]
            text = text.strip()

        result = json.loads(text)
        return {
            "would_auto_pass": result.get("would_auto_pass", False),
            "confidence": result.get("confidence", 50),
            "additional_issues": result.get("additional_issues", []),
            "garbage_validation": result.get("garbage_validation", "needs_review"),
            "historical_concern": result.get("historical_concern", False),
            "reasoning": result.get("reasoning", ""),
        }

    except Exception as e:
        print(f"[AI Review] Gemini error: {e}")
        return {
            "error": str(e),
            "would_auto_pass": False,
            "confidence": 0,
            "reasoning": f"Gemini analysis failed: {e}"
        }


@app.post("/api/ai-review/analyze")
def api_ai_review_analyze(pdf_id: str, date: str = None, use_gemini: bool = True, user: str = Depends(require_user)):
    """Run AI review on a bill and store suggestions.

    Phase 2.0: Deterministic review only (pattern matching, history).
    Phase 2.1: Add Gemini semantic review.

    Args:
        pdf_id: The pdf_id hash of the bill
        date: Optional date (YYYY-MM-DD) to narrow search
    """
    try:
        # 1. Load bill lines from Stage 4
        lines = []
        if date:
            # Use existing load_day pattern
            try:
                y, m, d = date.split("-")
                all_rows = load_day(y, m, d)
                lines = [r for r in all_rows if r.get("__s3_key__") and pdf_id_from_key(r.get("__s3_key__")) == pdf_id]
            except Exception as e:
                print(f"[AI Review] Date-based load failed: {e}")

        # Fallback: scan recent 7 days
        if not lines:
            import datetime as _dt
            today = _dt.date.today()
            for offset in range(7):
                check_date = today - _dt.timedelta(days=offset)
                try:
                    all_rows = load_day(str(check_date.year), f"{check_date.month:02d}", f"{check_date.day:02d}")
                    lines = [r for r in all_rows if r.get("__s3_key__") and pdf_id_from_key(r.get("__s3_key__")) == pdf_id]
                    if lines:
                        break
                except Exception:
                    pass

        if not lines:
            return JSONResponse({"error": "Bill not found"}, status_code=404)

        # Get bill metadata from first line
        first = lines[0]
        vendor_id = str(first.get("EnrichedVendorID") or "").strip()
        vendor_name = first.get("EnrichedVendorName") or first.get("Vendor Name") or ""
        property_id = str(first.get("EnrichedPropertyID") or "").strip()
        property_name = first.get("EnrichedPropertyName") or ""
        account_number = first.get("Account Number") or first.get("Line Item Account Number") or ""

        # 2. Detect garbage lines (deterministic pattern matching + learned patterns)
        garbage = _detect_garbage_lines(lines, vendor_id=vendor_id, property_id=property_id)

        # 3. Get historical context
        history = _get_account_history(vendor_id, property_id, account_number, limit=10)

        # 4. Get knowledge base notes
        knowledge_notes = []
        try:
            # Reuse existing endpoint logic
            resp = ddb.query(
                TableName=KNOWLEDGE_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": f"VENDOR#{vendor_id}"}},
                Limit=20
            )
            for item in resp.get("Items", []):
                knowledge_notes.append(_parse_knowledge_item(item))

            if property_id and vendor_id:
                combo_key = f"{property_id}|{vendor_id}"
                resp = ddb.query(
                    TableName=KNOWLEDGE_TABLE,
                    KeyConditionExpression="pk = :pk",
                    ExpressionAttributeValues={":pk": {"S": f"ACCOUNT#{combo_key}"}},
                    Limit=20
                )
                for item in resp.get("Items", []):
                    knowledge_notes.append(_parse_knowledge_item(item))
        except Exception as e:
            print(f"[AI Review] Knowledge fetch error: {e}")

        # 5. Historical analysis (deterministic)
        historical_flags = []
        if history:
            # Calculate stats
            totals = [h.get("total_amount", 0) for h in history]
            if totals:
                import statistics
                avg = statistics.mean(totals)
                std = statistics.stdev(totals) if len(totals) > 1 else avg * 0.3

                # Calculate current bill total
                current_total = 0.0
                for line in lines:
                    charge_raw = line.get("Line Item Charge", 0)
                    try:
                        if isinstance(charge_raw, str):
                            current_total += float(charge_raw.replace("$", "").replace(",", "").strip() or 0)
                        else:
                            current_total += float(charge_raw or 0)
                    except (ValueError, TypeError):
                        pass

                # Flag if > 2 std devs from mean
                if std > 0 and abs(current_total - avg) > 2 * std:
                    direction = "higher" if current_total > avg else "lower"
                    pct = abs(current_total - avg) / avg * 100 if avg else 0
                    historical_flags.append(
                        f"Amount ${current_total:,.2f} is {pct:.0f}% {direction} than average ${avg:,.2f}"
                    )

                # Flag unusual line count
                line_counts = [h.get("line_count", 0) for h in history]
                avg_lines = statistics.mean(line_counts)
                if len(lines) > avg_lines * 2:
                    historical_flags.append(
                        f"Line count ({len(lines)}) is unusually high vs average ({avg_lines:.0f})"
                    )

        # 6. Knowledge-based flags
        knowledge_flags = []
        for note in knowledge_notes:
            content = note.get("content", "").lower()
            category = note.get("category", "")

            # Simple flag for warning/issue categories
            if category in ("common_issue", "warning", "gotcha"):
                knowledge_flags.append(f"[{category}] {note.get('content', '')[:100]}")

        # 7. Determine auto-pass eligibility
        # A bill can auto-pass if:
        # - No garbage lines detected
        # - No historical flags
        # - No knowledge warnings
        # - Has historical data (not a new account)
        would_auto_pass = (
            len(garbage) == 0 and
            len(historical_flags) == 0 and
            len(knowledge_flags) == 0 and
            len(history) >= 3  # Need history to be confident
        )

        # Calculate confidence
        confidence = 50  # Base confidence
        if len(history) >= 5:
            confidence += 20
        if len(history) >= 10:
            confidence += 10
        if len(garbage) == 0:
            confidence += 10
        if len(historical_flags) == 0:
            confidence += 5
        if len(knowledge_flags) == 0:
            confidence += 5
        confidence = min(confidence, 95)  # Cap at 95 until Gemini adds semantic review

        # Build reasoning
        reasoning_parts = []
        if garbage:
            reasoning_parts.append(f"Detected {len(garbage)} potential garbage lines")
        if historical_flags:
            reasoning_parts.append(f"Historical anomalies: {'; '.join(historical_flags)}")
        if knowledge_flags:
            reasoning_parts.append(f"Knowledge notes flagged")
        if not history:
            reasoning_parts.append("No historical data for this account")
        elif len(history) < 3:
            reasoning_parts.append(f"Limited history ({len(history)} bills)")

        if not reasoning_parts:
            reasoning_parts.append("Bill appears normal based on deterministic checks")

        # 7b. Gemini semantic review (if enabled)
        gemini_result = None
        if use_gemini:
            try:
                gemini_result = _gemini_review_bill(lines, history, knowledge_notes, garbage)
                if gemini_result and not gemini_result.get("error"):
                    # Merge Gemini insights
                    if gemini_result.get("additional_issues"):
                        for issue in gemini_result["additional_issues"]:
                            if issue and issue not in historical_flags:
                                historical_flags.append(f"[Gemini] {issue}")

                    # If Gemini disagrees with auto-pass, be conservative
                    if would_auto_pass and not gemini_result.get("would_auto_pass", True):
                        would_auto_pass = False
                        reasoning_parts.append("Gemini flagged potential issues")

                    # If Gemini has historical concern
                    if gemini_result.get("historical_concern"):
                        if "Historical concern flagged by Gemini" not in historical_flags:
                            historical_flags.append("Historical concern flagged by Gemini")

                    # Adjust confidence based on Gemini
                    gemini_conf = gemini_result.get("confidence", 50)
                    # Weight: 60% deterministic, 40% Gemini
                    confidence = int(confidence * 0.6 + gemini_conf * 0.4)
                    confidence = max(0, min(100, confidence))

                    # Add Gemini reasoning
                    if gemini_result.get("reasoning"):
                        reasoning_parts.append(f"Gemini: {gemini_result['reasoning']}")

                    print(f"[AI Review] Gemini analysis complete: would_auto_pass={gemini_result.get('would_auto_pass')}, confidence={gemini_conf}")
            except Exception as gem_err:
                print(f"[AI Review] Gemini failed (non-blocking): {gem_err}")

        # 8. Build suggestion record
        analyzed_by = "ai_deterministic_gemini" if (gemini_result and not gemini_result.get("error")) else "ai_deterministic"
        suggestion = {
            "pdf_id": pdf_id,
            "analyzed_at": datetime.now(timezone.utc).isoformat(),
            "analyzed_by": analyzed_by,
            "garbage_lines": garbage,
            "historical_flags": historical_flags,
            "knowledge_flags": knowledge_flags,
            "would_auto_pass": would_auto_pass,
            "auto_pass_confidence": confidence,
            "suggested_deletions": [g.get("line_id") for g in garbage if g.get("confidence", 0) >= 0.8 and g.get("line_id")],
            "ai_reasoning": "; ".join(reasoning_parts),
        }

        # 9. Save to DynamoDB
        _save_ai_suggestion(pdf_id, suggestion)

        # Add context for response
        suggestion["history_count"] = len(history)
        suggestion["knowledge_count"] = len(knowledge_notes)
        suggestion["line_count"] = len(lines)

        return suggestion

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "ai review")}, status_code=500)


@app.get("/api/ai-review/suggestion/{pdf_id}")
def api_ai_review_suggestion(pdf_id: str, user: str = Depends(require_user)):
    """Get the AI suggestion for a bill (if one exists)."""
    try:
        suggestion = _get_ai_suggestion(pdf_id)
        if not suggestion:
            return {"found": False}

        suggestion["found"] = True
        return suggestion
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "get ai suggestion")}, status_code=500)


def _track_ai_accuracy(pdf_id: str, deleted_line_ids: set, human_made_changes: bool, user: str) -> bool:
    """Compare AI suggestions to human actions and track accuracy.

    Args:
        pdf_id: The pdf_id of the submitted bill
        deleted_line_ids: Set of line IDs that were deleted by the human
        human_made_changes: Whether the human made any changes (edits, deletions, additions)
        user: The user who submitted
    """
    try:
        # Get AI suggestion (if one exists)
        suggestion = _get_ai_suggestion(pdf_id)
        if not suggestion:
            return False  # No AI suggestion to compare against

        # Get AI suggested deletions
        ai_deletions = set(suggestion.get("suggested_deletions", []))
        human_deletions = deleted_line_ids

        # Calculate deletion accuracy
        true_positives = len(ai_deletions & human_deletions)  # AI said delete, human deleted
        false_positives = len(ai_deletions - human_deletions)  # AI said delete, human kept
        false_negatives = len(human_deletions - ai_deletions)  # AI said keep, human deleted

        # Compare auto-pass prediction
        ai_would_pass = suggestion.get("would_auto_pass", False)
        # Auto-pass is correct if: AI said pass AND human made no changes,
        # OR AI said review AND human made changes
        auto_pass_correct = (ai_would_pass and not human_made_changes) or (not ai_would_pass and human_made_changes)

        # Build accuracy record
        now = datetime.now(timezone.utc).isoformat()
        accuracy_item = {
            "pk": {"S": pdf_id},
            "sk": {"S": f"ACCURACY#{now}"},
            "timestamp": {"S": now},
            "user": {"S": user},
            "deletion_tp": {"N": str(true_positives)},
            "deletion_fp": {"N": str(false_positives)},
            "deletion_fn": {"N": str(false_negatives)},
            "auto_pass_predicted": {"BOOL": ai_would_pass},
            "auto_pass_correct": {"BOOL": auto_pass_correct},
            "human_changed": {"BOOL": human_made_changes},
            "ai_confidence": {"N": str(suggestion.get("auto_pass_confidence", 0))},
        }

        # Add deletion details for debugging
        if ai_deletions:
            accuracy_item["ai_suggested_deletions"] = {"SS": list(ai_deletions)}
        if human_deletions:
            accuracy_item["human_deletions"] = {"SS": list(human_deletions)}

        # Save to same table as suggestions
        ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=accuracy_item)
        print(f"[AI Accuracy] Tracked: pdf_id={pdf_id}, TP={true_positives}, FP={false_positives}, FN={false_negatives}, auto_pass_correct={auto_pass_correct}")
        return True

    except Exception as e:
        print(f"[AI Accuracy] Failed to track: {e}")
        return False


def _capture_human_actions(
    pdf_id: str,
    originals: list[dict],
    merged: list[dict],
    deleted_set: set,
    header_fields: dict,
    extra_lines: list[dict],
    deltas: list[dict],
    user: str,
    vendor_id: str = "",
    property_id: str = "",
    account_number: str = "",
) -> dict:
    """Capture detailed human actions during submit for learning.

    Compares original data to final data to determine what the human changed.
    This data is used to:
    1. Learn patterns from human corrections
    2. Detect when AI suggestions were wrong
    3. Build training data for improved suggestions

    Returns dict of captured actions.
    """
    import re
    try:
        now = datetime.now(timezone.utc).isoformat()

        # Build lookup of original line data by __id__
        orig_by_id = {}
        for orig in originals:
            orig_id = str(orig.get("__id__", ""))
            if orig_id:
                orig_by_id[orig_id] = orig

        # Build lookup of merged (final) line data
        merged_by_id = {}
        for mrg in merged:
            mrg_id = str(mrg.get("__id__", ""))
            if mrg_id:
                merged_by_id[mrg_id] = mrg

        # Capture deletions
        lines_deleted = list(deleted_set)

        # Capture lines that were deleted - with their descriptions for pattern learning
        deleted_descriptions = []
        for del_id in deleted_set:
            orig = orig_by_id.get(del_id, {})
            desc = orig.get("Line Item Description", "")
            charge = orig.get("Line Item Charge", 0)
            if desc:
                deleted_descriptions.append({
                    "line_id": del_id,
                    "description": desc,
                    "charge": str(charge),
                })

        # Capture GL code changes
        gl_changes = []
        for orig in originals:
            orig_id = str(orig.get("__id__", ""))
            if orig_id in deleted_set:
                continue  # Skip deleted lines
            final = merged_by_id.get(orig_id, {})
            orig_gl = str(orig.get("GL Code") or orig.get("EnrichedGLCode") or "").strip()
            final_gl = str(final.get("GL Code") or final.get("EnrichedGLCode") or "").strip()
            if orig_gl != final_gl and final_gl:
                gl_changes.append({
                    "line_id": orig_id,
                    "description": orig.get("Line Item Description", ""),
                    "from_gl": orig_gl,
                    "to_gl": final_gl,
                })

        # Capture amount changes
        amount_changes = []
        for orig in originals:
            orig_id = str(orig.get("__id__", ""))
            if orig_id in deleted_set:
                continue
            final = merged_by_id.get(orig_id, {})
            orig_amt_raw = orig.get("Line Item Charge", 0)
            final_amt_raw = final.get("Line Item Charge", 0)
            try:
                orig_amt = float(str(orig_amt_raw).replace("$", "").replace(",", "").strip() or 0)
                final_amt = float(str(final_amt_raw).replace("$", "").replace(",", "").strip() or 0)
            except (ValueError, TypeError):
                orig_amt, final_amt = 0.0, 0.0
            if abs(orig_amt - final_amt) > 0.01:  # Non-trivial change
                amount_changes.append({
                    "line_id": orig_id,
                    "from": orig_amt,
                    "to": final_amt,
                })

        # Capture header changes
        header_changes = {}
        first_orig = originals[0] if originals else {}
        header_edit_fields_to_check = [
            "Bill Date", "Due Date", "Invoice Number", "Account Number",
            "Bill Period Start", "Bill Period End", "Vendor Name",
            "EnrichedVendorID", "EnrichedPropertyID", "EnrichedVendorName", "EnrichedPropertyName"
        ]
        for field in header_edit_fields_to_check:
            orig_val = str(first_orig.get(field, "") or "").strip()
            new_val = str(header_fields.get(field, "") or "").strip()
            if new_val and orig_val != new_val:
                header_changes[field] = {"from": orig_val, "to": new_val}

        # Build actions record
        actions = {
            "pdf_id": pdf_id,
            "user": user,
            "timestamp": now,
            "vendor_id": vendor_id,
            "property_id": property_id,
            "account_number": account_number,
            "original_line_count": len(originals),
            "final_line_count": len(merged),
            "lines_deleted_count": len(lines_deleted),
            "lines_added_count": len(extra_lines),
            "gl_changes_count": len(gl_changes),
            "amount_changes_count": len(amount_changes),
            "header_changes_count": len(header_changes),
        }

        # Store detailed data as JSON strings (DynamoDB doesn't support nested lists well)
        if deleted_descriptions:
            actions["deleted_descriptions"] = deleted_descriptions
        if gl_changes:
            actions["gl_changes"] = gl_changes
        if amount_changes:
            actions["amount_changes"] = amount_changes
        if header_changes:
            actions["header_changes"] = header_changes

        # Save to DynamoDB
        item = {
            "pk": {"S": f"ACTION#{pdf_id}"},
            "sk": {"S": f"SUBMIT#{now}"},
            "timestamp": {"S": now},
            "user": {"S": user},
            "vendor_id": {"S": vendor_id},
            "property_id": {"S": property_id},
            "account_number": {"S": account_number},
            "original_line_count": {"N": str(len(originals))},
            "final_line_count": {"N": str(len(merged))},
            "lines_deleted_count": {"N": str(len(lines_deleted))},
            "lines_added_count": {"N": str(len(extra_lines))},
            "gl_changes_count": {"N": str(len(gl_changes))},
            "amount_changes_count": {"N": str(len(amount_changes))},
            "header_changes_count": {"N": str(len(header_changes))},
        }

        # Add detailed data as JSON strings
        if deleted_descriptions:
            item["deleted_descriptions"] = {"S": json.dumps(deleted_descriptions)}
        if gl_changes:
            item["gl_changes"] = {"S": json.dumps(gl_changes)}
        if amount_changes:
            item["amount_changes"] = {"S": json.dumps(amount_changes)}
        if header_changes:
            item["header_changes"] = {"S": json.dumps(header_changes)}

        ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
        print(f"[AI Learning] Captured actions: pdf_id={pdf_id}, deleted={len(lines_deleted)}, gl_changes={len(gl_changes)}, header_changes={len(header_changes)}")
        return actions

    except Exception as e:
        print(f"[AI Learning] Failed to capture actions: {e}")
        import traceback
        traceback.print_exc()
        return {}


def _store_correction_patterns(
    pdf_id: str,
    actions: dict,
    ai_suggestion: dict | None,
    vendor_id: str,
    property_id: str,
    utility_type: str = "",
    quarantine: bool = False,
    quarantine_reason: str = "",
) -> int:
    """Store correction patterns when AI was wrong.

    Analyzes human actions vs AI suggestions and stores patterns for learning:
    - Deletion patterns: lines human deleted that AI didn't suggest
    - Keep patterns: lines AI suggested deleting but human kept
    - GL patterns: GL codes human changed from what enricher suggested

    Args:
        pdf_id: The pdf_id
        actions: Dict from _capture_human_actions()
        ai_suggestion: Dict from _get_ai_suggestion() or None
        vendor_id: Vendor ID
        property_id: Property ID
        utility_type: Utility type (Electric, Gas, etc.)
        quarantine: If True, mark patterns as quarantined (don't use for learning yet)
        quarantine_reason: Why it's quarantined

    Returns:
        Number of patterns stored
    """
    try:
        now = datetime.now(timezone.utc).isoformat()
        patterns_stored = 0
        status = "quarantined" if quarantine else "active"

        # Get AI suggested deletions
        ai_deletions = set()
        if ai_suggestion:
            ai_deletions = set(ai_suggestion.get("suggested_deletions", []))

        # Get human deletions
        human_deletions = set()
        deleted_descs = actions.get("deleted_descriptions", [])
        for dd in deleted_descs:
            human_deletions.add(dd.get("line_id", ""))

        # Pattern Type 1: FALSE NEGATIVES - Human deleted, AI didn't suggest
        # These are patterns we should learn to delete
        missed_deletions = human_deletions - ai_deletions
        for dd in deleted_descs:
            if dd.get("line_id") in missed_deletions:
                desc = dd.get("description", "")
                if not desc:
                    continue
                # Extract pattern from description
                pattern_key = _normalize_description_pattern(desc)
                if not pattern_key:
                    continue

                item = {
                    "pk": {"S": f"PATTERN#DELETE#{vendor_id}"},
                    "sk": {"S": f"LEARN#{now}#{pdf_id[:8]}"},
                    "pattern_type": {"S": "DELETE"},
                    "vendor_id": {"S": vendor_id},
                    "property_id": {"S": property_id},
                    "description_pattern": {"S": pattern_key},
                    "original_description": {"S": desc},
                    "source_pdf_id": {"S": pdf_id},
                    "status": {"S": status},
                    "created_at": {"S": now},
                    "created_by": {"S": actions.get("user", "")},
                }
                if quarantine_reason:
                    item["quarantine_reason"] = {"S": quarantine_reason}
                ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
                patterns_stored += 1
                print(f"[AI Learning] Stored DELETE pattern: '{pattern_key}' for vendor {vendor_id}")

        # Pattern Type 2: FALSE POSITIVES - AI suggested delete, human kept
        # These are patterns we should learn to KEEP (not delete)
        if ai_suggestion:
            garbage_lines = ai_suggestion.get("garbage_lines", [])
            for gl in garbage_lines:
                line_id = gl.get("line_id")
                if line_id and line_id not in human_deletions and line_id in ai_deletions:
                    desc = gl.get("description", "")
                    if not desc:
                        continue
                    pattern_key = _normalize_description_pattern(desc)
                    if not pattern_key:
                        continue

                    item = {
                        "pk": {"S": f"PATTERN#KEEP#{vendor_id}"},
                        "sk": {"S": f"LEARN#{now}#{pdf_id[:8]}"},
                        "pattern_type": {"S": "KEEP"},
                        "vendor_id": {"S": vendor_id},
                        "property_id": {"S": property_id},
                        "description_pattern": {"S": pattern_key},
                        "original_description": {"S": desc},
                        "source_pdf_id": {"S": pdf_id},
                        "ai_suggested_reason": {"S": gl.get("reason", "")},
                        "status": {"S": status},
                        "created_at": {"S": now},
                        "created_by": {"S": actions.get("user", "")},
                    }
                    if quarantine_reason:
                        item["quarantine_reason"] = {"S": quarantine_reason}
                    ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
                    patterns_stored += 1
                    print(f"[AI Learning] Stored KEEP pattern: '{pattern_key}' for vendor {vendor_id}")

        # Pattern Type 3: GL CODE CORRECTIONS
        # When user changes GL code from what enricher suggested
        gl_changes = actions.get("gl_changes", [])
        for glc in gl_changes:
            desc = glc.get("description", "")
            from_gl = glc.get("from_gl", "")
            to_gl = glc.get("to_gl", "")
            if not desc or not to_gl:
                continue
            pattern_key = _normalize_description_pattern(desc)
            if not pattern_key:
                continue

            item = {
                "pk": {"S": f"PATTERN#GL#{vendor_id}#{property_id}"},
                "sk": {"S": f"LEARN#{utility_type or 'unknown'}#{pattern_key[:50]}"},
                "pattern_type": {"S": "GL"},
                "vendor_id": {"S": vendor_id},
                "property_id": {"S": property_id},
                "utility_type": {"S": utility_type},
                "description_pattern": {"S": pattern_key},
                "original_description": {"S": desc},
                "from_gl": {"S": from_gl},
                "to_gl": {"S": to_gl},
                "source_pdf_id": {"S": pdf_id},
                "status": {"S": status},
                "created_at": {"S": now},
                "created_by": {"S": actions.get("user", "")},
            }
            if quarantine_reason:
                item["quarantine_reason"] = {"S": quarantine_reason}
            ddb.put_item(TableName=AI_SUGGESTIONS_TABLE, Item=item)
            patterns_stored += 1
            print(f"[AI Learning] Stored GL pattern: '{pattern_key}' -> {to_gl} for vendor {vendor_id}")

        if patterns_stored > 0:
            print(f"[AI Learning] Stored {patterns_stored} correction patterns from pdf_id={pdf_id}")
        return patterns_stored

    except Exception as e:
        print(f"[AI Learning] Failed to store patterns: {e}")
        import traceback
        traceback.print_exc()
        return 0


def _normalize_description_pattern(description: str) -> str:
    """Normalize a line description into a pattern for matching.

    Removes numbers, special chars, extra whitespace to create a matchable pattern.
    Example: "Water Usage 1234 gallons" -> "water usage gallons"
    """
    import re
    if not description:
        return ""
    desc = description.lower().strip()
    # Remove numbers (except meaningful ones like "tier 1", "level 2")
    desc = re.sub(r'\b\d+\.?\d*\s*(gal|kwh|ccf|therm|cuft|units?|days?)?\b', '', desc, flags=re.IGNORECASE)
    # Remove special chars except hyphen
    desc = re.sub(r'[^a-z0-9\s\-]', ' ', desc)
    # Collapse whitespace
    desc = re.sub(r'\s+', ' ', desc).strip()
    # Remove very short patterns (less meaningful)
    if len(desc) < 3:
        return ""
    return desc


def _get_learned_patterns(pattern_type: str, vendor_id: str, property_id: str = "", limit: int = 100) -> list[dict]:
    """Get learned patterns for a vendor (and optionally property).

    Args:
        pattern_type: "DELETE", "KEEP", or "GL"
        vendor_id: Vendor ID to filter by
        property_id: Property ID to filter by (optional, for GL patterns)
        limit: Max patterns to return

    Returns:
        List of pattern dicts
    """
    try:
        patterns = []

        # Query patterns for this vendor
        if pattern_type in ("DELETE", "KEEP"):
            pk = f"PATTERN#{pattern_type}#{vendor_id}"
        else:  # GL
            pk = f"PATTERN#GL#{vendor_id}#{property_id}" if property_id else f"PATTERN#GL#{vendor_id}#"

        resp = ddb.query(
            TableName=AI_SUGGESTIONS_TABLE,
            KeyConditionExpression="pk = :pk",
            FilterExpression="#status = :active",
            ExpressionAttributeNames={"#status": "status"},
            ExpressionAttributeValues={
                ":pk": {"S": pk},
                ":active": {"S": "active"}
            },
            Limit=limit
        )

        for item in resp.get("Items", []):
            patterns.append({
                "pattern_type": item.get("pattern_type", {}).get("S", ""),
                "vendor_id": item.get("vendor_id", {}).get("S", ""),
                "property_id": item.get("property_id", {}).get("S", ""),
                "description_pattern": item.get("description_pattern", {}).get("S", ""),
                "original_description": item.get("original_description", {}).get("S", ""),
                "to_gl": item.get("to_gl", {}).get("S", ""),
                "utility_type": item.get("utility_type", {}).get("S", ""),
                "status": item.get("status", {}).get("S", ""),
                "created_at": item.get("created_at", {}).get("S", ""),
            })

        return patterns

    except Exception as e:
        print(f"[AI Learning] Failed to get patterns: {e}")
        return []


def _matches_learned_pattern(line_description: str, patterns: list[dict]) -> dict | None:
    """Check if a line description matches any learned pattern.

    Args:
        line_description: The line item description to check
        patterns: List of pattern dicts from _get_learned_patterns()

    Returns:
        Matching pattern dict or None
    """
    if not line_description or not patterns:
        return None

    normalized = _normalize_description_pattern(line_description)
    if not normalized:
        return None

    for pattern in patterns:
        pattern_str = pattern.get("description_pattern", "")
        if not pattern_str:
            continue
        # Check if pattern matches (simple substring or exact match)
        if pattern_str in normalized or normalized in pattern_str:
            return pattern
        # Also try word overlap (if >70% words match)
        pattern_words = set(pattern_str.split())
        desc_words = set(normalized.split())
        if pattern_words and desc_words:
            overlap = len(pattern_words & desc_words) / max(len(pattern_words), len(desc_words))
            if overlap >= 0.7:
                return pattern

    return None


def _detect_human_mistakes(
    lines: list[dict],
    deleted_ids: set,
    header_fields: dict,
    vendor_id: str,
    property_id: str,
    account_number: str,
) -> list[dict]:
    """Detect potential human mistakes before submit.

    Checks for:
    1. Amount outliers vs historical average
    2. Unusual GL codes for this vendor/utility
    3. Mass deletions (>70% of lines)
    4. Date in the future
    5. Duplicate invoice number

    Returns list of warnings with severity.
    """
    import statistics
    warnings = []

    # Calculate total from non-deleted lines
    total_amount = 0.0
    utility_type = ""
    for line in lines:
        if str(line.get("__id__", "")) in deleted_ids:
            continue
        charge_raw = line.get("Line Item Charge", 0)
        try:
            if isinstance(charge_raw, str):
                total_amount += float(charge_raw.replace("$", "").replace(",", "").strip() or 0)
            else:
                total_amount += float(charge_raw or 0)
        except (ValueError, TypeError):
            pass
        if not utility_type:
            utility_type = line.get("Utility Type", "")

    original_count = len(lines)
    deleted_count = len(deleted_ids)
    final_count = original_count - deleted_count

    # 1. Amount outliers vs history
    if vendor_id and property_id and account_number:
        try:
            history = _get_account_history(vendor_id, property_id, account_number, limit=10)
            if len(history) >= 3:
                historical_amounts = [h.get("total_amount", 0) for h in history]
                avg_amount = statistics.mean(historical_amounts)
                if avg_amount > 0:
                    # Check for spike (3x normal) or drop (less than 20% of normal)
                    if total_amount > avg_amount * 3:
                        warnings.append({
                            "type": "amount_spike",
                            "severity": "high",
                            "message": f"Total ${total_amount:,.2f} is more than 3x the historical average ${avg_amount:,.2f}. Are you sure?",
                            "details": {"current": total_amount, "average": avg_amount, "history_count": len(history)}
                        })
                    elif total_amount < avg_amount * 0.2 and total_amount > 0:
                        warnings.append({
                            "type": "amount_drop",
                            "severity": "medium",
                            "message": f"Total ${total_amount:,.2f} is less than 20% of the historical average ${avg_amount:,.2f}. Did you delete too many lines?",
                            "details": {"current": total_amount, "average": avg_amount, "history_count": len(history)}
                        })
        except Exception as e:
            print(f"[Validation] Warning: History check failed: {e}")

    # 2. Mass deletions
    if original_count > 1 and deleted_count > original_count * 0.7:
        warnings.append({
            "type": "mass_delete",
            "severity": "medium",
            "message": f"You're deleting {deleted_count} of {original_count} lines (over 70%). Are you sure?",
            "details": {"deleted": deleted_count, "original": original_count, "remaining": final_count}
        })

    # 3. Date in future
    bill_date_str = header_fields.get("Bill Date", "") or (lines[0].get("Bill Date", "") if lines else "")
    if bill_date_str:
        try:
            # Parse various date formats
            import re
            today = datetime.now(timezone.utc).date()
            bill_date = None
            if "/" in bill_date_str:
                parts = bill_date_str.split("/")
                if len(parts) == 3:
                    m, d, y = int(parts[0]), int(parts[1]), int(parts[2])
                    if y < 100:
                        y += 2000
                    bill_date = datetime(y, m, d).date()
            elif "-" in bill_date_str:
                parts = bill_date_str.split("-")
                if len(parts) == 3:
                    y, m, d = int(parts[0]), int(parts[1]), int(parts[2])
                    bill_date = datetime(y, m, d).date()

            if bill_date and bill_date > today:
                warnings.append({
                    "type": "future_date",
                    "severity": "high",
                    "message": f"Bill date {bill_date_str} is in the future. Please verify.",
                    "details": {"bill_date": bill_date_str, "today": today.isoformat()}
                })
        except Exception:
            pass

    # 4. Zero or negative total
    if total_amount <= 0 and final_count > 0:
        warnings.append({
            "type": "zero_total",
            "severity": "high",
            "message": f"Total amount is ${total_amount:,.2f}. Did you delete all the charge lines?",
            "details": {"total": total_amount, "final_line_count": final_count}
        })

    # 5. Check for duplicate invoice number
    invoice_number = header_fields.get("Invoice Number", "") or (lines[0].get("Invoice Number", "") if lines else "")
    if invoice_number and vendor_id:
        try:
            # Query for existing invoices with this number for this vendor
            # Check in Stage 6, 7, 8 via a quick scan
            duplicate_found = False
            for stage_prefix in [PRE_ENTRATA_PREFIX, POST_ENTRATA_PREFIX, UBI_ASSIGNED_PREFIX]:
                # Just check recent months
                now = datetime.now(timezone.utc)
                for months_back in range(3):
                    check_date = now - timedelta(days=months_back * 30)
                    prefix = f"{stage_prefix}yyyy={check_date.year}/mm={check_date.month:02d}/"
                    try:
                        resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix, MaxKeys=1000)
                        for obj in resp.get("Contents", []):
                            key = obj["Key"]
                            # Check if invoice number appears in the filename
                            if invoice_number.replace(" ", "-") in key or invoice_number.replace(" ", "_") in key:
                                # Don't flag if it's the same pdf (different version)
                                duplicate_found = True
                                break
                    except Exception:
                        pass
                    if duplicate_found:
                        break
                if duplicate_found:
                    break

            if duplicate_found:
                warnings.append({
                    "type": "duplicate_invoice",
                    "severity": "medium",
                    "message": f"Invoice #{invoice_number} may already exist for this vendor. Please verify.",
                    "details": {"invoice_number": invoice_number, "vendor_id": vendor_id}
                })
        except Exception as e:
            print(f"[Validation] Warning: Duplicate check failed: {e}")

    return warnings


@app.post("/api/validate-submit")
async def api_validate_submit(request: Request, user: str = Depends(require_user)):
    """Validate a bill before submit, checking for potential human mistakes.

    Returns warnings if any suspicious patterns are detected.
    Frontend should show confirmation dialog if warnings exist.
    """
    try:
        payload = await request.json()
        date = payload.get("date", "")
        ids_str = payload.get("ids", "")
        deleted_ids_str = payload.get("deleted_ids", "")

        if not ids_str:
            return {"warnings": [], "valid": True}

        # Parse inputs - use ||| delimiter to match main submit endpoint
        id_list = [i.strip() for i in ids_str.split("|||") if i.strip()]
        deleted_set = set(d.strip() for d in (deleted_ids_str or "").split("|||") if d.strip())

        if not id_list:
            return {"warnings": [], "valid": True}

        # Load the lines
        lines = []
        for line_id in id_list:
            # line_id format: s3_key#row_idx
            if "#" in line_id:
                s3_key, row_idx = line_id.rsplit("#", 1)
                try:
                    row_idx = int(row_idx)
                except ValueError:
                    continue

                # Get cached or fetch line
                cache_key = ("line", s3_key, row_idx)
                cached = _CACHE.get(cache_key)
                if cached and (time.time() - cached.get("ts", 0) < 300):
                    line = cached.get("data", {})
                else:
                    # Fetch from S3
                    try:
                        txt = _read_s3_text(BUCKET, s3_key)
                        raw_lines = [l for l in txt.strip().split('\n') if l.strip()]
                        if 0 <= row_idx < len(raw_lines):
                            line = json.loads(raw_lines[row_idx])
                            line["__id__"] = line_id
                            line["__s3_key__"] = s3_key
                            line["__row_idx__"] = row_idx
                            _CACHE[cache_key] = {"ts": time.time(), "data": line}
                        else:
                            continue
                    except Exception:
                        continue
                lines.append(line)

        if not lines:
            return {"warnings": [], "valid": True}

        # Get header fields from drafts
        first = lines[0]
        s3_key = first.get("__s3_key__", "")
        pid = pdf_id_from_key(s3_key) if s3_key else ""
        header_draft = get_draft(pid, "__header__", user) if pid else None
        header_fields = dict(header_draft.get("fields", {}) or {}) if header_draft else {}

        # Extract vendor/property info
        vendor_id = str(header_fields.get("EnrichedVendorID") or first.get("EnrichedVendorID") or "").strip()
        property_id = str(header_fields.get("EnrichedPropertyID") or first.get("EnrichedPropertyID") or "").strip()
        account_number = str(header_fields.get("Account Number") or first.get("Account Number") or "").strip()

        # Run validation
        warnings = _detect_human_mistakes(
            lines=lines,
            deleted_ids=deleted_set,
            header_fields=header_fields,
            vendor_id=vendor_id,
            property_id=property_id,
            account_number=account_number,
        )

        return {
            "warnings": warnings,
            "valid": len(warnings) == 0,
            "warning_count": len(warnings),
            "high_severity_count": len([w for w in warnings if w.get("severity") == "high"]),
        }

    except Exception as e:
        print(f"[Validation] Error: {e}")
        import traceback
        traceback.print_exc()
        return {"warnings": [], "valid": True, "error": str(e)}


@app.get("/api/ai-review/stats")
def api_ai_review_stats(days: int = 30, user: str = Depends(require_user)):
    """Get AI review accuracy statistics."""
    try:
        # Scan for ACCURACY# records from the last N days
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()

        # This is a full scan - in production you'd want a GSI
        # For now, keep it simple
        all_accuracy = []
        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="begins_with(sk, :prefix) AND #ts >= :cutoff",
            ExpressionAttributeNames={"#ts": "timestamp"},
            ExpressionAttributeValues={
                ":prefix": {"S": "ACCURACY#"},
                ":cutoff": {"S": cutoff}
            }
        ):
            for item in page.get("Items", []):
                all_accuracy.append({
                    "pdf_id": item.get("pk", {}).get("S", ""),
                    "timestamp": item.get("timestamp", {}).get("S", ""),
                    "user": item.get("user", {}).get("S", ""),
                    "deletion_tp": int(item.get("deletion_tp", {}).get("N", "0")),
                    "deletion_fp": int(item.get("deletion_fp", {}).get("N", "0")),
                    "deletion_fn": int(item.get("deletion_fn", {}).get("N", "0")),
                    "auto_pass_predicted": item.get("auto_pass_predicted", {}).get("BOOL", False),
                    "auto_pass_correct": item.get("auto_pass_correct", {}).get("BOOL", False),
                    "human_changed": item.get("human_changed", {}).get("BOOL", False),
                    "ai_confidence": float(item.get("ai_confidence", {}).get("N", "0")),
                })

        if not all_accuracy:
            return {
                "total_reviews": 0,
                "message": "No AI accuracy data yet. Reviews will be tracked as users submit bills."
            }

        # Aggregate stats
        total = len(all_accuracy)
        total_tp = sum(a["deletion_tp"] for a in all_accuracy)
        total_fp = sum(a["deletion_fp"] for a in all_accuracy)
        total_fn = sum(a["deletion_fn"] for a in all_accuracy)

        # Deletion precision/recall
        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        # Auto-pass accuracy
        auto_pass_count = sum(1 for a in all_accuracy if a["auto_pass_predicted"])
        auto_pass_correct_count = sum(1 for a in all_accuracy if a["auto_pass_correct"])
        auto_pass_accuracy = auto_pass_correct_count / total if total > 0 else 0

        # Average confidence
        avg_confidence = sum(a["ai_confidence"] for a in all_accuracy) / total if total > 0 else 0

        return {
            "total_reviews": total,
            "days": days,
            "deletion_stats": {
                "true_positives": total_tp,
                "false_positives": total_fp,
                "false_negatives": total_fn,
                "precision": round(precision * 100, 1),
                "recall": round(recall * 100, 1),
                "f1_score": round(f1 * 100, 1),
            },
            "auto_pass_stats": {
                "total_predicted": auto_pass_count,
                "correct": auto_pass_correct_count,
                "accuracy": round(auto_pass_accuracy * 100, 1),
            },
            "avg_confidence": round(avg_confidence, 1),
            "recent_reviews": sorted(all_accuracy, key=lambda x: x["timestamp"], reverse=True)[:10]
        }

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "ai stats")}, status_code=500)


@app.get("/api/ai-learning/stats")
def api_ai_learning_stats(user: str = Depends(require_user)):
    """Get AI learning statistics - patterns learned, quarantined, etc."""
    try:
        stats = {
            "delete_patterns": {"active": 0, "quarantined": 0},
            "keep_patterns": {"active": 0, "quarantined": 0},
            "gl_patterns": {"active": 0, "quarantined": 0},
            "total_actions_captured": 0,
            "recent_patterns": [],
        }

        # Count patterns by type and status
        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="begins_with(pk, :prefix)",
            ExpressionAttributeValues={":prefix": {"S": "PATTERN#"}}
        ):
            for item in page.get("Items", []):
                pk = item.get("pk", {}).get("S", "")
                status = item.get("status", {}).get("S", "active")
                is_quarantined = status == "quarantined"

                if "PATTERN#DELETE#" in pk:
                    if is_quarantined:
                        stats["delete_patterns"]["quarantined"] += 1
                    else:
                        stats["delete_patterns"]["active"] += 1
                elif "PATTERN#KEEP#" in pk:
                    if is_quarantined:
                        stats["keep_patterns"]["quarantined"] += 1
                    else:
                        stats["keep_patterns"]["active"] += 1
                elif "PATTERN#GL#" in pk:
                    if is_quarantined:
                        stats["gl_patterns"]["quarantined"] += 1
                    else:
                        stats["gl_patterns"]["active"] += 1

                # Collect recent patterns
                if len(stats["recent_patterns"]) < 20:
                    stats["recent_patterns"].append({
                        "type": item.get("pattern_type", {}).get("S", ""),
                        "vendor_id": item.get("vendor_id", {}).get("S", ""),
                        "property_id": item.get("property_id", {}).get("S", ""),
                        "description_pattern": item.get("description_pattern", {}).get("S", ""),
                        "status": status,
                        "created_at": item.get("created_at", {}).get("S", ""),
                        "created_by": item.get("created_by", {}).get("S", ""),
                    })

        # Count actions captured
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="begins_with(pk, :prefix)",
            ExpressionAttributeValues={":prefix": {"S": "ACTION#"}}
        ):
            stats["total_actions_captured"] += len(page.get("Items", []))

        # Sort recent patterns by created_at descending
        stats["recent_patterns"] = sorted(
            stats["recent_patterns"],
            key=lambda x: x.get("created_at", ""),
            reverse=True
        )[:10]

        # Calculate totals
        stats["total_active_patterns"] = (
            stats["delete_patterns"]["active"] +
            stats["keep_patterns"]["active"] +
            stats["gl_patterns"]["active"]
        )
        stats["total_quarantined_patterns"] = (
            stats["delete_patterns"]["quarantined"] +
            stats["keep_patterns"]["quarantined"] +
            stats["gl_patterns"]["quarantined"]
        )

        return stats

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "learning stats")}, status_code=500)


@app.get("/api/ai-learning/quarantined")
def api_ai_learning_quarantined(user: str = Depends(require_admin)):
    """Get quarantined patterns pending admin review."""
    try:
        quarantined = []
        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="begins_with(pk, :prefix) AND #status = :quarantined",
            ExpressionAttributeNames={"#status": "status"},
            ExpressionAttributeValues={
                ":prefix": {"S": "PATTERN#"},
                ":quarantined": {"S": "quarantined"}
            }
        ):
            for item in page.get("Items", []):
                quarantined.append({
                    "pk": item.get("pk", {}).get("S", ""),
                    "sk": item.get("sk", {}).get("S", ""),
                    "pattern_type": item.get("pattern_type", {}).get("S", ""),
                    "vendor_id": item.get("vendor_id", {}).get("S", ""),
                    "property_id": item.get("property_id", {}).get("S", ""),
                    "description_pattern": item.get("description_pattern", {}).get("S", ""),
                    "original_description": item.get("original_description", {}).get("S", ""),
                    "quarantine_reason": item.get("quarantine_reason", {}).get("S", ""),
                    "created_at": item.get("created_at", {}).get("S", ""),
                    "created_by": item.get("created_by", {}).get("S", ""),
                    "source_pdf_id": item.get("source_pdf_id", {}).get("S", ""),
                })

        return {"quarantined": sorted(quarantined, key=lambda x: x.get("created_at", ""), reverse=True)}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "quarantined patterns")}, status_code=500)


@app.post("/api/ai-learning/review-pattern")
async def api_ai_learning_review_pattern(request: Request, user: str = Depends(require_admin)):
    """Review a quarantined pattern - approve (activate) or reject (delete)."""
    try:
        payload = await request.json()
        pk = payload.get("pk", "")
        sk = payload.get("sk", "")
        action = payload.get("action", "")  # "approve" or "reject"

        if not pk or not sk or action not in ("approve", "reject"):
            return JSONResponse({"error": "Missing pk, sk, or valid action"}, status_code=400)

        if action == "approve":
            # Update status to active
            ddb.update_item(
                TableName=AI_SUGGESTIONS_TABLE,
                Key={"pk": {"S": pk}, "sk": {"S": sk}},
                UpdateExpression="SET #status = :active, approved_by = :user, approved_at = :now",
                ExpressionAttributeNames={"#status": "status"},
                ExpressionAttributeValues={
                    ":active": {"S": "active"},
                    ":user": {"S": user},
                    ":now": {"S": datetime.now(timezone.utc).isoformat()}
                }
            )
            return {"success": True, "message": "Pattern approved and activated"}
        else:
            # Delete the pattern
            ddb.delete_item(
                TableName=AI_SUGGESTIONS_TABLE,
                Key={"pk": {"S": pk}, "sk": {"S": sk}}
            )
            return {"success": True, "message": "Pattern rejected and deleted"}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "review pattern")}, status_code=500)


@app.post("/api/ai-learning/flag-bad-data")
async def api_ai_learning_flag_bad_data(request: Request, user: str = Depends(require_admin)):
    """Flag a pdf_id as bad training data - remove all patterns learned from it."""
    try:
        payload = await request.json()
        pdf_id = payload.get("pdf_id", "")
        reason = payload.get("reason", "admin_flagged")

        if not pdf_id:
            return JSONResponse({"error": "Missing pdf_id"}, status_code=400)

        removed_count = 0

        # Find and delete all patterns from this source
        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="source_pdf_id = :pdf_id",
            ExpressionAttributeValues={":pdf_id": {"S": pdf_id}}
        ):
            for item in page.get("Items", []):
                pk = item.get("pk", {}).get("S", "")
                sk = item.get("sk", {}).get("S", "")
                if pk and sk:
                    ddb.delete_item(
                        TableName=AI_SUGGESTIONS_TABLE,
                        Key={"pk": {"S": pk}, "sk": {"S": sk}}
                    )
                    removed_count += 1

        # Also mark the action record as bad
        try:
            action_pk = f"ACTION#{pdf_id}"
            resp = ddb.query(
                TableName=AI_SUGGESTIONS_TABLE,
                KeyConditionExpression="pk = :pk",
                ExpressionAttributeValues={":pk": {"S": action_pk}},
                Limit=10
            )
            for item in resp.get("Items", []):
                sk = item.get("sk", {}).get("S", "")
                if sk:
                    ddb.update_item(
                        TableName=AI_SUGGESTIONS_TABLE,
                        Key={"pk": {"S": action_pk}, "sk": {"S": sk}},
                        UpdateExpression="SET #status = :bad, flagged_by = :user, flagged_at = :now, flagged_reason = :reason",
                        ExpressionAttributeNames={"#status": "status"},
                        ExpressionAttributeValues={
                            ":bad": {"S": "bad_data"},
                            ":user": {"S": user},
                            ":now": {"S": datetime.now(timezone.utc).isoformat()},
                            ":reason": {"S": reason}
                        }
                    )
        except Exception:
            pass

        return {"success": True, "removed_patterns": removed_count, "message": f"Flagged {pdf_id} as bad data"}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "flag bad data")}, status_code=500)


# ========== GRANULAR AUTONOMY CONTROL ==========
# Per-vendor/property autonomy levels: shadow -> assisted -> autonomous

AUTONOMY_LEVELS = ["shadow", "assisted", "autonomous"]
AUTONOMY_CONFIG_PK = "AUTONOMY"


def _get_autonomy_config(vendor_id: str, property_id: str = "") -> dict:
    """Get autonomy configuration for a vendor/property combo.

    Checks in order: combo (vendor+property) > vendor > default (shadow)
    """
    default_config = {
        "level": "shadow",
        "enabled_actions": [],
        "min_accuracy_required": 95,
        "min_history_count": 20,
        "bills_processed": 0,
        "current_accuracy": 0,
    }

    try:
        # Try combo first (most specific)
        if property_id:
            combo_sk = f"COMBO#{vendor_id}#{property_id}"
            resp = ddb.get_item(
                TableName=CONFIG_TABLE,
                Key={"PK": {"S": AUTONOMY_CONFIG_PK}, "SK": {"S": combo_sk}}
            )
            if "Item" in resp:
                return _parse_autonomy_item(resp["Item"])

        # Try vendor level
        if vendor_id:
            vendor_sk = f"VENDOR#{vendor_id}"
            resp = ddb.get_item(
                TableName=CONFIG_TABLE,
                Key={"PK": {"S": AUTONOMY_CONFIG_PK}, "SK": {"S": vendor_sk}}
            )
            if "Item" in resp:
                return _parse_autonomy_item(resp["Item"])

        return default_config

    except Exception as e:
        print(f"[Autonomy] Error getting config: {e}")
        return default_config


def _parse_autonomy_item(item: dict) -> dict:
    """Parse DynamoDB autonomy config item to dict."""
    return {
        "level": item.get("level", {}).get("S", "shadow"),
        "enabled_actions": json.loads(item.get("enabled_actions", {}).get("S", "[]")),
        "min_accuracy_required": float(item.get("min_accuracy_required", {}).get("N", "95")),
        "min_history_count": int(item.get("min_history_count", {}).get("N", "20")),
        "bills_processed": int(item.get("bills_processed", {}).get("N", "0")),
        "current_accuracy": float(item.get("current_accuracy", {}).get("N", "0")),
        "graduated_at": item.get("graduated_at", {}).get("S", ""),
        "graduated_by": item.get("graduated_by", {}).get("S", ""),
        "vendor_id": item.get("vendor_id", {}).get("S", ""),
        "property_id": item.get("property_id", {}).get("S", ""),
        "SK": item.get("SK", {}).get("S", ""),
    }


def _compute_vendor_accuracy(vendor_id: str, property_id: str = "", days: int = 30) -> dict:
    """Compute accuracy stats for a specific vendor/property over the last N days."""
    try:
        cutoff = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()

        # Query accuracy records that include this vendor
        # This is inefficient (scan) but works for now
        total_reviews = 0
        total_tp = 0
        total_fp = 0
        total_fn = 0
        auto_pass_correct = 0

        paginator = ddb.get_paginator('scan')
        for page in paginator.paginate(
            TableName=AI_SUGGESTIONS_TABLE,
            FilterExpression="begins_with(sk, :prefix) AND #ts >= :cutoff",
            ExpressionAttributeNames={"#ts": "timestamp"},
            ExpressionAttributeValues={
                ":prefix": {"S": "ACCURACY#"},
                ":cutoff": {"S": cutoff}
            }
        ):
            for item in page.get("Items", []):
                # Check if this accuracy record is for our vendor
                pdf_id = item.get("pk", {}).get("S", "")
                # Would need to look up the pdf to get vendor - for now just aggregate all
                total_reviews += 1
                total_tp += int(item.get("deletion_tp", {}).get("N", "0"))
                total_fp += int(item.get("deletion_fp", {}).get("N", "0"))
                total_fn += int(item.get("deletion_fn", {}).get("N", "0"))
                if item.get("auto_pass_correct", {}).get("BOOL", False):
                    auto_pass_correct += 1

        if total_reviews == 0:
            return {"accuracy": 0, "bills_processed": 0, "eligible": False}

        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        auto_pass_rate = auto_pass_correct / total_reviews if total_reviews > 0 else 0

        # Overall accuracy is weighted average
        accuracy = (precision * 0.6 + auto_pass_rate * 0.4) * 100

        return {
            "accuracy": round(accuracy, 1),
            "bills_processed": total_reviews,
            "precision": round(precision * 100, 1),
            "auto_pass_rate": round(auto_pass_rate * 100, 1),
            "eligible": total_reviews >= 20 and accuracy >= 80,
        }

    except Exception as e:
        print(f"[Autonomy] Error computing accuracy: {e}")
        return {"accuracy": 0, "bills_processed": 0, "eligible": False}


@app.get("/api/autonomy/config")
def api_autonomy_config_list(user: str = Depends(require_admin)):
    """List all autonomy configurations."""
    try:
        configs = []
        resp = ddb.query(
            TableName=CONFIG_TABLE,
            KeyConditionExpression="PK = :pk",
            ExpressionAttributeValues={":pk": {"S": AUTONOMY_CONFIG_PK}}
        )
        for item in resp.get("Items", []):
            config = _parse_autonomy_item(item)
            configs.append(config)

        return {"configs": sorted(configs, key=lambda x: x.get("level", ""), reverse=True)}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "autonomy config")}, status_code=500)


@app.get("/api/autonomy/config/{vendor_id}")
def api_autonomy_config_get(vendor_id: str, property_id: str = "", user: str = Depends(require_user)):
    """Get autonomy config for a specific vendor/property."""
    try:
        config = _get_autonomy_config(vendor_id, property_id)
        accuracy = _compute_vendor_accuracy(vendor_id, property_id)
        config.update(accuracy)
        return config

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "autonomy config")}, status_code=500)


@app.post("/api/autonomy/promote")
async def api_autonomy_promote(request: Request, user: str = Depends(require_admin)):
    """Promote a vendor/property to a higher autonomy level."""
    try:
        payload = await request.json()
        vendor_id = payload.get("vendor_id", "")
        property_id = payload.get("property_id", "")
        target_level = payload.get("level", "")

        if not vendor_id:
            return JSONResponse({"error": "vendor_id required"}, status_code=400)
        if target_level not in AUTONOMY_LEVELS:
            return JSONResponse({"error": f"Invalid level. Must be one of: {AUTONOMY_LEVELS}"}, status_code=400)

        # Determine SK based on whether property is specified
        if property_id:
            sk = f"COMBO#{vendor_id}#{property_id}"
        else:
            sk = f"VENDOR#{vendor_id}"

        # Check current accuracy
        accuracy = _compute_vendor_accuracy(vendor_id, property_id)

        # Validate graduation criteria
        if target_level in ("assisted", "autonomous"):
            if accuracy["bills_processed"] < 20:
                return JSONResponse({
                    "error": f"Insufficient history. Need 20+ bills, have {accuracy['bills_processed']}"
                }, status_code=400)
            if target_level == "autonomous" and accuracy["accuracy"] < 95:
                return JSONResponse({
                    "error": f"Accuracy too low for autonomous. Need 95%+, have {accuracy['accuracy']}%"
                }, status_code=400)
            if target_level == "assisted" and accuracy["accuracy"] < 80:
                return JSONResponse({
                    "error": f"Accuracy too low for assisted. Need 80%+, have {accuracy['accuracy']}%"
                }, status_code=400)

        # Determine enabled actions based on level
        enabled_actions = []
        if target_level == "assisted":
            enabled_actions = ["pre_select_deletions", "highlight_suggestions"]
        elif target_level == "autonomous":
            enabled_actions = ["auto_delete_garbage", "auto_fix_dates", "auto_post"]

        now = datetime.now(timezone.utc).isoformat()

        item = {
            "PK": {"S": AUTONOMY_CONFIG_PK},
            "SK": {"S": sk},
            "level": {"S": target_level},
            "vendor_id": {"S": vendor_id},
            "property_id": {"S": property_id},
            "enabled_actions": {"S": json.dumps(enabled_actions)},
            "min_accuracy_required": {"N": "95"},
            "min_history_count": {"N": "20"},
            "bills_processed": {"N": str(accuracy["bills_processed"])},
            "current_accuracy": {"N": str(accuracy["accuracy"])},
            "graduated_at": {"S": now},
            "graduated_by": {"S": user},
        }

        ddb.put_item(TableName=CONFIG_TABLE, Item=item)

        return {
            "success": True,
            "message": f"Promoted to {target_level}",
            "vendor_id": vendor_id,
            "property_id": property_id,
            "level": target_level,
            "enabled_actions": enabled_actions,
        }

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "autonomy promote")}, status_code=500)


@app.post("/api/autonomy/demote")
async def api_autonomy_demote(request: Request, user: str = Depends(require_admin)):
    """Demote a vendor/property to shadow mode."""
    try:
        payload = await request.json()
        vendor_id = payload.get("vendor_id", "")
        property_id = payload.get("property_id", "")
        reason = payload.get("reason", "manual_demotion")

        if not vendor_id:
            return JSONResponse({"error": "vendor_id required"}, status_code=400)

        # Determine SK
        if property_id:
            sk = f"COMBO#{vendor_id}#{property_id}"
        else:
            sk = f"VENDOR#{vendor_id}"

        now = datetime.now(timezone.utc).isoformat()

        # Update to shadow
        ddb.update_item(
            TableName=CONFIG_TABLE,
            Key={"PK": {"S": AUTONOMY_CONFIG_PK}, "SK": {"S": sk}},
            UpdateExpression="SET #level = :shadow, demoted_at = :now, demoted_by = :user, demotion_reason = :reason",
            ExpressionAttributeNames={"#level": "level"},
            ExpressionAttributeValues={
                ":shadow": {"S": "shadow"},
                ":now": {"S": now},
                ":user": {"S": user},
                ":reason": {"S": reason}
            }
        )

        return {"success": True, "message": "Demoted to shadow mode", "reason": reason}

    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "autonomy demote")}, status_code=500)


def _check_autonomy_health():
    """Background task to check all autonomous vendors and demote if accuracy drops.

    Should be called periodically (e.g., daily).
    """
    try:
        demoted = []

        resp = ddb.query(
            TableName=CONFIG_TABLE,
            KeyConditionExpression="PK = :pk",
            ExpressionAttributeValues={":pk": {"S": AUTONOMY_CONFIG_PK}}
        )

        for item in resp.get("Items", []):
            config = _parse_autonomy_item(item)
            if config["level"] != "autonomous":
                continue

            vendor_id = config.get("vendor_id", "")
            property_id = config.get("property_id", "")

            # Check recent accuracy
            accuracy = _compute_vendor_accuracy(vendor_id, property_id, days=7)

            if accuracy["accuracy"] < config.get("min_accuracy_required", 95):
                # Demote
                sk = item.get("SK", {}).get("S", "")
                if sk:
                    ddb.update_item(
                        TableName=CONFIG_TABLE,
                        Key={"PK": {"S": AUTONOMY_CONFIG_PK}, "SK": {"S": sk}},
                        UpdateExpression="SET #level = :shadow, demoted_at = :now, demotion_reason = :reason",
                        ExpressionAttributeNames={"#level": "level"},
                        ExpressionAttributeValues={
                            ":shadow": {"S": "shadow"},
                            ":now": {"S": datetime.now(timezone.utc).isoformat()},
                            ":reason": {"S": f"accuracy_drop_{accuracy['accuracy']}%"}
                        }
                    )
                    demoted.append({"vendor_id": vendor_id, "accuracy": accuracy["accuracy"]})
                    print(f"[Autonomy] Auto-demoted vendor {vendor_id} - accuracy dropped to {accuracy['accuracy']}%")

        return demoted

    except Exception as e:
        print(f"[Autonomy] Health check error: {e}")
        return []


@app.post("/api/autonomy/health-check")
def api_autonomy_health_check(user: str = Depends(require_admin)):
    """Trigger autonomy health check - demote vendors with low accuracy."""
    try:
        demoted = _check_autonomy_health()
        return {"success": True, "demoted": demoted, "count": len(demoted)}
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "health check")}, status_code=500)


@app.get("/ai-review-dashboard", response_class=HTMLResponse)
def ai_review_dashboard(request: Request, user: str = Depends(require_user)):
    """AI Review accuracy dashboard - shows how well AI suggestions match human decisions."""
    if user not in ADMIN_USERS:
        return templates.TemplateResponse("error.html", {
            "request": request,
            "error": "Access denied. Admin only."
        })
    return templates.TemplateResponse("ai_review_dashboard.html", {"request": request, "user": user})


# ======================================================================
# SUBMETER RATES  Audit water & sewer per-gallon submetering rates
# ======================================================================

# Manual UOM-to-gallons factors for the override dropdown
_SUBMETER_UOM_OVERRIDES = {
    "Gallons":     1.0,
    "kGal":        1000.0,
    "HCF":         748.05,
    "CCF":         748.05,
    "100 Gallons": 100.0,
    "Acre-Feet":   325851.0,
}


def _consumption_to_gallons(raw_consumption, raw_uom: str, utility_type: str,
                            uom_override: str | None = None) -> tuple[float, float, str]:
    """Convert consumption value to gallons for water/sewer utilities.

    Returns (gallons, factor_used, conversion_label) where:
      - gallons = parsed_value * factor_used
      - factor_used = the multiplier that was applied
      - conversion_label = human-readable explanation, e.g. "748.05 (HCFGal)"
    Returns (0.0, 0.0, "no consumption") when consumption is missing.
    """
    parsed = _parse_consumption(raw_consumption)
    if parsed is None or parsed == 0:
        return (0.0, 0.0, "no consumption")

    # ---- Manual override takes precedence ----
    if uom_override and uom_override != "Auto":
        override_factor = _SUBMETER_UOM_OVERRIDES.get(uom_override)
        if override_factor is not None:
            label = f"{override_factor:g} ({uom_override}Gal)"
            return (parsed * override_factor, override_factor, label)

    # ---- Auto-detect from the invoice UOM field ----
    canonical, category, factor = _normalize_uom(raw_uom or "")

    ut = (utility_type or "").lower()
    if category == "water" or ut in ("water", "sewer"):
        if canonical == "Gallons":
            if factor == 1.0:
                return (parsed * factor, factor, "1 (Gallons)")
            label = f"{factor:g} ({(raw_uom or '?').strip()}Gal)"
            return (parsed * factor, factor, label)
        if category not in ("water",):
            return (0.0, 0.0, f"unknown UOM '{(raw_uom or '').strip()}'")
        label = f"{factor:g} ({(raw_uom or '?').strip()}Gal)"
        return (parsed * factor, factor, label)

    return (0.0, 0.0, f"not water/sewer")


@app.get("/submeter-rates", response_class=HTMLResponse)
def submeter_rates_view(request: Request, user: str = Depends(require_user)):
    """Submeter Rates page  audit water & sewer submetering rates by property."""
    return templates.TemplateResponse("submeter-rates.html", {"request": request, "user": user})


# ---- Async scan state ----
_SUBMETER_RATES_STATUS: dict = {
    "running": False,
    "ubi_period": None,
    "progress": "",       # human-readable progress string
    "files_total": 0,
    "files_done": 0,
    "result": None,       # final result dict once done
    "error": None,
}
_SUBMETER_RATES_LOCK = threading.Lock()


def _submeter_rates_scan(ubi_period: str):
    """Background worker: scan Stage 8 and compute submeter rates."""
    st = _SUBMETER_RATES_STATUS
    try:
        st["progress"] = "Listing Stage 8 files..."
        print(f"[SUBMETER RATES] Generating for period {ubi_period} ...")

        # ---- Scan Stage 8 ----
        prefixes_to_scan = []
        today = datetime.now()
        for i in range(365):
            d = today - timedelta(days=i)
            prefix = f"{UBI_ASSIGNED_PREFIX}yyyy={d.year}/mm={d.month:02d}/dd={d.day:02d}/"
            prefixes_to_scan.append(prefix)

        all_keys: list[str] = []
        for prefix in prefixes_to_scan:
            try:
                paginator = s3.get_paginator("list_objects_v2")
                for page in paginator.paginate(Bucket=BUCKET, Prefix=prefix):
                    for obj in page.get("Contents", []):
                        key = obj["Key"]
                        if key.endswith(".jsonl"):
                            all_keys.append(key)
            except Exception:
                pass

        st["files_total"] = len(all_keys)
        st["progress"] = f"Reading {len(all_keys)} files..."
        print(f"[SUBMETER RATES] Found {len(all_keys)} Stage 8 files")

        # ---- Read files concurrently ----
        matched_items: list[dict] = []
        files_done_counter = [0]

        def _process_file(key: str) -> list[dict]:
            try:
                body = _read_s3_text(BUCKET, key)
                items = []
                for line in body.splitlines():
                    line = (line or "").strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                    except Exception:
                        continue
                    # Only Water / Sewer
                    util = (rec.get("Utility Type") or rec.get("Mapped Utility Name") or rec.get("Utility Name") or rec.get("utility_name") or "").strip()
                    if util.lower() not in ("water", "sewer"):
                        continue
                    # Check if this record has the requested period
                    ubi_assignments = rec.get("ubi_assignments", [])
                    if not ubi_assignments:
                        lp = rec.get("ubi_period", "")
                        if lp == ubi_period:
                            rec["_jsonl_key"] = key
                            items.append(rec)
                    else:
                        for asn in ubi_assignments:
                            if asn.get("period") == ubi_period:
                                rec["_jsonl_key"] = key
                                items.append(rec)
                                break
                return items
            except Exception as e:
                print(f"[SUBMETER RATES] Error reading {key}: {e}")
                return []
            finally:
                files_done_counter[0] += 1
                st["files_done"] = files_done_counter[0]

        with ThreadPoolExecutor(max_workers=20) as executor:
            futures = {executor.submit(_process_file, key): key for key in all_keys}
            for future in as_completed(futures):
                matched_items.extend(future.result())

        st["progress"] = f"Aggregating {len(matched_items)} records..."
        print(f"[SUBMETER RATES] {len(matched_items)} records match period {ubi_period}")

        # ---- Load config for calculation_desc + uom_override ----
        cfg_items = _ddb_get_config("submeter-rate-config") or []
        cfg_calc_map: dict[str, str] = {}
        cfg_uom_map: dict[str, str] = {}
        for c in cfg_items:
            k = f"{c.get('property_name')}|{c.get('utility_name')}"
            cfg_calc_map[k] = c.get("calculation_desc", "From Invoice & Volume")
            cfg_uom_map[k] = c.get("uom_override", "Auto")

        # ---- Aggregate by (property_name, utility_type) ----
        agg: dict[str, dict] = {}

        for rec in matched_items:
            property_name = rec.get("EnrichedPropertyName") or rec.get("Property Name") or ""
            property_id = rec.get("EnrichedPropertyID") or rec.get("Property ID") or ""
            utility_name = (rec.get("Utility Type") or rec.get("Mapped Utility Name") or rec.get("Utility Name") or rec.get("utility_name") or "").strip()
            ar_code = rec.get("Charge Code") or ""

            ubi_assignments = rec.get("ubi_assignments", [])
            amount_overridden = rec.get("Amount Overridden") or rec.get("amount_overridden")

            if not ubi_assignments:
                if amount_overridden:
                    ubi_amount = float(rec.get("Current Amount") or rec.get("current_amount") or 0)
                else:
                    ubi_amount = float(rec.get("ubi_amount", 0))
                    if ubi_amount == 0:
                        charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                        ubi_amount = float(charge_str) if charge_str else 0.0
                total_bill_amount = ubi_amount
                period_amount = ubi_amount
            else:
                # Get the line-item charge as fallback when assignment amounts are 0
                line_charge = 0.0
                if rec.get("Current Amount") is not None:
                    line_charge = float(rec.get("Current Amount") or 0)
                else:
                    charge_str = str(rec.get("Line Item Charge", "0") or "0").replace("$", "").replace(",", "").strip()
                    line_charge = float(charge_str) if charge_str else 0.0

                period_amount = 0.0
                total_bill_amount = 0.0
                num_periods = len(ubi_assignments)
                for asn in ubi_assignments:
                    asn_amt = float(asn.get("amount", 0))
                    total_bill_amount += asn_amt
                    if asn.get("period") == ubi_period:
                        if amount_overridden:
                            period_amount = float(rec.get("Current Amount") or rec.get("current_amount") or 0)
                            orig_total = sum(float(a.get("amount", 0)) for a in ubi_assignments)
                            orig_this = float(asn.get("amount", 0))
                            if orig_total > 0:
                                period_amount = period_amount * (orig_this / orig_total)
                        else:
                            period_amount = asn_amt
                # If all assignment amounts are 0, fall back to line item charge
                if total_bill_amount == 0 and line_charge > 0:
                    total_bill_amount = line_charge
                    # Split evenly across periods if multi, or full amount if single
                    period_amount = line_charge / num_periods if num_periods > 1 else line_charge

            raw_consumption = rec.get("ENRICHED CONSUMPTION") or rec.get("Consumption Amount") or rec.get("Consumption") or rec.get("consumption") or rec.get("Line Item Consumption")
            raw_uom = rec.get("ENRICHED UOM") or rec.get("Unit of Measure") or rec.get("UOM") or rec.get("uom") or ""
            agg_key = f"{property_name}|{utility_name}"
            uom_override = cfg_uom_map.get(agg_key, "Auto")
            gallons, factor_used, conv_label = _consumption_to_gallons(
                raw_consumption, raw_uom, utility_name, uom_override=uom_override)
            parsed_raw = _parse_consumption(raw_consumption) or 0.0

            if ubi_assignments and len(ubi_assignments) > 1 and total_bill_amount > 0:
                prorate_ratio = (period_amount / total_bill_amount) if total_bill_amount else 0.0
                if gallons > 0:
                    gallons = gallons * prorate_ratio
                parsed_raw = parsed_raw * prorate_ratio

            if agg_key not in agg:
                agg[agg_key] = {
                    "property_name": property_name,
                    "property_id": property_id,
                    "ar_code": ar_code,
                    "invoice_total": 0.0,
                    "volume_total_gals": 0.0,
                    "raw_consumption_total": 0.0,
                    "utility_name": utility_name,
                    "line_count": 0,
                    "raw_uoms": set(),
                    "conversion_label": conv_label,
                    "factor_used": factor_used,
                    "source_lines": [],
                }
            agg[agg_key]["invoice_total"] += period_amount
            agg[agg_key]["volume_total_gals"] += gallons
            agg[agg_key]["raw_consumption_total"] += parsed_raw
            agg[agg_key]["line_count"] += 1
            if raw_uom:
                agg[agg_key]["raw_uoms"].add(raw_uom.strip())
            if ar_code and not agg[agg_key]["ar_code"]:
                agg[agg_key]["ar_code"] = ar_code

            # Collect per-line detail for drill-down
            vendor = rec.get("EnrichedVendorName") or rec.get("Vendor Name") or rec.get("Account Name") or ""
            acct = rec.get("Account Number") or rec.get("account_number") or ""
            svc_start = rec.get("Bill Period Start") or rec.get("Service Start Date") or ""
            svc_end = rec.get("Bill Period End") or rec.get("Service End Date") or ""
            # PDF location: prefer source_input_key, then pdfKey, then PDF_LINK if it looks like an S3 path
            pdf_s3_key = (rec.get("source_input_key") or rec.get("pdfKey") or rec.get("__pdf_s3_key__") or "").strip()
            if not pdf_s3_key:
                _pl = (rec.get("PDF_LINK") or "").strip()
                if _pl and ("Bill_Parser" in _pl or _pl.startswith("s3://") or ("/" in _pl and not _pl.startswith("http"))):
                    pdf_s3_key = _pl.replace("s3://", "").lstrip("/")
                    if pdf_s3_key.startswith(f"{BUCKET}/"):
                        pdf_s3_key = pdf_s3_key[len(BUCKET) + 1:]
            jsonl_key = rec.get("_jsonl_key", "")
            line_pdf_id = pdf_id_from_key(jsonl_key) if jsonl_key else ""
            agg[agg_key]["source_lines"].append({
                "vendor": vendor,
                "account": acct,
                "service_period": f"{svc_start} - {svc_end}" if svc_start else "",
                "amount": round(period_amount, 2),
                "raw_consumption": round(parsed_raw, 2),
                "raw_uom": (raw_uom or "").strip(),
                "gallons": round(gallons, 2),
                "conversion": conv_label,
                "pdf_s3_key": pdf_s3_key,
                "jsonl_key": jsonl_key,
                "pdf_id": line_pdf_id,
            })

        # ---- Build rows ----
        run_dt = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
        rows = []
        for entry in agg.values():
            vol = entry["volume_total_gals"]
            total = entry["invoice_total"]
            rate = (total / vol) if vol and vol > 0 else None
            cfg_key = f"{entry['property_name']}|{entry['utility_name']}"
            raw_uoms_sorted = sorted(entry.get("raw_uoms", set()))
            rows.append({
                "property_name": entry["property_name"],
                "property_id": entry["property_id"],
                "ar_code": entry["ar_code"],
                "invoice_total": round(total, 2),
                "volume_total_gals": round(vol, 2),
                "raw_consumption": round(entry.get("raw_consumption_total", 0), 2),
                "utility_name": entry["utility_name"],
                "rate": round(rate, 6) if rate is not None else None,
                "calculation_desc": cfg_calc_map.get(cfg_key, "From Invoice & Volume"),
                "raw_uom": ", ".join(raw_uoms_sorted) if raw_uoms_sorted else "",
                "uom_override": cfg_uom_map.get(cfg_key, "Auto"),
                "conversion": entry.get("conversion_label", ""),
                "factor_used": entry.get("factor_used", 0),
                "run_datetime": run_dt,
                "line_count": entry["line_count"],
                "source_lines": entry.get("source_lines", []),
            })

        rows.sort(key=lambda r: (r["property_name"].lower(), r["utility_name"]))

        result = {
            "ok": True,
            "ubi_period": ubi_period,
            "rows": rows,
            "run_datetime": run_dt,
            "files_scanned": len(all_keys),
            "records_matched": len(matched_items),
        }
        # Cache result
        cache_key = ("submeter-rates-generate", ubi_period)
        _CACHE[cache_key] = {"ts": time.time(), "data": result}

        with _SUBMETER_RATES_LOCK:
            st["result"] = result
            st["running"] = False
            st["progress"] = "Done"
        print(f"[SUBMETER RATES] Done  {len(rows)} rows, {len(matched_items)} records")

    except Exception as e:
        print(f"[SUBMETER RATES] Error: {e}")
        import traceback
        traceback.print_exc()
        with _SUBMETER_RATES_LOCK:
            st["running"] = False
            st["error"] = str(e)
            st["progress"] = f"Error: {e}"


@app.post("/api/submeter-rates/generate")
def api_submeter_rates_generate(ubi_period: str = "03/2026", bust_cache: str = "",
                                user: str = Depends(require_user)):
    """Kick off async Stage 8 scan. Returns immediately; poll /status for progress."""
    st = _SUBMETER_RATES_STATUS

    # If not busting cache, check for a recent cached result
    if not bust_cache:
        cache_key = ("submeter-rates-generate", ubi_period)
        cached = _CACHE.get(cache_key)
        if cached and (time.time() - cached.get("ts", 0) < 120):
            return cached.get("data")

    # If already running for same period, just acknowledge
    if st["running"] and st["ubi_period"] == ubi_period:
        return {"ok": True, "status": "running", "progress": st["progress"],
                "files_total": st["files_total"], "files_done": st["files_done"]}

    # Reset state and kick off background thread
    with _SUBMETER_RATES_LOCK:
        st["running"] = True
        st["ubi_period"] = ubi_period
        st["progress"] = "Starting..."
        st["files_total"] = 0
        st["files_done"] = 0
        st["result"] = None
        st["error"] = None

    threading.Thread(target=_submeter_rates_scan, args=(ubi_period,), daemon=True).start()
    return {"ok": True, "status": "started", "progress": "Starting..."}


@app.get("/api/submeter-rates/status")
def api_submeter_rates_status(user: str = Depends(require_user)):
    """Poll for async scan progress. Returns result when done."""
    st = _SUBMETER_RATES_STATUS
    if st["result"] and not st["running"]:
        return st["result"]
    return {
        "ok": True,
        "status": "running" if st["running"] else "idle",
        "progress": st["progress"],
        "files_total": st["files_total"],
        "files_done": st["files_done"],
        "error": st["error"],
    }


@app.get("/api/submeter-rates/config")
def api_submeter_rates_config_get(user: str = Depends(require_user)):
    """Return saved Calculation_Desc config for submeter rates."""
    items = _ddb_get_config("submeter-rate-config") or []
    return {"ok": True, "items": items}


@app.post("/api/submeter-rates/config")
def api_submeter_rates_config_save(request: Request, body: dict = Body(...), user: str = Depends(require_user)):
    """Save Calculation_Desc config for submeter rates."""
    try:
        items = body.get("items", [])
        ok = _ddb_put_config("submeter-rate-config", items)
        if ok:
            return {"ok": True, "saved": len(items)}
        return JSONResponse({"error": "Failed to save config"}, status_code=500)
    except Exception as e:
        return JSONResponse({"error": _sanitize_error(e, "save submeter config")}, status_code=500)


@app.get("/api/submeter-rates/export-csv")
def api_submeter_rates_export_csv(ubi_period: str = "03/2026", user: str = Depends(require_user)):
    """Export submeter rates as CSV download."""
    # Pull from cache or the last completed scan result
    cache_key = ("submeter-rates-generate", ubi_period)
    cached = _CACHE.get(cache_key)
    data = cached.get("data") if cached else (_SUBMETER_RATES_STATUS.get("result") or {})
    rows = data.get("rows", []) if isinstance(data, dict) else []

    from io import StringIO
    output = StringIO()
    output.write("Property_Name,AR_CODE,Invoice_Total,Volume_Total_Gals,Utility_Name,Rate,Calculation_Desc,Raw_UOM,UOM_Override,RunDateTime\n")
    for row in rows:
        prop = f'"{row["property_name"]}"' if "," in row.get("property_name", "") else row.get("property_name", "")
        ar = f'"{row["ar_code"]}"' if "," in row.get("ar_code", "") else row.get("ar_code", "")
        rate_str = f'{row["rate"]:.6f}' if row.get("rate") is not None else ""
        raw_uom = f'"{row.get("raw_uom", "")}"' if "," in row.get("raw_uom", "") else row.get("raw_uom", "")
        output.write(f'{prop},{ar},{row.get("invoice_total", 0)},{row.get("volume_total_gals", 0)},{row.get("utility_name", "")},{rate_str},{row.get("calculation_desc", "")},{raw_uom},{row.get("uom_override", "Auto")},{row.get("run_datetime", "")}\n')

    csv_content = output.getvalue()
    safe_period = ubi_period.replace("/", "-")
    return StreamingResponse(
        BytesIO(csv_content.encode("utf-8")),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename=submeter_rates_{safe_period}.csv"},
    )
